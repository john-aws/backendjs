<head><title>Backend Documentation</title><link rel="stylesheet" href="css/doc.css"></head>
<h1 id="-backend-img-logo-png-backend-documentation"><img src="img/logo.png" alt="Backend"> Backend Documentation</h1>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#backend-js-framework-for-node-js"> Backend.js framework for node.js</a></li>
<li><a href="#requirements-and-dependencies"> Requirements and dependencies</a></li>
<li><a href="#installation"> Installation</a></li>
<li><a href="#quick-start"> Quick start</a></li>
<li><a href="#database-schema-definition"> Database schema definition</a></li>
<li><a href="#api-endpoints-provided-by-the-backend"> API endpoints provided by the backend</a><ul>
<li><a href="#accounts"> Accounts</a></li>
<li><a href="#public-images-endpoint"> Public Images endpoint</a></li>
<li><a href="#icons"> Icons</a></li>
<li><a href="#connections"> Connections</a></li>
<li><a href="#locations"> Locations</a></li>
<li><a href="#messages"> Messages</a></li>
<li><a href="#counters"> Counters</a></li>
<li><a href="#history"> History</a></li>
<li><a href="#data"> Data</a></li>
<li><a href="#system-api"> System API</a></li>
</ul>
</li>
<li><a href="#backend-directory-structure"> Backend directory structure</a></li>
<li><a href="#internal-backend-functions"> Internal backend functions</a></li>
<li><a href="#cache-configurations"> Cache configurations</a><ul>
<li><a href="#internal-with-nanomsg"> Internal with nanomsg</a></li>
</ul>
</li>
<li><a href="#memcached"> memcached</a></li>
<li><a href="#redis"> Redis</a></li>
<li><a href="#pub-sub-configurations"> PUB/SUB configurations</a><ul>
<li><a href="#internal-with-nanomsg"> Internal with nanomsg</a></li>
<li><a href="#redis"> Redis</a></li>
</ul>
</li>
<li><a href="#security-configurations"> Security configurations</a><ul>
<li><a href="#api-only"> API only</a></li>
<li><a href="#secure-web-site-client-verification"> Secure Web site, client verification</a></li>
<li><a href="#secure-web-site-backend-verification"> Secure Web site, backend verification</a></li>
</ul>
</li>
<li><a href="#the-backend-provisioning-utility-bkjs"> The backend provisioning utility: bkjs</a></li>
<li><a href="#deployment-use-cases"> Deployment use cases</a><ul>
<li><a href="#aws-instance-setup"> AWS instance setup</a></li>
<li><a href="#configure-http-port"> Configure HTTP port</a></li>
</ul>
</li>
<li><a href="#security"> Security</a></li>
<li><a href="#backend-framework-development-mac-os-x-developers-"> Backend framework development (Mac OS X, developers)</a></li>
<li><a href="#author"> Author</a></li>
<li><a href="#configuration-parameters">Configuration parameters</a></li>
<li>Javascript API functions<ul>
<li><a href="#module-api">api</a></li>
<li><a href="#module-aws">aws</a></li>
<li><a href="#module-core">core</a></li>
<li><a href="#module-db">db</a></li>
<li><a href="#module-logger">logger</a></li>
<li><a href="#module-server">server</a></li>
</ul>
</li>
</ul>
<h1 id="backend-js-framework-for-node-js">Backend.js framework for node.js</h1>
<p>General purpose backend framework.</p>
<p>Features:</p>
<ul>
<li>Exposes a set of Web service APIs over HTTP(S) using Express framework.</li>
<li>Supports Sqlite, PostgreSQL, MySQL, DynamoDB, Cassandra, LevelDB, MongoDB, LMDB databases, easily extendable to support any kind of database.</li>
<li>Provides accounts, connections, locations, messaging and icons APIs with basic functionality for a qucik start.</li>
<li>Supports crontab-like and on-demand scheduling for local and remote(AWS) jobs.</li>
<li>Authentication is based on signed requests using API key and secret, similar to Amazon AWS signing requests.</li>
<li>Runs web server as separate processes to utilize multiple CPU cores.</li>
<li>Local jobs are executed by spawned processes</li>
<li>Supports several cache modes(Redis, memcached, local cache) for the database operations.</li>
<li>Supports several PUB/SUB modes of operatios using nanomsg, Redis.</li>
<li>Supports common database operations (Get, Put, Del, Update, Select) for all databases using the same DB API.</li>
<li>ImageMagick is compiled as C++ module for in-process image scaling.</li>
<li>nanomsg interface for messaging between processes and servers.</li>
<li>REPL(command line) interface for debugging and looking into server internals.</li>
<li>Geohash based location searches supported by all databases drivers.</li>
</ul>
<p>Check out the <a href="http://vseryakov.github.io/backendjs/">Documentation</a> for more documentation.</p>
<h1 id="requirements-and-dependencies">Requirements and dependencies</h1>
<p>The module supports several databases and includes ImageMagick interface so in order for such interfaces to be compiled the software must be installed
on the system before installing the backendjs. Not everything is required, if not available the interface will be skipped.</p>
<p>The optional packages that the backendjs may use if available, resolving packages is done with pkg-config:</p>
<ul>
<li>nanomsg - messaging, caching and pub/sub services</li>
<li>ImageMagick - image manipulation</li>
<li>libpq - PostgreSQL database driver</li>
<li>libmysql - MySQL database driver</li>
</ul>
<p>Installing dependencies on CentOS:</p>
<pre><code>    yum -y install libpng-devel libjpeg-turbo-devel postgresql-devel mysql-devel
</code></pre><p>Installing dependencies on Mac OS X using macports:</p>
<pre><code>    port install libpng jpeg tiff mysql56 postgresql93
</code></pre><h1 id="installation">Installation</h1>
<p>To install the module with all optional dependencies if they are available in the system</p>
<p>Note: if for example ImageMagick is not istalled it will be skipped, same goes to all database drivers(PostgreSQL, MySQL) and nanomsg.</p>
<pre><code>    npm install backendjs
</code></pre><p>To force internal nanomsg and ImageMagick to be compiled the following command must be used:</p>
<pre><code>    npm install backendjs --backend_deps_force
</code></pre><p>This may take some time because of compiling required dependencies like ImageMagick, nanomsg. They are not required in all
applications but still part of the core of the system to be available once needed.</p>
<h1 id="quick-start">Quick start</h1>
<ul>
<li><p>Simplest way of using the backendjs, it will start the server listening on port 8000</p>
<pre><code>  # node
  &gt; var bk = require(&#39;backendjs&#39;)
  &gt; bk.server.start()
</code></pre></li>
<li><p>Same but using the helper tool, by default it will use embedded Sqlite database and listen on port 8000</p>
<pre><code>  bkjs run-backend
</code></pre></li>
<li><p>To start node.js shell with backendjs loaded and initialized</p>
<pre><code>  bksh
</code></pre></li>
<li><p>Documentation is always available when the backend Web server is running at <a href="http://localhost:8000/doc.html">http://localhost:8000/doc.html</a></p>
</li>
<li><p>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> for the Web console to test API requests.
For this example let&#39;s create couple of accounts, type and execute the following URLs in the Web console</p>
<pre><code>  /account/add?name=test1&amp;secret=test1&amp;login=test1@test.com
  /account/add?name=test2&amp;secret=test2&amp;login=test2@test.com
</code></pre></li>
</ul>
<ul>
<li>Now login with any of the accounts above, click on Login at the top-right corner and enter login and secret in the login popup dialog.</li>
<li><p>If no error message appeared after the login, try to get your current account details:</p>
<pre><code>  /account/get
</code></pre></li>
<li><p>To see all public fields for all accounts just execute</p>
<pre><code>  /account/select
</code></pre></li>
<li><p>Shutdown the backend by pressing Ctrl-C</p>
</li>
<li><p>To make your own custom Web app, create a new directory (somewhere else) to store your project and run the following command from that directory:</p>
<pre><code>  bkjs init-app
</code></pre></li>
<li><p>The app.js file is created in your project directory with 2 additional API endpoints <code>/test/add</code> and <code>/test/[0-9]</code> to show the simplest way
of adding new tables and API commands.</p>
</li>
<li>The app.sh script is created for convenience, it specifies common arguments and can be customized as needed</li>
<li><p>Run new application now, it will start the Web server on port 8000:</p>
<pre><code>  ./app.sh
</code></pre></li>
</ul>
<ul>
<li>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> and issue <code>/test/add?id=1&amp;name=1</code> and then <code>/test/1</code> commands in the console to see it in action</li>
<li>Change in any of the source files will make the server restart automatically letting you focus on the source code and not server management, this mode
is only enabled by default in development mode, check app.sh for parameters before running it in te production.</li>
</ul>
<h1 id="database-schema-definition">Database schema definition</h1>
<p>The backend support multiple databases and provides the same db layer for access. Common operations are supported and all other specific usage can be achieved by
using SQL directly or other query language supported by any particular database.
The database operations supported in the unified way provide simple actions like get, put, update, delete, select, the query method provides generic
access to the databe driver and executes given query directly.</p>
<p>Before the tables can be queried the schema must be defined and created, the backend db layer provides simple functions to do it:</p>
<ul>
<li><p>first the table needs to be described, this is achieved by creating a Javascript object with properties describing each column, multiple tables can be described
at the same time, for example lets define album table and make sure it exists when we run our application:</p>
<pre><code>      api.describeTables({
          album: {
              id: { primary: 1 },                         // Primary key for an album
              name: { pub: 1 },                           // Album name, public column
              mtime: { type: &quot;bigint&quot; },                  // Modification timestamp
          },
          photo: {
              album_id: { primary: 1 },                   // Combined primary key
              id: { primary: 1 },                         // consiting of album and photo id
              name: { pub: 1, index: 1 },                 // Photo name or description, public column with the index for faster search
              mtime: { type: &quot;bigint&quot; }
          }
       });
</code></pre></li>
<li><p>the system will automatically create the album and photos tables, this definition must remain in the app source code
and be called on every app startup. This allows 1) to see the db schema while working with the app and 2) easily maintain it by adding new columns if
necessary, all new columns will be detected and the database tables updated accordingly. And it is all Javascript, no need to learn one more language or syntax
to maintain database tables.</p>
</li>
</ul>
<p>Each database may restrict how the schema is defined and used, the db layer does not provide an artificial layer hiding all specifics, it just provides the same
API and syntax, for example, DynamoDB tables must have only hash primary key or combined hash and range key, so when creating table to be used with DynamoDB, only
one or two columns can be marked with primary property while for SQL databases the composite primary key can conisit of more than 2 columns.</p>
<h1 id="api-endpoints-provided-by-the-backend">API endpoints provided by the backend</h1>
<h2 id="accounts">Accounts</h2>
<p>The accounts API manages accounts and authentication, it provides basic user account features with common fields like email, name, address.</p>
<ul>
<li><p><code>/account/get</code></p>
<p>Returns information about current account or other accounts, all account columns are returned for the current account and only public columns
returned for other accounts. This ensures that no private fields ever be exposed to other API clients. This call also can used to login into the service or
verifying if the given login and secret are valid, there is no special login API call because each call must be signed and all calls are stateless and independent.</p>
<p>Parameters:</p>
<ul>
<li>no id is given, return only one current account record as JSON</li>
<li>id=id,id,... - return information about given account(s), the id parameter can be a single account id or list of ids separated by comma</li>
<li>_session - after successful login setup a session with cookies so the Web app can perform requests without signing every request anymore</li>
</ul>
<p>Note: When retrieving current account, all properties will be present including the location, for other accounts only the properties marked as <code>pub</code> in the
<code>bk_account</code> table will be returned.</p>
<p>Response:</p>
<pre><code>      { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
        &quot;alias&quot;: &quot;Test User&quot;,
        &quot;name&quot;: &quot;Real Name&quot;,
        &quot;mtime&quot;: 1391824028,
        &quot;latitude&quot;: 34,
        &quot;longitude&quot;: -118,
        &quot;geohash&quot;: &quot;9qh1&quot;,
        &quot;login&quot;: &quot;testuser&quot;,
      }
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/add</code></p>
<p>Add new account, all parameters are the columns from the <code>bk_account</code> table, required columns are: <strong>name, secret, login</strong>.</p>
<p>By default, this URL is in the list of allowed paths that do not need authentication, this means that anybody can add an account. For the real
application this may not be a good choice so the simplest way to disable it to add api-disallow-path=^/account/add$ to the config file or
specify in the command line. More complex ways to perform registration will require adding pre and.or post callbacks to handle account registration
for example with invitation codes....</p>
<p>In the table <code>bk_auth</code>, the column type is used to distinguish between account roles, by default only account with type <code>admin</code> can
add other accounts with this type specified, this column can also be used in account permissions implementations. Because it is in the bk_auth table,
all columns of this table are available as <code>req.account</code> object after the successful authentication where req is Express request object used in the middleware
parameters.</p>
<p><em>Note: secret and login can be anything, the backend does not require any specific formats so one simple trick which is done by the
backend Web client is to scramble login/secret using HMAC-SHA1 and keep them in the local storage, this way the real login and secret is never exposed but
the login popup will still asking for real name, see backend.js in the web/js folder for more details.</em></p>
<p>Example:</p>
<pre><code>      /account/add?name=test&amp;login=test@test.com&amp;secret=test123&amp;gender=f&amp;phone=1234567
</code></pre></li>
</ul>
<p>  How to make an account as admin</p>
<pre><code>        # Run backend shell
        bkjs run-shell

        # Update record by login
        &gt; db.update(&quot;bk_auth&quot;, { login: &#39;login@name&#39;, type: &#39;admin&#39; });
</code></pre><ul>
<li><p><code>/account/select</code></p>
<p>Return list of accounts by the given condition, calls <code>db.select</code> for bk_account table. Parameters are the column values to be matched and
all parameters starting with underscore are control parameters that goes into options of the <code>db.select</code> call with underscore removed. This will work for SQL
databases only because DynamoDB or Cassandra will not search by non primary keys. In the DynamoDB case this will run ScanTable action which will be very expensive for
large tables. Supports special query parameters <code>_select,_keys,_ops</code>, see docs about <code>db.select</code> for more info.</p>
<p>Example:</p>
<pre><code>      /account/search?email=test&amp;_ops=email,begins_with
      /account/search?name=test&amp;_keys=name
</code></pre></li>
</ul>
<p>  Response:</p>
<pre><code>        {  &quot;data&quot;: [{
                      &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
                      &quot;alias&quot;: &quot;Test User1&quot;,
                      &quot;name&quot;: &quot;User1&quot;,
                      &quot;mtime&quot;: 1391824028,
                      &quot;login&quot;: &quot;test1&quot;,
                    },
                    {
                      &quot;id&quot;: &quot;57d07a4e2824fc43bd669f6c8e04d6c3&quot;,
                      &quot;alias&quot;: &quot;Test User2&quot;,
                      &quot;name&quot;: &quot;User2&quot;,
                      &quot;mtime&quot;: 1391824028,
                      &quot;login&quot;: &quot;test2&quot;,
                    }],
            &quot;next_token&quot;: &quot;&quot;
        }
</code></pre><ul>
<li><p><code>/account/del</code></p>
<p>Delete current account, after this call no more requests will be authenticated with the current credentials</p>
</li>
<li><p><code>/account/update</code></p>
<p>Update current account with new values, the parameters are columns of the table <code>bk_account</code>, only columns with non empty values will be updated.</p>
<p>Example:</p>
<pre><code>      /account/update?name=New%2BName&amp;alias=Hidden%2BName&amp;gender=m
</code></pre></li>
<li><p><code>/account/put/secret</code></p>
<p>Change account secret for the current account, no columns except the secret will be updated and expected.</p>
<p>Parameters:</p>
<ul>
<li>secret - new secret for the account</li>
</ul>
<p>Example:</p>
<pre><code>      /account/put/secret?secret=blahblahblah
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/subcribe</code></p>
<p>Subscribe to account events delivered via HTTP Long Poll, a client makes the connection and waits for events to come, whenever
somebody updates the account&#39;s counter or send a message or creates a connection to this account the event about it will be sent to this HTTP
connection and delivered as JSON object. This is not a persistent queue so if not listening, all events will just be ignored, only events published
since the connect will be delivered. To specify what kind of events needs to be delivered, <code>match</code> query parameter can be specified which is a
RegExp of the whole event body string.</p>
<p><em>Note: On the server side there is a config parameter <code>subscribeInterval</code> which defines how often to deliver notifications, by default it is 5 seconds which means
only every 5 seconds new events will be delivered to the Web client, if more than one event happened, they all accumulate and will be sent as a JSON list.</em></p>
<p>Example:</p>
<pre><code>  /account/subscribe
  /account/subscribe?match=connection/add.*type:*like
</code></pre></li>
</ul>
<p>  Response:</p>
<pre><code>    [ { &quot;path&quot;: &quot;/message/add&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;1&quot; },
      { &quot;path&quot;: &quot;/counter/incr&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;like,invite&quot; } },
      { &quot;path&quot;: &quot;/connection/add&quot;, &quot;mtime&quot;: 1223345545, &quot;type&quot;: &quot;like&quot; } ]
</code></pre><ul>
<li><p><code>/account/select/icon</code></p>
<p>Return a list of available account icons, icons that have been uploaded previously with /account/put/icon calls. The <code>url</code> property is an URL to retrieve this particular icon.</p>
<p>Parameters:</p>
<ul>
<li>id - if specified then icons for the given account will be returned</li>
</ul>
<p>Example:</p>
<pre><code>  /account/select/icon?id=12345
</code></pre><p>Response:</p>
<pre><code>  [ { id: &#39;12345&#39;, type: &#39;1&#39;, url: &#39;/account/get/icon?id=12345&amp;type=1&#39; },
    { id: &#39;12345&#39;, type: &#39;2&#39;, url: &#39;/account/get/icon?id=12345&amp;type=2&#39; } ]
</code></pre></li>
<li><p><code>/account/get/icon</code></p>
<p>Return an account icon, <em>the icon is returned in the body as binary BLOB</em>, if no icon with specified type exists, i.e. never been uploaded then 404 is returned.</p>
<p>Parameters:</p>
<ul>
<li>type - a number from 0 to 9 or any single letter a..z which defines which icon to return, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/get/icon?type=2
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/put/icon</code></p>
<p>Upload an account icon, once uploaded, the next <code>/account/get</code> call will return propertis in the format <code>iconN</code> wheer N is any of the
type query parameters specified here, for example if we uploaded an icon with type 5, then /account/get will return property icon5 with the URL
to retrieve this icon.
<em>By default all icons uploaded only accessible for the account which uploaded them.</em></p>
<p>Parameters:</p>
<ul>
<li>type - icon type, a number between 0 and 9 or any single letter a..z, if not specified 0 is used</li>
<li>icon - can be passed as base64 encoded image in the query,<ul>
<li>can be passed as base64 encoded string in the body as JSON, like: { type: 0, icon: &#39;iVBORw0KGgoA...&#39; },
for JSON the Content-Type HTTP headers must be set to <code>application/json</code> and data should be sent with POST request</li>
<li>can be uploaded from the browser using regular multi-part form</li>
</ul>
</li>
<li>acl_allow - icon access permissions:<ul>
<li>&quot;&quot; (empty) - only own account can access</li>
<li>all - public, everybody can see this icon</li>
<li>auth - only authenticated users can see this icon</li>
<li>id,id.. - list of account ids that can see this account</li>
</ul>
</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if th eimage width is less than given keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg, jp2</li>
</ul>
<p>Example:</p>
<pre><code>  /account/put/icon?type=1&amp;icon=iVBORw0KGgoAAAANSUhEUgAAAAcAAAAJCAYAAAD+WDajAAAABGdBTUEAALGPC/xhBQAAAAlwSFlzAAAOwgAADs....
</code></pre></li>
<li><p><code>/account/del/icon</code></p>
<p>Delete account icon</p>
<p>Parameters:</p>
<ul>
<li>type - what icon to delete, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/icon/del?type=1
</code></pre></li>
</ul>
<h2 id="public-images-endpoint">Public Images endpoint</h2>
<p>This endpoint can server any icon uploaded to the server for any account, it is supposed to be a non-secure method, i.e. no authentication will be performed and no signagture
will be needed once it is confgiured which prefix can be public using <code>api-allow</code> or <code>api-allow-path</code> config parameters.</p>
<p>The format of the endpoint is:</p>
<pre><code>/image/prefix/id/type

Example:

    # Configure accounts icons to be public in the etc/config
    api-allow-path=/image/account/

    # Or pass in the command line
    ./app.sh -api-allow-path /image/account/

    # Make requests
    /image/account/12345/0
    /image/account/12345/1

    #Return icons for account 12345 for types 0 and 1
</code></pre><h2 id="icons">Icons</h2>
<p>The icons API provides ability for an account to store icons of different types. Each account keeps its own icons separate form other
accounts, within the account icons can be separated by <code>prefix</code> which is just a namespace assigned to the icons set, for example to keep messages
icons separate from albums, or use prefix for each separate album. Within the prefix icons can be assigned with unique type which can be any string.</p>
<p>Prefix and type can consist from alphabetical characters and numbers, dots, underscores and dashes: [a-z0-9._-]. This means, they are identificators, not real titles or names,
a special mapping between prefix/type and album titles for example needs to be created separately.</p>
<p>The supposed usage for type is to concatenate common identifiers first with more specific to form unique icon type which later can be queried
by prefix or exactly by icon type. For example album id can be prefixed first, then sequential con number like album1:icon1, album1:icon2....
then retrieving all icons for an album would be only query with album1: prefix.</p>
<ul>
<li><code>/icon/get/prefix</code></li>
<li><p><code>/icon/get/prefix/type</code></p>
<p> Return icon for the current account in the given prefix, icons are kept on the local disk in the directory
 configured by -api-images-dir parameter(default is images/ in the backend directory). Current account id is used to keep icons
 separate from other accounts. <code>type</code> is used to specify unique icon created with such type which can be any string.</p>
</li>
<li><p><code>/icon/put/prefix</code></p>
</li>
<li><p><code>/icon/put/prefix/type</code></p>
<p>Upload new icon for the given account in the folder prefix, if type is specified it creates an icons for this type to separate
multiple icons for the same prefix. <code>type</code> can be any string consisting from alpha and digits characters.</p>
<p>The following parameters can be used:</p>
<ul>
<li>descr - optional description of the icon</li>
<li>latitude, longitude - optional coordinates for the icon</li>
<li>acl_allow - allow access permissions, see <code>/account/put/icon</code> for the format and usage</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if th eimage width is less than given keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg</li>
</ul>
</li>
<li><p><code>/icon/del/prefix</code></p>
</li>
<li><p><code>/icon/del/prefix/type</code></p>
<p> Delete the default icon for the current account in the folder prefix or by type</p>
</li>
<li><p><code>/icon/select/prefix</code></p>
</li>
<li><p><code>/icon/select/prefix/type</code>
Return list of available icons for the given prefix adn type, all icons starting with prefix/type will be returned,
the <code>url</code> property will provide full URL to retrieve the icon contents</p>
<p>Example:</p>
<pre><code>  /icon/select/album/me
  /icon/select/album/12345
</code></pre><p>Responses:</p>
<pre><code>  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-1&#39;, icon: &#39;/icon/get/album/me1&#39; },
    { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-2&#39;, icon: &#39;/icon/get/album/me2&#39; } ]

  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;12345-f0deaa81a&#39;, icon: &#39;/icon/get/album/12345-f0deaa81a&#39; } ]
</code></pre></li>
</ul>
<h2 id="connections">Connections</h2>
<p>The connections API maintains two tables <code>bk_connection</code> and <code>bk_reference</code> for links between accounts of any type. bk_connection table maintains my
links, i.e. when i make explicit connection to other account, and bk_reference table is automatically updated with reference for that other account that i made
a connection with it. No direct operations on bk_reference is allowed.</p>
<ul>
<li><code>/connection/add</code></li>
<li><p><code>/connection/put</code>
Create or replace a connection between two accounts, required parameters are:</p>
<ul>
<li><code>id</code> - id of account to connect to</li>
<li><code>type</code> - type of connection, like,dislike,....</li>
<li>_connected - the reply will contain a property <code>connected</code> set to 1 if the other side of our connection is connected to us as well</li>
</ul>
<p>This call automatically creates a record in the bk_reference table which is reversed connection for easy access to information like
&#39;&#39;who is connected to me&#39;&#39; and auto-increment like0, like1 counters for both accounts in the bk_counter table.</p>
<p>Also, this call updates the counters in the <code>bk_counter</code> table for my account which match the connection type, for example if the type of
connection is &#39;invite&#39; and the <code>bk_counter</code> table contain 2 columns <code>invite0</code> and <code>invite1</code>, then both counters will be increased.</p>
<p>Example:</p>
<pre><code>  /connection/add?id=12345&amp;type=invite&amp;state=sent
</code></pre></li>
<li><p><code>/connection/update</code>
Update other properties of the existing connection, for connections that may take more than i step or if a connection has other data associated with it beside
the type of the connection.</p>
<p>Example:</p>
<pre><code>  /connection/update?id=12345&amp;type=invite&amp;state=accepted
</code></pre></li>
<li><p><code>/connection/del</code>
Delete existing connection(s), <code>id</code> and/or <code>type</code> may be be specified, if not all existing connections will be deleted.</p>
<p>Example:</p>
<pre><code>  /connection/del?type=invite&amp;id=12345
</code></pre></li>
<li><p><code>/connection/get</code>
Receive all my connections of the given type, i.e. connection(s) i made, if <code>id</code> is given only one record for the specified connection will be returned. Supports special
query parameters <code>_select,_keys,_ops</code>, see docs about <code>db.select</code> for more info.</p>
<p>Example:</p>
<pre><code>  /connection/get?type=invite             - return all accounts who i invited
  /connection/get?type=invite&amp;id=12345
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;12345&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }
</code></pre></li>
<li><p><code>/reference/get</code>
Receive all references that connected with my account, i.e. connections made by somebody else with me, works the same way as for connection query call</p>
<p>Example:</p>
<pre><code>  # Return all accounts who invited me
  /reference/get?type=invite
  # Return accounts who invited me after specified mtime
  /reference/get?type=invite&amp;_keys=id,type,mtime&amp;_ops=mtime,gt&amp;mtime=12334312543
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }
</code></pre></li>
</ul>
<h2 id="locations">Locations</h2>
<p>The location API maintains a table <code>bk_location</code> with geolocation coordinates for accounts and allows searching it by distance. The configuration parameter
<code>min-distance</code> defines the radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. By default min-distance is 5 km
which means all geohashes in bk_location table will have geohash of size 4. Once min-distance is set it cannot be changed without rebuilding the bk_location table with new geohash size.</p>
<ul>
<li><p><code>/location/put</code>
Store currenct location for current account, latitude and longitude parameters must be given, this call will update the bk_account table as well with
these coordinates</p>
<p>Example:</p>
<pre><code>  /location/put?latitude=-188.23232&amp;longitude=23.4545454
</code></pre></li>
<li><p><code>/location/get</code>
Return matched accounts within the distance(radius) specified by <code>distance=</code> parameter in kilometers and current position specified by latitude/longitude paraemeters. This
call returns results in chunks and requires navigation through all pages to receive all matched records. Records returned will start with the closest to the current
point. If there are more matched records than specified by the <code>_count</code>, the <code>next_token</code> property is set with the token to be used in the subsequent call,
it must be passed as is as <code>_token=</code> parameter with all original query parameters.</p>
<p>By default only locations with account ids will be returned, specifying <code>_details=1</code> will return public account columns like name as well.</p>
<p>Note: The current account will not be present in the results  even if it is within the range, to know my own location use <code>/account/get</code> call.</p>
<p>Example:</p>
<pre><code>      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25
      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25&amp;_token=FGTHTRHRTHRTHTTR.....
</code></pre><p>Response:</p>
<pre><code>     { &quot;data&quot;: [ { &quot;id&quot;: &quot;12345&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.123,
                   &quot;longitude&quot;: 23.45
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 },
                 { &quot;id&quot;: &quot;45678&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.133,
                   &quot;longitude&quot;: 23.5
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 }],
       &quot;next_token&quot;: &quot;&quot;
     }
</code></pre></li>
</ul>
<h2 id="messages">Messages</h2>
<p>The messaging API allows sending and recieving messages between accounts, it supports text and images. The typical usage of this API is to
poll the counter record using <code>/counter/get</code> from time to time and check for <code>msg_count</code> and <code>msg_read</code> counters, once <code>msg_count</code> is greater than <code>msg_read</code> this means
there is a new message arrived. Then call <code>/message/get</code> to retrieve all or only new messages arrived after some point in time and store the mtime
from the last messages received so the next time we will use this time to get only new messages.</p>
<ul>
<li><p><code>/message/image</code>
Return the image data for the given message, the required parameters are:</p>
<ul>
<li>sender - id of the sender returned in the by <code>/message/get</code> reply results for every message</li>
<li>mtime - exact timestamp of the message</li>
</ul>
</li>
<li><p><code>/message/get</code>
Receive messages, the parameter <code>mtime</code> defines which messages to get, if omitted all messages will be returned. By <code>mtime</code> it is possible to
specify that only messages received since that time to return, it must be in milliseconds since midnight GMT on January 1, 1970, this is what
Date.now() return in Javascript. The images are not returned, only link to the image in <code>icon</code> property of reach record,
the actual image data must be retrieved separately.</p>
<p>When <code>sender</code> is specified then all messages for given sender will be returned.</p>
<p>NOTE: The <code>mtime</code> is when the backend server received the message, if client and the server clocks are off this may return wrong data or not return anything at all,
also because the arrival order of the messages cannot be guaranteed, sending fast multiple messages may be received in different order by the backend and this will
result in mtimes that do not correspond to actual times when the message has been sent.</p>
<p>Example:</p>
<pre><code>  # Get all messages
  /message/get

  # Get all messages received after given mtime
  /message/get?mtime=123475658690

  # Get all messages with custom filter: if msg text contains Hi
  /message/get?_keys=id,mtime,msg&amp;_ops=msg,iregexp&amp;msg=Hi

  # Get all messages for specific sender
  /message/get?sender=12345
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;sender&quot;: &quot;12345&quot;,
                &quot;msg&quot;: &quot;Hi, how r u?&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              },
              { &quot;sender&quot;: &quot;45678&quot;,
                &quot;msg&quot;: &quot;check this out!&quot;,
                &quot;icon&quot;: &quot;/message/image?sender=45678&amp;mtime=12334312543&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              }],
       &quot;next_token&quot;: &quot;&quot;
     }
</code></pre></li>
<li><p><code>/message/get/unread</code>
Read all unread messages, i.e. the messages that never been issued <code>/message/read</code> call.</p>
<p>Parameters:</p>
<ul>
<li><code>_read</code> - if set to 1, all returned messages will be marked as read automatically, so no individual /message/read call needed</li>
</ul>
</li>
</ul>
<ul>
<li><p><code>/message/add</code>
Send a message to an account, the following parametrrs must be specified:</p>
<ul>
<li><code>id</code> - account id of the receiver</li>
<li><code>msg</code> - text of the message, can be empty if <code>icon</code> property exists</li>
<li><code>icon</code> - icon of the message, it can be base64 encoded image in the query or JSON string if the whole message is posted as JSON or
can be a multipart file upload if submitted via browser, can be omitted if <code>msg/connection/get?type=invite&amp;id=12345</code> property exists.</li>
</ul>
<p>After successful post the message counters of the destination account will be updated: msg_count will be increased automatically</p>
<p>Example:</p>
<pre><code>  /message/add?id=12345&amp;msg=Hello
  /message/add?id=12345&amp;msg=this%2Bis%2Bthe%2Bpic&amp;icon=KHFHTDDKH7676758JFGHFDRDEDET....TGJNK%2D
</code></pre></li>
<li><p><code>/message/read</code>
Mark a message as read, this will update account counter <code>msg_read</code> automatically. The required query parameters are <code>sender</code> and <code>mtime</code>.</p>
<p>Example:</p>
<pre><code>  /message/read?sender=12345&amp;mtime=12366676434
</code></pre></li>
<li><p><code>/message/del</code>
Delete the message by <code>sender</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del?sender=12345&amp;mtime=124345656567676
</code></pre></li>
</ul>
<h2 id="counters">Counters</h2>
<p>The counters API maintains realtime counters for every account records, the counters record may contain many different counter columns for different purposes and
is always cached with whatever cache service is used, by default it is cached by the Web server process on every machine. Web worker processes ask the master Web server
process for the cached records thus only one copy of the cache per machine even in the case of multiple CPU cores.</p>
<ul>
<li><p><code>/counter/get</code>
Return counter record for current account with all available columns of if <code>id</code> is given return public columns for given account, it works with <code>bk_counter</code> table
which by default defines some common columns:</p>
<ul>
<li>like0 - how many i liked, how many time i liked someone, i.e. made a new record in bk_connection table with type &#39;like&#39;</li>
<li>like1 - how many liked me, reverse counter, who connected to me with type &#39;like&#39;</li>
<li>dislike0 - how many i disliked</li>
<li>dislike1 - how many disliked me</li>
<li>follow0 - how many i follow</li>
<li>follow1 - how many follows me</li>
<li>invite0 - how many i invited</li>
<li>invite1 - how many invited me</li>
<li>msg_count - how messages i received via messaging API</li>
<li>msg_read - how many messages i read using messaging API, these counters allow to keep track of new messages, as soon as msg_count greater than msg_read
it means i have a new message
More columns can be added to the bk_counter table.</li>
</ul>
<p>NOTE: The columns with suffixes 0 and 1 are special columns that support the Connections API, every time a new connection is created, the type of new connection
is checked against any columns in the bk_counter table, if a property type0 exists and marked in the table descriptnio as <code>autoincr</code> then the corresponding
counter property is increased, this is how every time new connectio like/dislike/invite/follow is added, the counters in the bk_counter table are increased.</p>
</li>
<li><p><code>/counter/put</code>
Replace my counters record, all values if not specified will be set to 0</p>
</li>
<li><p><code>/counter/incr</code>
Increase one or more counter fields, each column can provide a numeric value and it will be added to the existing value, negative values will be substracted.
if <code>id</code> parameter is specified, only public columns will be increased for other account.</p>
<p>Example:</p>
<pre><code>  /counter/incr?msg_read=5&amp;
  /counter/incr?id=12345&amp;ping=1
</code></pre></li>
</ul>
<h2 id="history">History</h2>
<p>The history API maintains one table for all application specific logging records. All operations deal with current account only.</p>
<ul>
<li><p><code>/history/add</code>
Add a record to the <code>bk_history</code> table for current account, timestamp is added automatically, all other fields are optional but by default
this table contains only 2 columns: <code>type</code> and <code>data</code> for genetic logging, it can to be extended to support any other application logic if needed.</p>
</li>
<li><p><code>/history/get</code>
Return history record for current account, if mtime is not specified all records from the beginning will be returned, use <code>_count</code> and <code>_start</code> parameters to paginate through
all available records or specify <code>mtime=</code> with the timestamp in milliseconds to start with particular time.</p>
</li>
</ul>
<h2 id="data">Data</h2>
<p>The data API is a generic way to access any table in the database with common operations, as oppose to the any specific APIs above this API only deals with
one table and one record without maintaining any other features like auto counters, cache...</p>
<p><em>Because it exposes the whole database to anybody who has a login it is a good idea to disable this endpoint in the production or provide access callback that verifies
who can access it.</em></p>
<ul>
<li>To disable this endpoint completely in the config: api-disable=data</li>
<li><p>To allow admins to access it only:</p>
<pre><code>api.registerAuthCheck(&#39;GET&#39;, &#39;/data&#39;, function(req, status, cb) { if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &#39;access denied&#39; }; cb(status)); });
</code></pre></li>
</ul>
<ul>
<li><code>/data/columns</code></li>
<li><p><code>/data/columns/TABLE</code>
Return columns for all tables or the specific TABLE</p>
</li>
<li><p><code>/data/keys/TABLE</code>
Return primary keys for the given TABLE</p>
</li>
<li><p><code>/data/(select|search|list|get|add|put|update|del|incr|replace)/TABLE</code>
Perform database operation on the given TABLE, all options for the <code>db</code> functiobns are passed as query parametrrs prepended with underscore,
regular parameters are the table columns.</p>
<p>Example:</p>
<pre><code>  /data/get/bk_account?id=12345
  /data/put/bk_counter?id=12345&amp;like0=1
  /data/select/bk_account?name=john&amp;_ops=name,gt&amp;_select=name,alias,email
</code></pre></li>
</ul>
<h2 id="system-api">System API</h2>
<p>The system API returns information about the backend statistics, allows provisioning and configuration commands and other internal maintenance functions. By
default is is open for access to all users but same security considerations apply here as for the Data API.</p>
<ul>
<li><p><code>/system/cache/(init|stats|keys|get|set|put|incr|del|clear)</code>
  Access to the caching functions</p>
</li>
<li><p><code>/system/msg/(msg)</code>
  Access to the messaging functions</p>
</li>
<li><p><code>/system/stats</code>
Database pool statistics and other diagnostics</p>
<ul>
<li>pool - database metrics<ul>
<li>process - stats about how long it takes between issuing the db request and till the final moment all records are ready to be sent to the client</li>
<li>response - stats about only response times from the db without any local processing times of the result records</li>
<li>queue - stats about db requests at any given moment queued for the execution</li>
<li>rate - req/sec rates</li>
</ul>
</li>
<li>api - Web requests metrics, same structure as for the db pool metrics</li>
</ul>
<p>Individual sub-objects:</p>
<ul>
<li>rate or meter - Things that are measured as events / interval.<ul>
<li>mean: The average rate since the meter was started.</li>
<li>count: The total of all values added to the meter.</li>
<li>currentRate: The rate of the meter since the last toJSON() call.</li>
<li>1MinuteRate: The rate of the meter biased towards the last 1 minute.</li>
<li>5MinuteRate: The rate of the meter biased towards the last 5 minutes.</li>
<li>15MinuteRate: The rate of the meter biased towards the last 15 minutes.</li>
</ul>
</li>
<li>queue or histogram - Keeps a resevoir of statistically relevant values biased towards the last 5 minutes to explore their distribution<ul>
<li>min: The lowest observed value.</li>
<li>max: The highest observed value.</li>
<li>sum: The sum of all observed values.</li>
<li>variance: The variance of all observed values.</li>
<li>mean: The average of all observed values.</li>
<li>stddev: The stddev of all observed values.</li>
<li>count: The number of observed values.</li>
<li>median: 50% of all values in the resevoir are at or below this value.</li>
<li>p75: See median, 75% percentile.</li>
<li>p95: See median, 95% percentile.</li>
<li>p99: See median, 99% percentile.</li>
<li>p999: See median, 99.9% percentile.</li>
</ul>
</li>
</ul>
<p>Response:</p>
<pre><code>   {
      &quot;toobusy&quot;: 0,
      &quot;pool&quot;: {
          &quot;process&quot;: {
              &quot;meter&quot;: {
                  &quot;mean&quot;: 0.001194894762493158,
                  &quot;count&quot;: 65,
                  &quot;currentRate&quot;: 0.001194894762493158,
                  &quot;1MinuteRate&quot;: 2.413646785930864e-158,
                  &quot;5MinuteRate&quot;: 1.2021442332952544e-33,
                  &quot;15MinuteRate&quot;: 7.127940837162242e-13
              },
              &quot;histogram&quot;: {
                  &quot;min&quot;: 1,
                  &quot;max&quot;: 4,
                  &quot;sum&quot;: 99,
                  &quot;variance&quot;: 0.4096153846153847,
                  &quot;mean&quot;: 1.523076923076923,
                  &quot;stddev&quot;: 0.6400120191179106,
                  &quot;count&quot;: 65,
                  &quot;median&quot;: 1,
                  &quot;p75&quot;: 2,
                  &quot;p95&quot;: 2.6999999999999957,
                  &quot;p99&quot;: 4,
                  &quot;p999&quot;: 4
              }
          },
          &quot;queue&quot;: {
              &quot;min&quot;: 1,
              &quot;max&quot;: 1,
              &quot;sum&quot;: 65,
              &quot;variance&quot;: 0,
              &quot;mean&quot;: 1,
              &quot;stddev&quot;: 0,
              &quot;count&quot;: 65,
              &quot;median&quot;: 1,
              &quot;p75&quot;: 1,
              &quot;p95&quot;: 1,
              &quot;p99&quot;: 1,
              &quot;p999&quot;: 1
          },
          &quot;count&quot;: 0,
          &quot;rate&quot;: {
              &quot;mean&quot;: 0.0011948946746301802,
              &quot;count&quot;: 65,
              &quot;currentRate&quot;: 0.0011948946746301802,
              &quot;1MinuteRate&quot;: 2.413646785930864e-158,
              &quot;5MinuteRate&quot;: 1.2021442332952544e-33,
              &quot;15MinuteRate&quot;: 7.127940837162242e-13
          },
          &quot;response&quot;: {
              &quot;meter&quot;: {
                  &quot;mean&quot;: 0.0011948947405274121,
                  &quot;count&quot;: 65,
                  &quot;currentRate&quot;: 0.0011948947405274121,
                  &quot;1MinuteRate&quot;: 2.413646785930864e-158,
                  &quot;5MinuteRate&quot;: 1.2021442332952544e-33,
                  &quot;15MinuteRate&quot;: 7.127940837162242e-13
              },
              &quot;histogram&quot;: {
                  &quot;min&quot;: 0,
                  &quot;max&quot;: 2,
                  &quot;sum&quot;: 65,
                  &quot;variance&quot;: 0.12500000000000003,
                  &quot;mean&quot;: 1,
                  &quot;stddev&quot;: 0.3535533905932738,
                  &quot;count&quot;: 65,
                  &quot;median&quot;: 1,
                  &quot;p75&quot;: 1,
                  &quot;p95&quot;: 2,
                  &quot;p99&quot;: 2,
                  &quot;p999&quot;: 2
              }
          },
          &quot;misses&quot;: 3,
          &quot;hits&quot;: 50
      },
      &quot;api&quot;: {
       &quot;rss&quot;: {
          &quot;min&quot;: 77926400,
          &quot;max&quot;: 145408000,
          &quot;sum&quot;: 23414882304,
          &quot;variance&quot;: 234721528417225.16,
          &quot;mean&quot;: 128653199.47252747,
          &quot;stddev&quot;: 15320624.282881724,
          &quot;count&quot;: 182,
          &quot;median&quot;: 124903424,
          &quot;p75&quot;: 144999424,
          &quot;p95&quot;: 145408000,
          &quot;p99&quot;: 145408000,
          &quot;p999&quot;: 145408000
      },
      &quot;heap&quot;: {
          &quot;min&quot;: 14755896,
          &quot;max&quot;: 31551408,
          &quot;sum&quot;: 4174830592,
          &quot;variance&quot;: 19213862445722.168,
          &quot;mean&quot;: 22938629.626373626,
          &quot;stddev&quot;: 4383362.002586846,
          &quot;count&quot;: 182,
          &quot;median&quot;: 22453472,
          &quot;p75&quot;: 26436622,
          &quot;p95&quot;: 30530277.599999998,
          &quot;p99&quot;: 31331225.599999998,
          &quot;p999&quot;: 31551408
      },
      &quot;loadavg&quot;: {
          &quot;min&quot;: 0,
          &quot;max&quot;: 0.14208984375,
          &quot;sum&quot;: 8.33349609375,
          &quot;variance&quot;: 0.0013957310299007465,
          &quot;mean&quot;: 0.04578844007554945,
          &quot;stddev&quot;: 0.03735948380131538,
          &quot;count&quot;: 182,
          &quot;median&quot;: 0.043701171875,
          &quot;p75&quot;: 0.0806884765625,
          &quot;p95&quot;: 0.103857421875,
          &quot;p99&quot;: 0.13803710937499994,
          &quot;p999&quot;: 0.14208984375
      },
      &quot;freemem&quot;: {
          &quot;min&quot;: 1731198976,
          &quot;max&quot;: 1815724032,
          &quot;sum&quot;: 319208222720,
          &quot;variance&quot;: 335910913486664.44,
          &quot;mean&quot;: 1753891333.6263735,
          &quot;stddev&quot;: 18327872.584854588,
          &quot;count&quot;: 182,
          &quot;median&quot;: 1757151232,
          &quot;p75&quot;: 1763340288,
          &quot;p95&quot;: 1785729638.4,
          &quot;p99&quot;: 1798348267.5199997,
          &quot;p999&quot;: 1815724032
      },
      &quot;rate&quot;: {
          &quot;mean&quot;: 0.005091340673514894,
          &quot;count&quot;: 277,
          &quot;currentRate&quot;: 0.005091340673514894,
          &quot;1MinuteRate&quot;: 0.014712537947741827,
          &quot;5MinuteRate&quot;: 0.003251074139103506,
          &quot;15MinuteRate&quot;: 0.0011131541240945431
      },
      &quot;response&quot;: {
          &quot;meter&quot;: {
              &quot;mean&quot;: 0.005072961780912493,
              &quot;count&quot;: 276,
              &quot;currentRate&quot;: 0.005072961780912493,
              &quot;1MinuteRate&quot;: 0.014712537947741827,
              &quot;5MinuteRate&quot;: 0.003251074139103506,
              &quot;15MinuteRate&quot;: 0.0011131541240946244
          },
          &quot;histogram&quot;: {
              &quot;min&quot;: 1,
              &quot;max&quot;: 11847,
              &quot;sum&quot;: 13614,
              &quot;variance&quot;: 508182.2787351782,
              &quot;mean&quot;: 49.32608695652174,
              &quot;stddev&quot;: 712.8690473959282,
              &quot;count&quot;: 276,
              &quot;median&quot;: 2,
              &quot;p75&quot;: 5.75,
              &quot;p95&quot;: 27.149999999999977,
              &quot;p99&quot;: 122.99000000000024,
              &quot;p999&quot;: 11847
          }
      },
      &quot;queue&quot;: {
          &quot;min&quot;: 1,
          &quot;max&quot;: 2,
          &quot;sum&quot;: 286,
          &quot;variance&quot;: 0.03154920734578558,
          &quot;mean&quot;: 1.032490974729242,
          &quot;stddev&quot;: 0.17762096538918368,
          &quot;count&quot;: 277,
          &quot;median&quot;: 1,
          &quot;p75&quot;: 1,
          &quot;p95&quot;: 1,
          &quot;p99&quot;: 2,
          &quot;p999&quot;: 2
      },
      &quot;count&quot;: 1
      }
  }
</code></pre></li>
</ul>
<h1 id="backend-directory-structure">Backend directory structure</h1>
<p>When the backend server starts and no -home argument passed in the command line the backend makes its home environment in the ~/.backend directory.</p>
<p>The backend directory structure is the following:</p>
<ul>
<li><p><code>etc</code> - configuration directory, all config files are there</p>
<ul>
<li><p><code>etc/config</code> - config parameters, same as specified in the command line but without leading -, each config parameter per line:</p>
<p>  Example:</p>
<pre><code>  debug=1
  db-pool=dynamodb
  db-dynamodb-pool=http://localhost:9000
  db-pgsql-pool=postgresql://postgres@127.0.0.1/backend

  To specify other config file: bkjs run-backend -config-file file
</code></pre></li>
<li><p>some config parameters can be condigured in DNS as TXT records, the backend on startup will try to resolve such records and use the value if not empty.
All params that  marked with DNS TXT can be configured in the DNS server for the domain where the backend is running, the config parameter name is
concatenated with the domain and queried for the TXT record, for example: <code>lru-host</code> parameter will be queried for lru-host.domain.name for TXT record type.</p>
</li>
<li><p><code>etc/crontab</code> - jobs to be run with intervals, local or remote, JSON file with a list of cron jobs objects:</p>
<p>  Example:</p>
<ol>
<li><p>Create file in ~/.backend/etc/crontab with the following contents:</p>
<pre><code> [ { &quot;type&quot;: &quot;local&quot;, &quot;cron&quot;: &quot;0 1 1 * * 1,3&quot;, &quot;job&quot;: { &quot;api.cleanSessions&quot;: { &quot;interval&quot;: 3600000 } } } ]
</code></pre></li>
<li><p>Define the funtion that the cron will call with the options specified, callback must be called at the end, create this app.js file</p>
<pre><code> var backend = require(&quot;backendjs&quot;);
 backend.api.cleanSessions = function(options, callback) {
      backend.db.del(&quot;session&quot;, { mtime: options.interval + Date.now() }, { ops: &quot;le&quot;, keys: [ &quot;mtime&quot; ] }, callback);
 }
 backend.server.start()
</code></pre></li>
<li><p>Start the scheduler and the web server at once</p>
<pre><code> bkjs run-backend -master -web
</code></pre></li>
</ol>
</li>
<li><p><code>etc/proxy</code> - HTTP proxy config file, from http-proxy (<a href="https://github.com/nodejitsu/node-http-proxy">https://github.com/nodejitsu/node-http-proxy</a>)</p>
<p>  Example:</p>
<ol>
<li><p>Create file ~/.backend/etc/proxy with the following contents:</p>
<pre><code> { &quot;target&quot; : { &quot;host&quot;: &quot;localhost&quot;, &quot;port&quot;: 8001 } }
</code></pre></li>
<li><p>Start the proxy</p>
<pre><code> bkjs -proxy
</code></pre></li>
<li><p>Now all requests will be sent to localhost:8001</p>
</li>
</ol>
</li>
<li><p><code>etc/profile</code> - shell script loaded by the bkjs utility to customize env variables</p>
</li>
</ul>
</li>
<li><code>images</code> - all images to be served by the API server, every subfolder represent naming space with lots of subfolders for images</li>
<li><code>var</code> - database files created by the server</li>
<li><code>tmp</code> - temporary files</li>
<li><code>web</code> - Web pages served by the static Express middleware</li>
</ul>
<h1 id="internal-backend-functions">Internal backend functions</h1>
<p>The backend includes internal C++ module which provide some useful functions available in the Javascript. The module is exposed as &quot;backend&quot; submodule, to see
all functions for example run the below:</p>
<pre><code>var backend = require(&#39;backendjs&#39;);
console.log(backend.backend)
</code></pre><p>List of available functions:</p>
<ul>
<li>rungc() - run V8 garbage collector on demand</li>
<li>setsegv() - install SEGV signal handler to show crash backtrace</li>
<li>setbacktrace() - install special V8-aware backtrace handler</li>
<li>backtrace() - show V8 backtrace from current position</li>
<li>heapSnapshot(file) - dump current memory heap snapshot into a file</li>
<li>splitArray(str) - split a string into an array separated by commas, supports double quotes</li>
<li>logging([level]) - set or return logging level, this is internal C++ logging facility</li>
<li>loggingChannel(channelname) - redirect logging into stdout or stderr, this is internal C++ logging</li>
<li>countWords(word, text) - return how many time word appers in the text, uses Knuth-Morris-Pratt algorithm</li>
<li>countAllWords(list, text) - return an object with counters for each word from the list, i.e. how many times each word appears in the text, uses Aho-Corasick algorithm</li>
<li>countWordsInit() - clears word counting cache</li>
<li><p>resizeImage(source, options, callback) - resize image using ImageMagick,</p>
<ul>
<li>source can be a Buffer or file name</li>
<li>options can have the following properties:<ul>
<li>width - output image width, if negative and the original image width is smaller than the specified, nothing happens</li>
<li>height - output image height, if negative and the original image height is smaller this the specified, nothing happens</li>
<li>quality - 0 -99</li>
<li>out - output file name</li>
<li>ext - image extention</li>
</ul>
</li>
</ul>
</li>
<li><p>resizeImageSync(name,width,height,format,filter,quality,outfile) - resize an image synchronically</p>
</li>
<li>snappyCompress(str) - compress a string</li>
<li>snappyUncompress(str) - decompress a string</li>
<li>geoDistance(lat1, lon1, lat2, lon2) - return distance between 2 coordinates in km</li>
<li>geoBoundingBox(lat, lon, distance) - return bounding box geohash for given point around distance</li>
<li>geoHashEncode(lat, lon, len) - return geohash for given coordinate, len defines number of bytesin geohash</li>
<li>geoHashDecode(hash) - return coordinates for given geohash</li>
<li>geoHashAdjacent()</li>
<li>geoHashGrid()</li>
<li>geoHashRow()</li>
<li>cacheSave() - general purpose caching functions that have no memory limits and do not use V8 heap</li>
<li>cacheSet()</li>
<li>cacheGet()</li>
<li>cacheDel()</li>
<li>cacheKeys()</li>
<li>cacheClear()</li>
<li>cacheNames()</li>
<li>cacheSize()</li>
<li>cacheEach()</li>
<li>cacheForEach()</li>
<li>cacheForEachNext()</li>
<li>cacheBegin()</li>
<li>cacheNext()</li>
<li>lruInit(max) - init LRU cache with max number of keys, this is in-memory cache which evicts older keys</li>
<li>lruStats() - return statistics about the LRU cache</li>
<li>lruSize() - return size of the current LRU cache</li>
<li>lruCount() - number of keys in the LRU cache</li>
<li>lruSet(name, val) - set/replace value by name</li>
<li>lruGet(name) - return value by name</li>
<li>lruIncr(name, val) - increase value by given number, non existent items assumed to be 0</li>
<li>lruDel(name) - delete by name</li>
<li>lruKeys() - return all cache key names</li>
<li>lruClear() - clear LRU cache</li>
<li>lruServer()</li>
<li>syslogInit(name, priority, facility) - initialize syslog client, used by the logger module</li>
<li>syslogSend(level, text)</li>
<li>syslogClose()</li>
<li>listStatements() - list all active Sqlite statements</li>
<li>NNSocket() - create a new nanomsg socket object</li>
</ul>
<h1 id="cache-configurations">Cache configurations</h1>
<p>Database layer support caching of the responses using <code>db.getCached</code> call, it retrieves exactly one record from the configured cache, if no record exists it
will pull it from the database and on success will store it in the cache before returning to the client. When dealing with cached records, there is a special option
that must be passed to all put/update/del database methods in order to clear local cache, so next time the record will be retrieved with new changes from the database
and refresh the cache, that is <code>{ cached: true }</code> can be passed in the options parameter for the db methods that may modify records with cached contents. In any case
it is required to clear cache manually there is <code>db.clearCached</code> method for that.</p>
<h2 id="internal-with-nanomsg">Internal with nanomsg</h2>
<p>For cache management signaling, all servers maintain local cache per machine, it is called <code>LRU</code> cache. This cache is maintained in the master Web process and
serves all Web worker processes via IPC channel. Every Web master process if compiled with nanomsg library can accept cache messages on a TCP port (<code>cache-port=20194</code>)
from other backend nodes. Every time any Web worker updates the local cache, its master process re-broadcasts the same request to other connected Web master
processes on other nodes thus keeping in sync caches on all nodes.</p>
<p>The basic flow is the following using a hypothetical example:</p>
<ul>
<li>there are 4 nodes running in the network: node1, node2, node3 and node4, instead of the names nodeX we can use IP addresses as well.</li>
<li>all nodes configured with the parameter <code>cache-host=node1,node2</code></li>
<li>nanomsg sockets may be connected to multiple endpoints, so we can have 2 cache servers for redundency. This config parameter can be specified in the local config file on all nodes
 or can be defined in the DNS server as TXT record <code>cache-host</code>. If running in AWS the config parameters also can be specified in the user-data the same was as command line arguments.</li>
<li>any node which is requested for a cached record asks its local cache for such key and if it not found, retrieves it from the database and puts into local cache,
 if running for a while, potentially every node now may contain same items in the cache and all requests for such items will be served without touching the db</li>
<li>An update request comes to the node3 which deletes a key from the local cache:<ul>
<li>node3 also sends &#39;del&#39; request for the requested key to both node1 and node2 servers via connected sockets because of <code>cache-host</code> config parameter</li>
<li>node1 and node2 servers receive &#39;del&#39; request and delete the key from their local caches</li>
<li>node1 and node2 re-send same request to all connected clients which are actually all nodes in the network, so all nodes receive &#39;del&#39; cache request</li>
<li>because we have 2 LRU servers, every node will receive the same del request twice which is very small packet and removing same item from the cache
costs nothing, both requests will be received within milliseonds from each other.</li>
<li>next request to any of the nodes for that key we just deleted will result in retrieving the record from the database and putting back to the local cache
of the node until the next &#39;del&#39; request.</li>
<li>Important: each node maintains its own version of the cache, i.e. all nodes do not have exactly the same
items in the cache, over time they all retieve same records but only on demand and when one node puts an item in the cache it is not sent to all other nodes.
To make the cache synhronized set <code>cache-sync=1</code>, in this case all cache commands will be sent to all nodes, with multiple broadcast nodes
the same item will be sent multiple times from each broadcast node, this is why it is disabled by default, sending &#39;del&#39; multiple times does not add
much overhead as opposed to re-sending actual items.</li>
</ul>
</li>
</ul>
<p>For very frequent items there is no point using local cache but for items reasonable static with not so often changes this cache model will work reliably and similar to
what <code>memcached</code> or <code>Redis</code> server would do as well.</p>
<p>The benefits of this approach is not to run any separate servers and dealing with its own configuration and support, using nanomsg
internal backend cache system is self contained and does not need additional external resources, any node can be LRU server whose only role is to make sure all other
nodes flush their caches if needed. Using redundant broadcast servers makes sure such flush requests reach all nodes in the cluster and there is no single point of failure.</p>
<p>Essentually, setting <code>cache-host</code> to the list of any nodes in the network is what needs to be done to support distributed cache with nanomsg sockets.</p>
<h1 id="memcached">memcached</h1>
<p>Setting <code>cache-type=memcache</code> and pointing <code>memcache-host</code> to one or more hosts running memcached servers is what needs to be done only, the rest of the
system works similar to the internal nanomsg caching but using memcache client instead. The great benefit using memcache is to configure more than one
server in <code>memcache-host</code> separated by comma which makes it more reliable and eliminates single point of failure if one of the memcache servers goes down.</p>
<h1 id="redis">Redis</h1>
<p>Set <code>cache-type=redis</code> and point <code>redis-host</code> to the server running Redis server. Only single redis server can be specified.</p>
<h1 id="pub-sub-configurations">PUB/SUB configurations</h1>
<p>Publish/subscribe functionality allows clients to receive notifications without constantly polling for new events. A client can be anything but
the backend provides some partially implemented subscription notifications for Web clients using the Long Poll.
The Account API call <code>/account/subscribe</code> can use any pub/sub mode.</p>
<p>The flow of the pub/sub operations is the following:</p>
<ul>
<li>a client makes <code>/account/subscribe</code> API request, the connection is made and is kept open indefenitely or as long as configured using <code>api-subscribe-timeput</code>.</li>
<li>the backend receives this request, and runs the <code>core.ipcSubscribe</code> method with the key being the account id<ul>
<li>if nanomsg pub/sub hosts are configured, the method connect to the sub hosts with the subscription key and wait for the events to be published</li>
</ul>
</li>
<li>some other client makes an API call that triggers an event, like update a counter, sends a message, on such event the backend
always runs <code>core.ipcPublish</code> method and if there is no publish host configured nothing happens.
If there is a host to publish, it sends the message to it, the message being a JSON object with the request API path and mtime, other properties depend on the call made.</li>
<li>this step depends on the pub/sub system being used:<ul>
<li>nanomsg, the publish server receives nanomsg request and re-broadcasts it to all subscription servers connected, there can be more than one pub/sub host configured for
redundancy purposes and to eliminate single point of failure. All subscription servers will receive the published message and send it to the connected HTTP clients
with matched subscribed key.</li>
<li>redis, uses native Redis pub/sub system, every client directly connected to the redis server and subscribed to the events</li>
</ul>
</li>
<li>the client who issued <code>/account/subscribe</code> command may receive the same event more than once, this is expected behaiviour,
it is responsibility of the client to handle duplicates at this moment, this may change in the future.</li>
</ul>
<h2 id="internal-with-nanomsg">Internal with nanomsg</h2>
<p>To use publish/subcribe with nanomsg, first nanomsg must be compiled in the backend module. Usually this is done when explicitely installed with <code>--backend_deps_force</code>
options to the npm install, see above how to install the package.</p>
<p>All nodes must have the same configuration, similar to the LRU cache otherwise some unexpected behaviour may happen.
The config parameter <code>msg-host</code> defines where to publish messages and from where messages can be retrieved. Having more than one hosts listed will ensure
better reliability of delivering messages but may result in message duplication in some cases when some servers are much slower than others for any reason.</p>
<h2 id="redis">Redis</h2>
<p>To configure the backend to use Redis for local cache set <code>msg-type=redis</code> and <code>redis-host=HOST</code> where HOST is IP address or hostname of the redis server.
After that all cache requests will be directed to that redis server. The work flow itself is similar to nanomsg internal system, just instead of nanomsg sockets redis
client is used.</p>
<h1 id="security-configurations">Security configurations</h1>
<h2 id="api-only">API only</h2>
<p>This is default setup of the backend when all API requests except <code>/account/add</code> must provide valid signature and all HTML, Javascript, CSS and image files
are available to everyone. This mode assumes that Web developmnt will be based on &#39;single-page&#39; design when only data is requested from the Web server and all
rendering is done using Javascript. This is how the <code>api.html</code> develpers console is implemented, using JQuery-UI and Knockout.js.</p>
<p>To see current default config parameters run any of the following commands:</p>
<pre><code>    bkjs run-backend -help | grep api-allow

    node -e &#39;require(&quot;backendjs&quot;).core.showHelp()&#39;
</code></pre><p>To disable open registration in this mode just add config parameter <code>api-disallow-path=^/account/add$</code> or if developing an application add this in the initMiddleware</p>
<pre><code>    api.initMiddleware = function(callback) {
        this.allow.splice(this.allow.indexOf(&#39;^/account/add$&#39;), 1);
    }
</code></pre><h2 id="secure-web-site-client-verification">Secure Web site, client verification</h2>
<p>This is a mode when the whole Web site is secure by default, even access to the HTML files must be authenticated. In this mode the pages must defined &#39;Backend.session = true&#39;
during the initialization on every html page, it will enable Web sessions for the site and then no need to sign every API reauest.</p>
<p>The typical client Javascript verification for the html page may look like this, it will redirect to login page if needed,
this assumes the default path &#39;/public&#39; still allowed without the signature:</p>
<pre><code>    &lt;link href=&quot;/styles/jquery-ui.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot; /&gt;
    &lt;script src=&quot;/js/jquery.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/jquery-ui.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/knockout.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/crypto.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/backend.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/backend-jquery-ui.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
    $(function () {
        Backend.session = true;
        Backend.scramble = true;
        ko.applyBindings(Backend);

        Backend.login(function(err, data) {
            if (err) window.location=&#39;/public/index.html&#39;;
        });
    });
    &lt;/script&gt;
</code></pre><h2 id="secure-web-site-backend-verification">Secure Web site, backend verification</h2>
<p>On the backend side in your application app.js it needs more secure settings defined i.e. no html except /public will be accessible and
in case of error will be redirected to the login page by the server. Note, in the login page <code>Backend.session</code> must be set to true for all
html pages to work after login without singing every API request.</p>
<p>First we disable all allowed paths to the html and registration:</p>
<pre><code>    api.initMiddleware = function(callback) {
        self.allow.splice(self.allow.indexOf(&#39;^/$&#39;), 1);
        self.allow.splice(self.allow.indexOf(&#39;\\.html$&#39;), 1);
        self.allow.splice(self.allow.indexOf(&#39;^/account/add$&#39;), 1);
    }
</code></pre><p>Second we define auth callback in the app and redirect to login if the reauest has no valid signature, we check all html pages, all allowed html pages from the /public
will never end up in this callback because it is called after the signature check but allowed pages are served before that:</p>
<pre><code>    api.registerAuthCheck(&#39;&#39;, /^\/$|\.html$/, function(req, status, callback) {
        if (status.status != 200) {
            status.status = 302;
            status.url = &#39;/public/index.html&#39;;
        }
        callback(status);
    });
</code></pre><h1 id="the-backend-provisioning-utility-bkjs">The backend provisioning utility: bkjs</h1>
<p>The purpose of the <code>bkjs</code> shell script is to act as a helper tool in configuring and managing the backend environment
and as well to be used in operations on production systems. It is not required for the backend operations and provided as a convenience tool
which is used in the backend development and can be useful for others running or testing the backend.</p>
<p>Running without arguments will bring help screen with description of all available commands.</p>
<p>The tool is multi-command utility where the first argument is the command to be executed with optional additional arguments if needed.
On startup the bkjs tries to load and source the following config files:</p>
<pre><code>    /etc/backendrc
    /usr/local/etc/backendrc
    ~/.backend/etc/profile
</code></pre><p>Any of the following config files can redefine any environmnt variable thus pointing to the correct backend environment directory or
customize the running environment, these should be regular shell scripts using bash syntax.</p>
<p>Most common used commands are:</p>
<ul>
<li>bkjs run-backend - run the backend or the app for development purposes, uses local app.js if exists otherwise runs generic server</li>
<li>bkjs run-shell - start REPL shell with the backend module loaded and available for use, all submodules are availablein the shell as well like core, db, api</li>
<li>bkjs init-app - create the app skeleton</li>
<li>bkjs put-backend path [-host host] - sync sources of the app with the remote site, uses BACKEND_MASTER env variable for host if not specified in the command line, this is for developent version of the backend only</li>
<li>bkjs setup-server [-root path] [-user user] - initialize Linux instance(Amazon,CentOS) for backend use, optional -root can be specified where the backend
 home will be instead of ~/.backend, optional -user tells to use existing user instead of creating user backend.</li>
</ul>
<h1 id="deployment-use-cases">Deployment use cases</h1>
<h2 id="aws-instance-setup">AWS instance setup</h2>
<p>Here is the typical example how to setup new AWS server, it is not required and completely optional but bkjs provies some helpful commands that may simplify
new image configuration.</p>
<ul>
<li>start new AWS instance via AWS console, use Amazon Linux or CentOS 6</li>
<li>login as <code>ec2-user</code></li>
<li>get bkjs: `curl -OL <a href="https://raw.githubusercontent.com/vseryakov/backendjs/master/bkjs">https://raw.githubusercontent.com/vseryakov/backendjs/master/bkjs</a>&#39;</li>
<li>run <code>sudo ./bkjs setup-server -root /home/backend</code></li>
<li>global system-wide options will be defined in the <code>/etc/backendrc</code> like BACKEND_ARGS, BACKEND_NAME, BACKEND_ROOT env variables</li>
<li>reboot</li>
<li>login as <code>backend</code> user using the AWS keypair private key</li>
<li>run <code>ps agx</code>, it should show several backend processes running</li>
<li>try to access the instance via HTTP port for the API console or documentation</li>
</ul>
<h2 id="configure-http-port">Configure HTTP port</h2>
<p>The first thing when deploying the backend into production is to change API HTTP port, by default is is 8000, but we would want port 80 so regardless
how the environment is setup it is ultimatley 2 ways to specify the port for HTTP server to use:</p>
<ul>
<li><p>config file</p>
<p>The config file is always located in the etc/ folder in the backend home directory, how the home is specified depends on the system but basically it can be
defined via command line arguments as <code>-home</code> or via environment variables when using bkjs. See bkjs documentation but on AWS instances created with bkjs
<code>setup-server</code> command, for non-standard home use <code>/etc/backendrc</code> profile, specify <code>BACKEND_HOME=/home/backend</code> there and the rest will be taken care of</p>
</li>
<li><p>command line arguments</p>
<p>When running node scripts which use the backend, just specify <code>-home</code> command line argument with the directory where yor backend should be and the backend will use it</p>
<p>Example:</p>
<pre><code>  node app.js -home $HOME -port 80
</code></pre></li>
</ul>
<h1 id="security">Security</h1>
<p>All requests to the API server must be signed with account login/secret pair.</p>
<ul>
<li>The algorithm how to sign HTTP requests (Version 1, 2):<ul>
<li>Split url to path and query parameters with &quot;?&quot;</li>
<li>Split query parameters with &quot;&amp;&quot;</li>
<li>&#39;&#39;&#39;ignore parameters with empty names&#39;&#39;&#39;</li>
<li>&#39;&#39;&#39;Sort&#39;&#39;&#39; list of parameters alphabetically</li>
<li>Join sorted list of parameters with &quot;&amp;&quot;<ul>
<li>Make sure all + are encoded as %2B</li>
</ul>
</li>
<li>Form canonical string to be signed as the following:<ul>
<li>Line1: The HTTP method(GET), followed by a newline.</li>
<li>Line2: the host, lowercase, followed by a newline.</li>
<li>Line3: The request URI (/), followed by a newline.</li>
<li>Line4: The sorted and joined query parameters as one string, followed by a newline.</li>
<li>Line5: The expiration value in milliseconds, required, followed by a newline</li>
<li>Line6: The Content-Type HTTP header, lowercase, followed by a newline</li>
</ul>
</li>
<li>Computed HMAC-SHA1 digest from the canonical string and encode it as BASE64 string, preserve trailing = if any</li>
<li>Form BK-Signature HTTP header as the following:<ul>
<li>The header string consist of multiple fields separated by pipe |<ul>
<li>Field1: Signature version:<ul>
<li>version 1, normal signature</li>
<li>version 2, only used in session cookies, not headers</li>
<li>version 3, same as 1 but uses SHA256</li>
</ul>
</li>
<li>Field2: Application version or other app specific data</li>
<li>Field3: account login or whatever it might be in the login column</li>
<li>Field4: HMAC-SHA digest from the canonical string, version 1 o 3 defines SHA1 or SHA256</li>
<li>Field5: expiration value in milliseconds, same as in the canonical string</li>
<li>Field6: SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query paremeters</li>
<li>Field7: empty, reserved for future use</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The resulting signature is sent as HTTP header bk-signature: string</p>
<p>For JSON content type, the method must be POST and no query parameters specified, instead everything should be inside the JSON object
which is placed in the body of the request. For additional safety, SHA1 checksum of the JSON paylod can be calculated and passed in the signature,
this is the only way to ensure the body is not modified when not using query parameters.</p>
<p>See web/js/backend.js for function Backend.sign or function core.signRequest in the core.js for the Javascript implementation.</p>
<h1 id="backend-framework-development-mac-os-x-developers-">Backend framework development (Mac OS X, developers)</h1>
<ul>
<li><p>for DB drivers and ImageMagick to work propely it needs some dependencies to be installed:</p>
<pre><code>  port install libpng jpeg tiff lcms2 mysql56 postgresql93
</code></pre></li>
<li><p>make sure there is no openjpeg15 installed, it will conflict with ImageMagick jp2 codec</p>
</li>
<li><p><code>git clone https://github.com/vseryakov/backendjs.git</code> or <code>git clone git@github.com:vseryakov/backendjs.git</code></p>
</li>
<li><p>cd backendjs</p>
</li>
<li><p>if node.js is already installed skip to the next section</p>
<ul>
<li>node.js can be compiled by the bkjs and installed into default location, on Darwin it is /opt/local</li>
<li>to install node into other location, BACKEND_PREFIX needs to be set</li>
<li><p>to change $BACKEND_PREFIX, create ~/.backend/etc/profile file, for example:</p>
<p>  mkdir -p ~/.backend/etc
  echo &quot;BACKEND_PREFIX=$HOME/local&quot; &gt; ~/.backend/etc/profile</p>
</li>
<li><p><strong>Important</strong>: Add NODE_PATH=$BACKEND_PREFIX/lib/node_modules to your environment in .profile or .bash_profile so
node can find global modules, replace $BACKEND_PREFIX with the actual path unless this variable is also set in the .profile</p>
</li>
<li><p>to install node.js in $BACKEND_PREFIX/bin run command:</p>
<pre><code>  ./bkjs build-node
</code></pre></li>
</ul>
</li>
<li><p>to compile the binary module and all required dependencies just type <code>make</code> or <code>npm build .</code></p>
<ul>
<li><p>to see the actual compiler settings during compilation the following helps:</p>
<pre><code>  make V=1
</code></pre></li>
<li><p>to compile with internal nanomsg and ImageMagick use:</p>
<pre><code>  make force V=1
</code></pre></li>
</ul>
</li>
<li><p>to install all dependencies and make backendjs module and bkjs globally available:</p>
<pre><code>      npm link
</code></pre></li>
<li><p>to run local server on port 8000 run command:</p>
<pre><code>  ./bkjs run-backend
</code></pre></li>
<li><p>to start the backend in command line mode, the backend environment is prepared and initialized including all database pools.
 This command line access allows you to test and run all functions from all modules of the backend without running full server
 similar to node.js REPL functionality. All modules are accessible from the command line.</p>
<pre><code>  $ ./bkjs run-shell
  &gt; core.version
   &#39;2013.10.20.0&#39;
  &gt; logger.setDebug(2)
</code></pre></li>
</ul>
<h1 id="author">Author</h1>
<p>  Vlad Seryakov</p>
<p>Check out the <a href="http://vseryakov.github.io/backendjs/">Documentation</a> for more documentation.</p>
<h2 id="configuration-parameters">Configuration parameters</h2>
<ul>
<li><code>help</code> - Print help and exit</li>
<li><code>debug</code> - Enable debugging messages, short of -log debug</li>
<li><code>log</code> - Set debugging level: none, log, debug, dev</li>
<li><code>log-file</code> - Log to a file, if not specified used default logfile, disables syslog. Default: log/backend.log</li>
<li><code>syslog</code> - Write all logging messages to syslog, connect to the local syslog server over Unix domain socket</li>
<li><code>console</code> - All logging goes to the console resetting all previous log related settings, this is used in the development mode mostly</li>
<li><code>home</code> - Specify home directory for the server, the server will try to chdir there or exit if it is not possible, the directory must exist. Default: /Users/vlad/.backend</li>
<li><code>concurrency</code> - How many simultaneous tasks to run at the same time inside one process, this is used by async module only to perform several tasks at once, this is not multithreading but and only makes sense for I/O related tasks. Default: 2</li>
<li><code>config-file</code> - Path to the config file instead of the default etc/config, can be absolute or relative path</li>
<li><code>err-file</code> - Path to the error log file where daemon will put app errors and crash stacks. Default: log/error.log</li>
<li><code>etc-dir</code> - Path where to keep config files</li>
<li><code>web-dir</code> - Path where to keep web pages</li>
<li><code>tmp-dir</code> - Path where to keep temp files</li>
<li><code>spool-dir</code> - Path where to keep modifiable files</li>
<li><code>log-dir</code> - Path where to keep other log files, log-file and err-file are not affected by this</li>
<li><code>files-dir</code> - Path where to keep uploaded files</li>
<li><code>images-dir</code> - Path where to keep images</li>
<li><code>uid</code> - User id to switch after startup if running as root, used by Web servers and job workers. Default: 777</li>
<li><code>gid</code> - Group id to switch after startup if running to root</li>
<li><code>umask</code> - Permissions mask for new files, calls system umask on startup, if not specified the current umask is used. Default: 0002</li>
<li><code>port</code> - port to listen for the HTTP server, this is global default. Default: 8000</li>
<li><code>bind</code> - Bind to this address only, if not specified listen on all interfaces. Default: 0.0.0.0</li>
<li><code>ssl-port</code> - port to listen for HTTPS server, this is global default</li>
<li><code>ssl-bind</code> - Bind to this address only for HTTPS server, if not specified listen on all interfaces</li>
<li><code>ssl-key</code> - Path to SSL prvate key</li>
<li><code>ssl-cert</code> - Path to SSL certificate</li>
<li><code>ssl-pfx</code> - A string or Buffer containing the private key, certificate and CA certs of the server in PFX or PKCS12 format. (Mutually exclusive with the key, cert and ca options.)</li>
<li><code>ssl-ca</code> - An array of strings or Buffers of trusted certificates in PEM format. If this is omitted several well known root CAs will be used, like VeriSign. These are used to authorize connections.</li>
<li><code>ssl-passphrase</code> - A string of passphrase for the private key or pfx</li>
<li><code>ssl-crl</code> - Either a string or list of strings of PEM encoded CRLs (Certificate Revocation List)</li>
<li><code>ssl-ciphers</code> - A string describing the ciphers to use or exclude. Consult <a href="http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT">http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT</a> for details on the format</li>
<li><code>ssl-request-cert</code> - If true the server will request a certificate from clients that connect and attempt to verify that certificate. </li>
<li><code>timeout</code> - HTTP request idle timeout for servers in ms, how long to keep the connection socket open, this does not affect Long Poll requests. Default: 30000</li>
<li><code>daemon</code> - Daemonize the process, go to the background, can be specified only in the command line</li>
<li><code>shell</code> - Run command line shell, load the backend into the memory and prompt for the commands, can be specified only in the command line, no servers will be initialized, only the core and db modules</li>
<li><code>monitor</code> - For production use, monitor the master and Web server processes and restarts if crashed or exited, can be specified only in the command line</li>
<li><code>master</code> - Start the master server, can be specified only in the command line, this process handles job schedules and starts Web server, keeps track of failed processes and restarts them</li>
<li><code>proxy</code> - Start the HTTP proxy server, uses etc/proxy config file, can be specified only in the command line</li>
<li><code>proxy-port</code> - Proxy server port. Default: 8000</li>
<li><code>proxy-bind</code> - Proxy server listen address. Default: 0.0.0.0</li>
<li><code>local-db-pool</code> - Local database pool for properties, cookies and other instance specific stuff. Default: sqlite</li>
<li><code>web</code> - Start Web server processes, spawn workers that listen on the same port, without this flag no Web servers will be started by default</li>
<li><code>repl-port-web</code> - Web server REPL port, if specified it initializes REPL in the Web server process</li>
<li><code>repl-bind-web</code> - Web server REPL listen address. Default: 0.0.0.0</li>
<li><code>repl-port</code> - Port for REPL interface in the master, if specified it initializes REPL in the master server process</li>
<li><code>repl-bind</code> - Listen only on specified address for REPL server in the master process. Default: 0.0.0.0</li>
<li><code>repl-file</code> - User specified file for REPL history. Default: .history</li>
<li><code>lru-max</code> - Max number of items in the LRU cache, this cache is managed by the master Web server process and available to all Web processes maintaining only one copy per machine, Web proceses communicate with LRU cache via IPC mechanism between node processes. Default: 10000</li>
<li><code>no-msg</code> - Disable nanomsg messaging sockets</li>
<li><code>msg-port</code> - Ports to use for nanomsg sockets for message publish and subscribe, 2 ports will be used, this one and the next. Default: 20195</li>
<li><code>msg-type</code> - One of the redis or nanomsg to use for PUB/SUB messaging, default is nanomsg sockets</li>
<li><code>msg-host</code> - Server(s) where clients publish and subscribe messages using nanomsg sockets, IPs or hosts separated by comma, TCP port is optional, msg-port is used. DNS TXT configurable.</li>
<li><code>memcache-host</code> - List of memcached servers for cache messages: IP[:port],host[:port]... DNS TXT configurable.</li>
<li><code>memcache-options</code> - JSON object with options to the Memcached client, see npm doc memcached</li>
<li><code>redis-host</code> - Address to Redis server for cache messages. DNS TXT configurable.</li>
<li><code>redis-options</code> - JSON object with options to the Redis client, see npm doc redis</li>
<li><code>amqp-options</code> - JSON object with options to the AMQP client, see npm doc amqp</li>
<li><code>cache-type</code> - One of the redis or memcache to use for caching in API requests</li>
<li><code>cache-host</code> - Address of nanomsg cache servers, IPs or hosts separated by comma: IP:[port],host[:[port], if TCP port is not specified, cache-port is used. DNS TXT configurable.</li>
<li><code>cache-port</code> - Port to use for nanomsg sockets for cache requests. Default: 20194</li>
<li><code>no-cache</code> - Disable caching, all gets will result in miss and puts will have no effect</li>
<li><code>cache-sync</code> - Maintain all caches in sync when using nanomsg, put/incr commands will be broadcasted, otherwise only del commands</li>
<li><code>no-remote-config</code> - Disable any attempts to read config from supported remote destinations like DNS...</li>
<li><code>worker</code> - Set this process as a worker even it is actually a master, this skips some initializations</li>
<li><code>logwatcher-email</code> - Email address for the logwatcher notifications, the monitor process scans system and backend log files for errors and sends them to this email address, if not specified no log watching will happen. DNS TXT configurable.</li>
<li><code>logwatcher-from</code> - Email address to send logwatcher notifications from, for cases with strict mail servers accepting only from known addresses</li>
<li><code>logwatcher-ignore</code> - Regexp with patterns that needs to be ignored by logwatcher process, it is added to the list of ignored patterns. Default: NOTICE: ,DEBUG: ,DEV: </li>
<li><code>logwatcher-match</code> - Regexp patterns that match conditions for logwatcher notifications, this is in addition to default backend logger patterns. Default: [[0-9]+]: (ERROR|WARNING): ,message&quot;:&quot;ERROR:,queryAWS:.+Errors:</li>
<li><code>logwatcher-interval</code> - How often to check for errors in the log files. Default: 3600</li>
<li><code>logwatcher-file</code> - Add a file to be watched by the logwatcher, it will use all configured match patterns</li>
<li><code>user-agent</code> - Add HTTP user-agent header to be used in HTTP requests, for scrapers or other HTTP requests that need to be pretended coming from Web browsers. Default: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:18.0) Gecko/20100101 Firefox/18.0,Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0,Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:20.0) Gecko/20100101 Firefox/20.0,Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.29.13 (KHTML, like Gecko) Version/6.0.4 Safari/536.29.13,Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.65 Safari/537.31,Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.34 (KHTML, like Gecko) Safari/534.34,Opera/9.80 (Macintosh; Intel Mac OS X 10.7.5) Presto/2.12.388 Version/12.15,Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0,Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/6.0; SLCC2; .NET CLR 2.0.50727,Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/6.0; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.2; BRI/2</li>
<li><code>backend-host</code> - Host of the master backend, can be used for backend nodes communications using core.sendRequest function calls with relative URLs, also used in tests.</li>
<li><code>backend-login</code> - Credentials login for the master backend access</li>
<li><code>backend-secret</code> - Credentials secret for the master backend access</li>
<li><code>domain</code> - Domain to use for communications, default is current domain of the host machine</li>
<li><code>config-domain</code> - Domain to query for configuration TXT records, default is current domain of the host machine</li>
<li><code>max-distance</code> - Max searchable distance(radius) in km, for location searches to limit the upper bound. Default: 50</li>
<li><code>min-distance</code> - Radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. Default: 5</li>
<li><code>instance</code> - Enables instance mode, it means the backend is running in the cloud to execute a job or other task and can be terminated during the idle timeout</li>
<li><code>backtrace</code> - Enable backtrace facility, trap crashes and report the backtrace stack</li>
<li><code>watch</code> - Watch sources directory for file changes to restart the server, for development, the backend module files will be added to the watch list automatically, so only app specific directores should be added</li>
<li><code>db-pool</code> - Default pool to be used for db access without explicit pool specified. Default: sqlite</li>
<li><code>db-no-pools</code> - Do not use other db pools except default sqlite</li>
<li><code>db-sqlite-max</code> - Max number of open connection for the pool</li>
<li><code>db-sqlite-idle</code> - Number of ms for a connection to be idle before being destroyed</li>
<li><code>db-sqlite-tables</code> - Sqlite tables, list of tables that belong to this pool only</li>
<li><code>db-pgsql-pool</code> - PostgreSQL pool access url or options string</li>
<li><code>db-pgsql-max</code> - Max number of open connection for the pool</li>
<li><code>db-pgsql-idle</code> - Number of ms for a connection to be idle before being destroyed</li>
<li><code>db-pgsql-tables</code> - PostgreSQL tables, list of tables that belong to this pool only</li>
<li><code>db-mysql-pool</code> - MySQL pool access url in the format: mysql://user:pass@host/db</li>
<li><code>db-mysql-max</code> - Max number of open connection for the pool</li>
<li><code>db-mysql-idle</code> - Number of ms for a connection to be idle before being destroyed</li>
<li><code>db-mysql-tables</code> - PostgreSQL tables, list of tables that belong to this pool only</li>
<li><code>db-dynamodb-pool</code> - DynamoDB endpoint url</li>
<li><code>db-dynamodb-tables</code> - DynamoDB tables, list of tables that belong to this pool only</li>
<li><code>db-mongodb-pool</code> - MongoDB endpoint url</li>
<li><code>db-mongodb-tables</code> - MongoDB tables, list of tables that belong to this pool only</li>
<li><code>db-cassandra-pool</code> - Casandra endpoint url</li>
<li><code>db-cassandra-max</code> - Max number of open connection for the pool</li>
<li><code>db-cassandra-idle</code> - Number of ms for a connection to be idle before being destroyed</li>
<li><code>db-cassandra-tables</code> - DynamoDB tables, list of tables that belong to this pool only</li>
<li><code>aws-key</code> - AWS access key</li>
<li><code>aws-secret</code> - AWS access secret</li>
<li><code>aws-region</code> - AWS region. Default: us-east-1</li>
<li><code>aws-keypair</code> - AWS instance keypair name</li>
<li><code>aws-image-id</code> - AWS image id to be used for remote jobs</li>
<li><code>aws-instance-type</code> - AWS instance type. Default: t1.micro</li>
<li><code>aws-dynamodb-host</code> - Custom DynamoDB host for local installations</li>
<li><code>aws-nometadata</code> - Skip retrieval from instance metadata</li>
<li><code>api-images-url</code> - URL where images are stored, for cases of central image server(s)</li>
<li><code>api-images-s3</code> - S3 bucket name where to store images</li>
<li><code>api-files-s3</code> - S3 bucket name where to store files</li>
<li><code>api-busy-latency</code> - Max time in ms for a request to wait in the queue, if exceeds this value server returns too busy error. Default: 1000</li>
<li><code>api-access-log</code> - File for access logging</li>
<li><code>api-templating</code> - Templating engne to use, see consolidate.js for supported engines, default is ejs</li>
<li><code>api-session-age</code> - Session age in milliseconds, for cookie based authentication. Default: 1209600000</li>
<li><code>api-session-secret</code> - Secret for session cookies, session support enabled only if it is not empty</li>
<li><code>api-data-endpoint-unsecure</code> - Allow the Data API functions to retrieve and show all columns, not just public, this exposes the database to every authenticated call, use with caution</li>
<li><code>api-caching</code> - List of tables that can be cached: bk_auth, bk_counter. This list defines which DB calls will cache data with whatever cache configured. Default: </li>
<li><code>api-disable</code> - Disable default API by endpoint name: account, message, icon...... Default: </li>
<li><code>api-disable-session</code> - Disable access to API endpoints for Web sessions, must be signed properly. Default: </li>
<li><code>api-allow-admin</code> - URLs which can be accessed by admin accounts only, can be partial urls or Regexp, thisis a convenient options which registers AuthCheck callback for the given endpoints</li>
<li><code>api-allow</code> - Regexp for URLs that dont need credentials, replace the whole access list. Default: ^/$,.html$,.(ico|gif|png|jpg|svg)$,.(ttf|eof|woff)$,.(js|css)$,^/public,^/account/add$</li>
<li><code>api-allow-path</code> - Add to the list of allowed URL paths without authentication</li>
<li><code>api-disallow-path</code> - Remove from the list of allowed URL paths that dont need authentication, most common case is to to remove ^/account/add$ to disable open registration</li>
<li><code>api-allow-ssl</code> - Add to the list of allowed URL paths using HTRPs only, plain HTTP requetss to these urls will be refused</li>
<li><code>api-mime-body</code> - Collect full request body in the req.body property for the given MIME type in addition to json and form posts, this is for custom body processing. Default: </li>
<li><code>api-deny</code> - Regexp for URLs that will be denied access, replaces the whole access list</li>
<li><code>api-deny-path</code> - Add to the list of URL paths to be denied without authentication</li>
<li><code>api-subscribe-timeout</code> - Timeout for Long POLL subscribe listener, how long to wait for events, milliseconds. Default: 600000</li>
<li><code>api-subscribe-interval</code> - Interval between delivering events to subscribed clients, milliseconds. Default: 5000</li>
<li><code>api-upload-limit</code> - Max size for uploads, bytes. Default: 10485760</li>
<li><code>server-max-processes</code> - Max processes to launch for servers. Default: 1</li>
<li><code>server-max-workers</code> - Max number of worker processes to launch for jobs. Default: 1</li>
<li><code>server-idle-time</code> - If set and no jobs are submitted the backend will be shutdown, for instance mode only. Default: 120</li>
<li><code>server-crash-delay</code> - Delay between respawing the crashed process. Default: 30000</li>
<li><code>server-restart-delay</code> - Delay between respawning the server after changes. Default: 1000</li>
<li><code>server-job</code> - Job specification, JSON encoded as base64 of the job object</li>
<li><code>server-jobs-tag</code> - This server executes jobs that match this tag, cannot be empty, default is current hostname. Default: Vlad-Macbook</li>
<li><code>server-max-jobs</code> - How many jobs to execute at any iteration, this relates to the bk_jobs queue only. Default: 1</li>
<li><code>server-jobs-interval</code> - Interval between executing job queue, 0 disables job processing, min interval is 60 secs. Default: 180000</li>
</ul>
<h2 id="module-api">Module: API</h2>
<ul>
<li><p><code>api</code></p>
<p> HTTP API to the server from the clients, this module implements the basic HTTP(S) API functionality with some common features. The API module
incorporates the Express server which is exposed as api.app object, the master server spawns Web workers which perform actual operations and monitors
the worker processes if they die and restart them automatically. How many processes to spawn can be configured via <code>-server-max-workers</code> config parameter.</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Authentication by login, only keeps id and secret to check the siganture
      bk_auth: { login: { primary: 1 },               // Account login
                 id: {},                              // Auto generated UUID
                 secret: {},                          // Account password
                 type: {},                            // Account type: admin, ....
                 acl_deny: {},                        // Deny access to matched url
                 acl_allow: {},                       // Only grant access if matched this regexp
                 expires: { type: &quot;bigint&quot; },         // Deny access to the account if this value is before current date, milliseconds
                 mtime: { type: &quot;bigint&quot;, now: 1 } },

      // Basic account information
      bk_account: { id: { primary: 1, pub: 1 },
                    login: {},
                    name: {},
                    alias: { pub: 1 },
                    status: {},
                    email: {},
                    phone: {},
                    website: {},
                    birthday: {},
                    gender: {},
                    address: {},
                    city: {},
                    state: {},
                    zipcode: {},
                    country: {},
                    latitude: { type: &quot;real&quot;, noadd: 1 },
                    longitude: { type: &quot;real&quot;, noadd: 1 },
                    geohash: { noadd: 1 },
                    location: { noadd: 1 },
                    ltime: { type: &quot;bigint&quot;, noadd: 1 },
                    ctime: { type: &quot;bigint&quot; },
                    mtime: { type: &quot;bigint&quot;, now: 1 } },

     // Keep track of icons uploaded
     bk_icon: { id: { primary: 1, pub: 1 },                 // Account id
                type: { primary: 1, pub: 1 },               // prefix:type
                acl_allow: {},                              // Who can see it: all, auth, id:id...
                descr: {},
                latitude: { type: &quot;real&quot; },
                longitude: { type: &quot;real&quot; },
                geohash: {},
                mtime: { type: &quot;bigint&quot;, now: 1 }},         // Last time added/updated

     // Locations for all accounts to support distance searches
     bk_location: { geohash: { primary: 1, semipub: 1 },        // geohash, minDistance defines the size
                    id: { primary: 1, pub: 1 },                 // my account id, part of the primary key for pagination
                    latitude: { type: &quot;real&quot;, semipub: 1 },     // for distance must be semipub or no distance and no coordinates
                    longitude: { type: &quot;real&quot;, semipub: 1 },
                    mtime: { type: &quot;bigint&quot;, now: 1 }},

     // All connections between accounts: like,dislike,friend...
     bk_connection: { id: { primary: 1 },                    // my account_id
                      type: { primary: 1 },                  // type:connection_id
                      state: {},
                      mtime: { type: &quot;bigint&quot;, now: 1 }},

     // References from other accounts, likes,dislikes...
     bk_reference: { id: { primary: 1 },                    // connection_id
                     type: { primary: 1 },                  // type:account_id
                     state: {},
                     mtime: { type: &quot;bigint&quot;, now: 1 }},

     // Messages between accounts
     bk_message: { id: { primary: 1, index: 1, index1: 1 },  // my account_id
                   mtime: { primary: 1 },                    // mtime:sender, the current timestamp in milliseconds and the sender
                   status: { index: 1 },                     // status: R:mtime:sender or N:mtime:sender, where R - read, N - new
                   sender: { index1: 1 },                    // sender:mtime, reverse index by sender
                   msg: { type: &quot;text&quot; },                    // Text of the message
                   icon: {}},                                // Icon base64 or url

     // All accumulated counters for accounts
     bk_counter: { id: { primary: 1, pub: 1 },                               // account id
                   ping: { type: &quot;counter&quot;, value: 0, pub: 1 },              // public column to ping the buddy
                   like0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },            // who i liked
                   like1: { type: &quot;counter&quot;, value: 0 },                     // reversed, who liked me
                   dislike0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },
                   dislike1: { type: &quot;counter&quot;, value: 0 },
                   follow0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },
                   follow1: { type: &quot;counter&quot;, value: 0, },
                   invite0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },
                   invite1: { type: &quot;counter&quot;, value: 0, },
                   view0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },
                   view1: { type: &quot;counter&quot;, value: 0, },
                   msg_count: { type: &quot;counter&quot;, value: 0 },                  // total msgs received
                   msg_read: { type: &quot;counter&quot;, value: 0 }},                  // total msgs read

     // Keep historic data about account activity
     bk_history: { id: { primary: 1 },
                   mtime: { type: &quot;bigint&quot;, primary: 1, now: 1 },
                   type: {},
                   data: {} }
</code></pre></li>
</ul>

<ul>
<li><p><code>api.init(callback)</code></p>
<p> Initialize API layer, this must be called before the <code>api</code> module can be used but it is called by the server module automatically so <code>api.init</code> is
rearely need to called directly, only for new server implementation or if using in the shell for testing.</p>
<p>During the init sequence, this function calls <code>api.initMiddleware</code> and <code>api.initApplication</code> methods which by default are empty but can be redefined in the user aplications.</p>
<p>The backend.js uses its own request parser that places query parameters into <code>req.query</code> or <code>req.body</code> depending on the method.</p>
<p>For GET method, <code>req.query</code> contains all url-encoded parameters, for POST method <code>req.body</code> contains url-encoded parameters or parsed JSON payload or multipart payload.</p>
<p>The simple way of dealing transparently with this is to check for method in the route handler like this:</p>
<pre><code>if (req.method == &quot;POST&quot;) req.query = req.body;
</code></pre><p>The reason not to do this by default is that this may not be the alwayse wanted case and distinguishing data coming in the request or in the body may be desirable,
also, this will needed only for Express handlers <code>.all</code>, when registering handler by method like <code>.get</code> or <code>.post</code> then the handler needs to deal with only either source of the request data.</p>
</li>
</ul>

<ul>
<li><p><code>api.initApplication(callback)</code></p>
<p> This handler is called after the Express server has been setup and all default API endpoints initialized but the server
is not ready for incoming requests yet. This handler can setup additional API endpoints, add/modify table descriptions.</p>
</li>
</ul>

<ul>
<li><p><code>api.initMiddleware()</code></p>
<p> This handler is called during the Express server initialization just after the security middleware.
this.app refers to the Express instance.</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRequest(req, res, callback)</code></p>
<p> Perform authorization of the incoming request for access and permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.checkQuery(req, res, next)</code></p>
<p> Parse incoming query parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkBody(req, res, next)</code></p>
<p> Parse multipart forms for uploaded files</p>
</li>
</ul>

<ul>
<li><p><code>api.checkAccess(req, callback)</code></p>
<p> Perform URL based access checks
Check access permissions, calls the callback with the following argument:</p>
<ul>
<li>nothing if checkSignature needs to be called</li>
<li>an object with status: 200 to skip authorization and proceed with the next module</li>
<li>an object with status: 0 means response has been sent, just stop</li>
<li>an object with status other than 0 or 200 to return the status and stop request processing</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkAuthorization(req, status, callback)</code></p>
<p> Perform authorization checks after the account been checked for valid signature, this is called even if the signature verification failed</p>
<ul>
<li>req is Express request object</li>
<li>status contains the signature verification status, an object wth status: and message: properties</li>
<li>callback is a function(req, status) to be called with the resulted status where status must be an object with status and message properties as well</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkSignature(req, callback)</code></p>
<p> Verify request signature from the request object, uses properties: .host, .method, .url or .originalUrl, .headers</p>
</li>
</ul>

<ul>
<li><p><code>api.initAccountAPI()</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>api.initIconAPI()</code></p>
<p> Generic icon management</p>
</li>
</ul>

<ul>
<li><p><code>api.initMessageAPI()</code></p>
<p> Messaging management</p>
</li>
</ul>

<ul>
<li><p><code>api.initHistoryAPI()</code></p>
<p> History management</p>
</li>
</ul>

<ul>
<li><p><code>api.initCounterAPI()</code></p>
<p> Counters management</p>
</li>
</ul>

<ul>
<li><p><code>api.initConnectionAPI()</code></p>
<p> Connections management</p>
</li>
</ul>

<ul>
<li><p><code>api.initLocationAPI()</code></p>
<p> Geo locations management</p>
</li>
</ul>

<ul>
<li><p><code>api.initSystemAPI()</code></p>
<p> API for internal provisioning and configuration</p>
</li>
</ul>

<ul>
<li><p><code>api.initDataAPI()</code></p>
<p> API for full access to all tables</p>
</li>
</ul>

<ul>
<li><p><code>api.initTables(callback)</code></p>
<p> Called in the master process to create/upgrade API related tables</p>
</li>
</ul>

<ul>
<li><p><code>api.getOptions(req)</code></p>
<p> Convert query options into database options, most options are the same as for <code>db.select</code> but prepended with underscore to
distinguish control parameters from query parameters.</p>
</li>
</ul>

<ul>
<li><p><code>api.describeTables(tables)</code></p>
<p> Define new tables or extned/customize existing tables. Table definitions are used with every database operation,
on startup, the backend read all existing table columns from the database and cache them in the memory but some properties
like public columns are only specific to the backend so to mark such columns the table with such properties must be described
using this method. Only columns with changed properties need to be specified, other columns will be left as it is.</p>
<p>Example</p>
<pre><code>    api.describeTables({ bk_account: { name: { pub: 1 } },

                         test: { id: { primary: 1, type: &quot;int&quot; },
                                 name: { pub: 1, index: 1 } });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.clearQuery(req, options, table, name)</code></p>
<p> Clear request query properties specified in the table definition, if any columns for the table contains the property <code>name</code> nonempty, then
all request properties with the same name as this column name will be removed from the query. This for example is used for the <code>bk_account</code>
table to disable updating location related columns because speial location API maintains location data and updates the accounts table.</p>
</li>
</ul>

<ul>
<li><p><code>api.findHook(type, method, path)</code></p>
<p> Find registered hook for given type and path</p>
</li>
</ul>

<ul>
<li><p><code>api.registerAccessCheck(method, path, callback)</code></p>
<p> Register a handler to check access for any given endpoint, it works the same way as the global accessCheck function and is called before
validating the signature or session cookies.</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, cb) {}, to indicate an error condition pass an object
with the callback with status: and message: properties, status != 200 means error</li>
</ul>
<p>Example:</p>
<pre><code>    api.registerAccessCheck(&#39;&#39;, &#39;account&#39;, function(req, cb) { cb({status:500,message:&quot;access disabled&quot;}) }))

    api.registerAccessCheck(&#39;POST&#39;, &#39;/account/add&#39;, function(req, cb) {
       if (!req.query.invitecode) return cb({ status: 400, message: &quot;invitation code is required&quot; });
       cb();
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerAuthCheck(method, path, callback)</code></p>
<p> Similar to <code>registerAccessCheck</code> but this callback will be called after the signature or session is verified.
The purpose of this hook is too check permissions of a valid user to resources or in case of error perform any other action
like redirection or returning something explaining what to do in case of failure. The callback for this call is different then in <code>checkAccess</code> hooks.</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similr to registering Express routes</li>
<li>callback is a function(req, status, cb) where status is an object { status:..., message: ..} passed from the checkSignature call, if status != 200 it means
an error condition, the callback must pass the same or modified status object in its own <code>cb</code> callback</li>
</ul>
<p>Example:</p>
<pre><code>     api.registerAuthCheck(&#39;GET&#39;, &#39;/account/get&#39;, function(req, status, cb) {
          if (status.status != 200) status = { status: 302, url: &#39;/error.html&#39; };
          cb(status)
     });
</code></pre><p>Example with admin access only:</p>
<pre><code>    api.registerAccessCheck(&#39;POST&#39;, &#39;/data/&#39;, function(req, cb) {
        if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &quot;access denied, admins only&quot; });
        cb();
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerPostProcess(method, path, callback)</code></p>
<p> Register a callback to be called after successfull API action, status 200 only.
The purpose is to perform some additional actions after the standard API completed or to customize the result</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, res, rows) where rows is the result returned by the API handler,
the callback MUST return data back to the client or any other status code</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendJSON(req, res, rows)</code></p>
<p> Send result back with possibly executing post-process callback, this is used by all API handlers to allow custom post processing in teh apps</p>
</li>
</ul>

<ul>
<li><p><code>api.sendReply(res, status, msg)</code></p>
<p> Send formatted JSON reply to API client, if status is an instance of Error then error message with status 500 is sent back</p>
</li>
</ul>

<ul>
<li><p><code>api.sendStatus(res, options)</code></p>
<p> Return reply to the client using the options object, it cantains the following properties:</p>
<ul>
<li>status - defines the respone status code</li>
<li>message  - property to be sent as status line and in the body</li>
<li>type - defines Content-Type header, the message will be sent in the body</li>
<li>url - for redirects when status is 301 or 302</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendFile(req, res, file, redirect)</code></p>
<p> Send file back to the client, res is Express response object</p>
</li>
</ul>

<ul>
<li><p><code>api.incrCounters(req, options, callback)</code></p>
<p> Increase a counter, used in /counter/incr API call, options.op can be set to &#39;put&#39;</p>
</li>
</ul>

<ul>
<li><p><code>api.getConnections(req, options, callback)</code></p>
<p> Return all connections for the current account, this function is called by the <code>/connection/get</code> API call.</p>
</li>
</ul>

<ul>
<li><p><code>api.putConnections(req, options, callback)</code></p>
<p> Create a connection between 2 accounts, this function is called by the <code>/connection/add</code> API call.</p>
</li>
</ul>

<ul>
<li><p><code>api.delConnections(req, options, callback)</code></p>
<p> Delete a connection, this function is called by the <code>/connection/del</code> API call</p>
</li>
</ul>

<ul>
<li><p><code>api.getLocations(req, options, callback)</code></p>
<p> Perform locations search, request comes from the Express server, callback will takes err and data to be returned back to the client, this function
is used in <code>/location/get</code> request. It can be used in the applications with customized input and output if neccesary for the application specific logic.</p>
<p>Example</p>
<pre><code>    # Request will look like: /recent/locations?latitude=34.1&amp;longitude=-118.1&amp;mtime=123456789
    this.app.all(/^\/recent\/locations$/, function(req, res) {
        var options = self.getOptions(req);
        options.keys = [&quot;geohash&quot;,&quot;mtime&quot;];
        options.ops = { mtime: &#39;gt&#39; };
        options.details = true;
        self.getLocations(req, options, function(err, data) {
            self.sendJSON(req, res, data);
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.putLocations(req, options, callback)</code></p>
<p> Save locstion coordinates for current account, this function is called by the <code>/location/put</code> API call</p>
</li>
</ul>

<ul>
<li><p><code>api.handleIcon(req, res, options)</code></p>
<p> Process icon request, put or del, update table and deal with the actual image data, always overwrite the icon file</p>
</li>
</ul>

<ul>
<li><p><code>api.formatIcon(row, account)</code></p>
<p> Return formatted icon URL for the given account</p>
</li>
</ul>

<ul>
<li><p><code>api.getIcon(req, res, id, options)</code></p>
<p> Return icon to the client, checks the bk_icon table for existence and permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.sendIcon(req, res, id, options)</code></p>
<p> Send an icon to the client, only handles files</p>
</li>
</ul>

<ul>
<li><p><code>api.checkIcon(req, id, row)</code></p>
<p> Verify icon permissions for given account id, returns true if allowed</p>
</li>
</ul>

<ul>
<li><p><code>api.putIcon(req, id, options, callback)</code></p>
<p> Store an icon for account, .type defines icon prefix</p>
</li>
</ul>

<ul>
<li><p><code>api.storeIcon(icon, id, options, callback)</code></p>
<p> Place the icon data to the destination</p>
</li>
</ul>

<ul>
<li><p><code>api.delIcon(id, options, callback)</code></p>
<p> Delete an icon for account, .type defines icon prefix</p>
</li>
</ul>

<ul>
<li><p><code>api.putIconS3(file, id, options, callback)</code></p>
<p> Same as putIcon but store the icon in the S3 bucket, icon can be a file or a buffer with image data</p>
</li>
</ul>

<ul>
<li><p><code>api.putFile(req, name, options, callback)</code></p>
<p> Upload file and store in the filesystem or S3, try to find the file in multipart form, in the body or query by the given name</p>
<ul>
<li>name is the name property to look for in the multipart body or in the request body or query</li>
<li>callback will be called with err and actual filename saved
Output file name is built according to the following options properties:</li>
<li>name - defines the basename for the file, no extention, if not given same name as property will be used</li>
<li>ext - what file extention to use, appended to name, if no ext is given the extension from the uploaded file will be used or no extention if could not determine one.</li>
<li>extkeep - tells always to keep actual extention from the uploaded file</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.storeFile(tmpfile, outfile, options, callback)</code></p>
<p> Place the uploaded tmpfile to the destination pointed by outfile</p>
</li>
</ul>

<ul>
<li><p><code>api.deleteFile(file, options, callback)</code></p>
<p> Delete file by name</p>
</li>
</ul>

<ul>
<li><p><code>api.addMessage(req, options, callback)</code></p>
<p> Add new message, used in /message/add API call</p>
</li>
</ul>

<ul>
<li><p><code>api.delMessages(req, options, callback)</code></p>
<p> Delete a message or all messages for the given account from the given sender, used in /messge/del` API call</p>
</li>
</ul>

<ul>
<li><p><code>api.addAccount(req, options, callback)</code></p>
<p> Register new account, used in /account/add API call</p>
</li>
</ul>

<ul>
<li><p><code>api.updateAccount(req, options, callback)</code></p>
<p> Update existing account, used in /account/update API call</p>
</li>
</ul>

<ul>
<li><p><code>api.deleteAccount(obj, options, callback)</code></p>
<p> Delete account specified by the obj. Used in <code>/account/del</code> API call.
The options may contain keep: {} object with table names to be kept without the bk_ prefix, for example
delete an account but keep all messages and location: keep: { message: 1, location: 1 }</p>
</li>
</ul>

<ul>
<li><p><code>api.getStatistics()</code></p>
<p> Returns an object with collected db and api statstics and metrics</p>
</li>
</ul>

<ul>
<li><p><code>api.collectStatistics()</code></p>
<p> Metrics about the process</p>
</li>
</ul>

<h2 id="module-aws">Module: AWS</h2>
<ul>
<li><p><code>aws.queryAWS(proto, method, host, path, obj, callback)</code></p>
<p> Make AWS request, return parsed response as Javascript object or null in case of error</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryEC2(action, obj, callback)</code></p>
<p> AWS EC2 API parameters</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySign(service, host, method, path, body, headers)</code></p>
<p> Build version 4 signature headers</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryDDB (action, obj, options, callback)</code></p>
<p> DynamoDB requests</p>
</li>
</ul>

<ul>
<li><p><code>aws.signS3(method, bucket, key, query, headers, expires)</code></p>
<p> Sign S3 AWS request, returns url to be send to S3 server, options will have all updated headers to be sent as well</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryS3(bucket, key, options, callback)</code></p>
<p> S3 requests
Options may contain the following properties:</p>
<ul>
<li>method - HTTP method</li>
<li>query - query parameters for the url as an object</li>
<li>postdata - any data to be sent with POST</li>
<li>postfile - file to be uploaded to S3 bucket</li>
<li>expires - absolute time when this request is expires</li>
<li>headers - HTTP headers to be sent with request</li>
<li>file - file name where to save downloaded contents</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.runInstances(count, args, callback)</code></p>
<p> Run AWS instances with given arguments in user-data</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceMeta(path, callback)</code></p>
<p> Retrieve instance meta data</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceInfo(callback)</code></p>
<p> Retrieve instance launch index from the meta data if running on AWS instance</p>
</li>
</ul>

<ul>
<li><p><code>aws.toDynamoDB(value, level)</code></p>
<p> Convert a Javascript object into DynamoDB object</p>
</li>
</ul>

<ul>
<li><p><code>aws.fromDynamoDB(value)</code></p>
<p> Convert a DynamoDB object into Javascript object</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryFilter(obj, options)</code></p>
<p> Build query or scan filter objects for the given object, all properties in the obj are used</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbListTables(options, callback)</code></p>
<p> Return list of tables in .TableNames property of the result
Example:</p>
<pre><code>    { TableNames: [ name, ...] }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbDescribeTable(name, options, callback)</code></p>
<p> Return table definition and parameters in the result structure with property of the given table name
Example:</p>
<pre><code>    { name: { AttributeDefinitions: [], KeySchema: [] ...} }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbCreateTable(name, attrs, keys, options, callback)</code></p>
<p> Create a table</p>
<ul>
<li>attrs can be an array in native DDB JSON format or an object with name:type properties, type is one of S, N, NN, NS, BS</li>
<li>keys can be an array in native DDB JSON format or an object with name:keytype properties, keytype is one of HASH or RANGE value in the same format as for primary keys</li>
<li>options may contain any valid native property if it starts with capital letter and the following:<ul>
<li>local - an object with each property for a local secondary index name defining key format the same way as for primary keys, all Uppercase properties are added to the top index object</li>
<li>global - an object for global secondary indexes, same format as for local indexes</li>
<li>projection - an object with index name and list of projected properties to be included in the index or &quot;ALL&quot; for all properties, if omitted then default KEYS_ONLY is assumed
Example:<pre><code>ddbCreateTable(&#39;users&#39;, { id:&#39;S&#39;,mtime:&#39;N&#39;,name:&#39;S&#39;},
                        { id:&#39;HASH&#39;,name:&#39;RANGE&#39;},
                        { local: { mtime: { mtime: &quot;HASH&quot; } },
                          global: { name: { name: &#39;HASH&#39;, ProvisionedThroughput: { ReadCapacityUnits: 50 } } },
                          projection: { mtime: [&#39;gender&#39;,&#39;age&#39;],
                                        name: [&#39;name&#39;,&#39;gender&#39;] },
                          ReadCapacityUnits: 10,
                          WriteCapacityUnits: 10 });
</code></pre></li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbGetItem(name, keys, options, callback)</code></p>
<p> Retrieve one item by primary key</p>
<ul>
<li>keys - an object with primary key attributes name and value.</li>
<li>select - list of columns to return, otherwise all columns will be returned</li>
<li>options may contain any native property allowed in the request or special properties:<ul>
<li>consistent - set consistency level for the request
Example:
  ddbGetItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39; })</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbPutItem(name, item, options, callback)</code></p>
<p> Put or add an item</p>
<ul>
<li>item is an object, type will be inferred from the native js type.</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>expected - an object with column names to be used in Expected clause and value as null to set condition to { Exists: false } or
any other exact value to be checked against which corresponds to { Exists: true, Value: value }
Example:
  ddbPutItem(&quot;users&quot;, { id: 1, name: &quot;john&quot;, mtime: 11233434 }, { expected: { name: null } })</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbUpdateItem(name, keys, item, options, callback)</code></p>
<p> Update an item</p>
<ul>
<li>keys is an object with primary key attributes name and value.</li>
<li>item is an object with properties where value can be:<ul>
<li>number/string/array - action PUT, replace or add new value</li>
<li>null - action DELETE</li>
</ul>
</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>ops - an object with operators to be used for properties if other than PUT</li>
<li>expected - an object with column names to be used in Expected clause and value as null to set condition to { Exists: false } or
any other exact value to be checked against which corresponds to { Exists: true, Value: value }
Example:
  ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39; }, { op: { icons: &#39;ADD&#39; }, expected: { id: 1 } })</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbDeleteItem(name, keys, options, callback)</code></p>
<p> Delete an item from a table</p>
<ul>
<li>keys is an object with name: value for hash/range attributes</li>
<li>options may contain any valid native property if it starts with capital letter.
Example:<pre><code>  ddbDeleteItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, {})
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbBatchWriteItem(items, options, callback)</code></p>
<p> Update items from the list at the same time</p>
<ul>
<li>items is a list of objects with table name as property and list of operations, an operation can be PutRequest or DeleteRequest</li>
<li>options may contain any valid native property if it starts with capital letter.
Example:<pre><code>  { table: [ { PutRequest: { id: 1, name: &quot;tt&quot; } }, ] }
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbBatchGetItem(items, options, callback)</code></p>
<p> Retrieve all items for given list of keys</p>
<ul>
<li>items is an object with table name as property name and list of options for GetItem request</li>
<li>options may contain any valid native property if it starts with capital letter.
Example:<pre><code>  { users: { keys: [{ id: 1, name: &quot;john&quot; },{ id: .., name: .. }], select: [&#39;name&#39;,&#39;id&#39;], consistent: true }, ... }
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbQueryTable(name, condition, options, callback)</code></p>
<p> Query on a table, return all matching items</p>
<ul>
<li>condition is an object with name: value pairs, by default EQ opeartor is used for comparison</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key when paginating, can be a string/number for hash or an object with hash/range properties</li>
<li>consistent - set consistency level for the request</li>
<li>select - list of attributes to get only</li>
<li>total - return number of matching records</li>
<li>count - limit number of record in result</li>
<li>desc - descending order</li>
<li>sort - index name to use, indexes are named the same as the corresponding column, with index primary keys for Keycondition will be used</li>
<li>ops - an object with operators to be used for properties if other than EQ.</li>
<li>keys - list of primary key columns, if there are other properties in the condition then they will be
 put into QueryFilter instead of KeyConditions. If keys is absent, all properties in the condition are treated as primary keys.
Example:
  ddbQueryTable(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39;, ops: { name: &#39;gt&#39; } })</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbScanTable(name, condition, options, callback)</code></p>
<p> Scan a table for all matching items</p>
<ul>
<li>condition is an object with name: value pairs</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key</li>
<li>ops - an object with operators to be used for properties if other than EQ.
Example:
 ddbScanTable(&quot;users&quot;, { id: 1, name: &#39;a&#39; }, { ops: { name: &#39;gt&#39; }})</li>
</ul>
</li>
</ul>
</li>
</ul>

<h2 id="module-core">Module: CORE</h2>
<ul>
<li><p><code>core</code></p>
<p> The primary object containing all config options and common functions</p>
</li>
</ul>

<ul>
<li><p><code>core.init(callback)</code></p>
<p> Main initialization, must be called prior to perform any actions</p>
</li>
</ul>

<ul>
<li><p><code>core.postInit(callback)</code></p>
<p> Called after the core.init has been initialized successfully, this can be redefined in the applications to add additional
init steps that all processes require to have.</p>
</li>
</ul>

<ul>
<li><p><code>core.run(callback)</code></p>
<p> Run any backend function after environment has been initialized, this is to be used in shell scripts,
core.init will parse all command line arguments, the simplest case to run from /data directory and it will use
default environment or pass -home dir so the script will reuse same config and paths as the server
context can be specified for the callback, if no then it run in the core context</p>
<ul>
<li>require(&#39;backendjs&#39;).run(function() {}) is one example where this call is used as a shortcut for ad-hoc scripting</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.setHome(home)</code></p>
<p> Switch to new home directory, exit if we cannot, this is important for relative paths to work if used,
no need to do this in worker because we already switched to home directory in the master and all child processes
inherit current directory
Important note: If run with combined server or as a daemon then this MUST be an absolute path, otherwise calling
it in the spawned web master will fail due to the fact that we already set the home and relative path will not work after that.</p>
</li>
</ul>

<ul>
<li><p><code>core.parseArgs(argv)</code></p>
<p> Parse command line arguments</p>
</li>
</ul>

<ul>
<li><p><code>core.processArgs(name, ctx, argv, pass)</code></p>
<p> Config parameters defined in a module as a list of parameter names prefixed with module name, a parameters can be
a string which defines text parameter or an object with the properties: name, type, value, decimals, min, max, separator
type can be bool, number, list, json</p>
</li>
</ul>

<ul>
<li><p><code>core.showHelp(options)</code></p>
<p> Print help about command line arguments and exit</p>
</li>
</ul>

<ul>
<li><p><code>core.loadConfig(file, callback)</code></p>
<p> Parse the config file, configFile can point to a file or can be skipped and the default file will be loaded</p>
</li>
</ul>

<ul>
<li><p><code>core.retrieveConfig(callback)</code></p>
<p> Retrieve config parameters from the network, dns...</p>
</li>
</ul>

<ul>
<li><p><code>core.initServerCaching()</code></p>
<p> Initialize caching system for the configured cache type, can be called many time to re-initialize if the environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>core.initServerMessaging()</code></p>
<p> Initialize  messaging system for the server process, can be called multiple times in case environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>core.initClientMessaging()</code></p>
<p> Initialize web worker messaging system, client part, sends all publish messages to this socket which will be broadcasted into the
publish socket by the receiving end. Can be called anytime to reconfigure if the environment has changed.</p>
</li>
</ul>

<ul>
<li><p><code>core.initClientCaching()</code></p>
<p> Initialize web worker caching system, can be called anytime the environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>core.ipcSend(cmd, key, value, callback)</code></p>
<p> Send cache command to the master process via IPC messages, callback is used for commands that return value back</p>
</li>
</ul>

<ul>
<li><p><code>core.ipcConfigure(type)</code></p>
<p> Reconfigure the caching and/or messaging in the client and server, sends init command to the master which will reconfigure all workers after
reconfiguring itself, this command can be sent by any worker. type can be one of cche or msg</p>
</li>
</ul>

<ul>
<li><p><code>core.ipcSubscribe(key, callback)</code></p>
<p> Subscribe to the publishing server for messages starting with the given key, the callback will be called only on new data received
Returns a non-zero handle which must be unsubscribed when not needed. If no pubsub system is available or error occurred returns 0.</p>
</li>
</ul>

<ul>
<li><p><code>core.ipcUnsubscribe(sock, key)</code></p>
<p> Close subscription</p>
</li>
</ul>

<ul>
<li><p><code>core.ipcPublish(key, data)</code></p>
<p> Publish an event to be sent to the subscribed clients</p>
</li>
</ul>

<ul>
<li><p><code>core.encodeURIComponent(str)</code></p>
<p> Encode with additional symbols</p>
</li>
</ul>

<ul>
<li><p><code>core.processId()</code></p>
<p> Return unique process id based on the cluster status, worker or master and the role. This is can be reused by other workers within the role thus
making it usable for repeating environments or storage solutions.</p>
</li>
</ul>

<ul>
<li><p><code>core.toTitle(name)</code></p>
<p> Convert text into capitalized words</p>
</li>
</ul>

<ul>
<li><p><code>core.toCamel(name)</code></p>
<p> Convert into camelized form</p>
</li>
</ul>

<ul>
<li><p><code>core.toUncamel(str)</code></p>
<p> Convert Camel names into names with dashes</p>
</li>
</ul>

<ul>
<li><p><code>core.toNumber(str, decimals, dflt, min, max)</code></p>
<p> Safe version, use 0 instead of NaN, handle booleans, if decimals specified, returns float</p>
</li>
</ul>

<ul>
<li><p><code>core.toBool(val)</code></p>
<p> Return true if value represents true condition</p>
</li>
</ul>

<ul>
<li><p><code>core.toDate(val)</code></p>
<p> Return Date object for given text or numeric date representation, for invalid date returns 1969</p>
</li>
</ul>

<ul>
<li><p><code>core.toValue(val, type)</code></p>
<p> Convert value to the proper type</p>
</li>
</ul>

<ul>
<li><p><code>core.isTrue(val1, val2, op, type)</code></p>
<p> Evaluate expr, compare 2 values with optional type and operation</p>
</li>
</ul>

<ul>
<li><p><code>core.httpGet(uri, params, callback)</code></p>
<p> Downloads file using HTTP and pass it to the callback if provided</p>
<ul>
<li>uri can be full URL or an object with parts of the url, same format as in url.format</li>
<li>params can contain the following options:<ul>
<li>method - GET, POST</li>
<li>headers - object with headers to pass to HTTP request, properties must be all lower case</li>
<li>cookies - a list with cookies or a boolean to load cookies from the db</li>
<li>file - file name where to save response, in case of error response the error body will be saved as well</li>
<li>postdata - data to be sent with the request in the body</li>
<li>postfile - file to be uploaded in the POST body, not as multipart</li>
<li>query - additional query parameters to be added to the url as an object or as encoded string</li>
<li>sign - sign request with provided email/secret properties</li>
</ul>
</li>
<li>callback will be called with the arguments:
 first argument is error object if any
 second is params object itself with updated fields
 third is HTTP response object
On end, the object params will contains the following updated properties:</li>
<li>data if file was not specified, data will contain collected response body as string</li>
<li>status - HTTP response status code</li>
<li>mtime - Date object with the last modified time of the requested file</li>
<li>size - size of the response body or file
Note: SIDE EFFECT: params object is modified in place so many options will be changed/removed or added</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.signUrl(login, secret, host, uri, options)</code></p>
<p> Produce signed URL to be used in embedded cases or with expiration so the url can be passed and be valid for longer time.
Host passed here must be the actual host where the request will be sent</p>
</li>
</ul>

<ul>
<li><p><code>core.parseSignature(req)</code></p>
<p> Parse incoming request for signature and return all pieces wrapped in an object, this object
will be used by checkSignature function for verification against an account
signature version:</p>
<ul>
<li>1 regular signature signed with secret for specific requests</li>
<li>2 to be sent in cookies and uses wild support for host and path
If the signature successfully recognized it is saved in the request for subsequent use as req.signature</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.checkSignature(sig, account)</code></p>
<p> Verify signature with given account, signature is an object returned by parseSignature</p>
</li>
</ul>

<ul>
<li><p><code>core.signRequest(login, secret, method, host, uri, options)</code></p>
<p> Sign HTTP request for the API server:
url must include all query parameters already encoded and ready to be sent
options may contains the following:</p>
<ul>
<li>expires is absolute time in milliseconds when this request will expire, default is 30 seconds from now</li>
<li>sigversion a version number defining how the signature will be signed</li>
<li>type - content-type header, may be omitted</li>
<li>checksum - SHA1 digest of the whole content body, may be omitted</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.sendRequest(uri, options, callback)</code></p>
<p> Make a request to the backend endpoint, save data in the queue in case of error, if data specified,
POST request is made, if data is an object, it is converted into string.
Returns params as in httpGet with .json property assigned with an object from parsed JSON response.
<em>When used with API endpoints, the <code>backend-host</code> parameter must be set in the config or command line to the base URL of the backend,
like <a href="http://localhost:8000">http://localhost:8000</a>, this is when <code>uri</code> is relative URL. Absolute URLs do not need this parameter.</em>
Special parameters for options:</p>
<ul>
<li>login - login to use for access credentials instead of global credentials</li>
<li>secret - secret to use for access instead of global credentials</li>
<li>proxy - used as a proxy to backend, handles all errors and returns .status and .json to be passed back to API client</li>
<li>queue - perform queue management, save in queue if cannot send right now, delete from queue if sent</li>
<li>rowid - unique record id to be used in case of queue management</li>
<li>checksum - calculate checksum from the data</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.processQueue(callback)</code></p>
<p> Send all pending updates from the queue table</p>
</li>
</ul>

<ul>
<li><p><code>core.getArg(name, dflt)</code></p>
<p> Return commandline argument value by name</p>
</li>
</ul>

<ul>
<li><p><code>core.getArgInt(name, dflt)</code></p>
<p> Return commandline argument value as a number</p>
</li>
</ul>

<ul>
<li><p><code>core.sendmail(from, to, subject, text, callback)</code></p>
<p> Send email</p>
</li>
</ul>

<ul>
<li><p><code>core.forEachLine(file, options, lineCallback, endCallback)</code></p>
<p> Call callback for each line in the file
options may specify the following parameters:</p>
<ul>
<li>sync - read file synchronously and call callback for every line</li>
<li>abort - signal to stop processing</li>
<li>limit - number of lines to process and exit</li>
<li>progress - if &gt; 0 report how many lines processed so far every specified lines</li>
<li>until - skip lines until this regexp matches</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.geoHash(latitude, longitude, options)</code></p>
<p> Return object with geohash for given coordinates to be used for location search
options may contain the following properties:</p>
<ul>
<li>distance - limit the range key with the closest range smaller than then distance, required for search but for updates may be omitted</li>
<li>minDistance - radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
 this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table
 if not specified default <code>min-distance</code> value willbe used.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.geoDistance(latitude1, longitude1, latitude2, longitude2, options)</code></p>
<p> Return distance between two locations, options can specify the following properties:</p>
<ul>
<li>round - a number how to round the distance</li>
</ul>
<p>Example: round to the nearest full 5 km and use only 1 decimal point, if the distance is 13, it will be 15.0</p>
<pre><code>core.geoDistance(34, -188, 34.4, -119, { round: 5.1 })
</code></pre></li>
</ul>

<ul>
<li><p><code>core.encrypt(key, data, algorithm)</code></p>
<p> Encrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>core.decrypt(key, data, algorithm)</code></p>
<p> Decrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>core.sign (key, data, algorithm, encode)</code></p>
<p> HMAC signing and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>core.hash (data, algorithm, encode)</code></p>
<p> Hash and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>core.uuid()</code></p>
<p> Return unique Id without any special characters and in lower case</p>
</li>
</ul>

<ul>
<li><p><code>core.random(size)</code></p>
<p> Generate random key, size if specified defines how many random bits to generate</p>
</li>
</ul>

<ul>
<li><p><code>core.randomInt(min, max)</code></p>
<p> Return random integer between min and max inclusive</p>
</li>
</ul>

<ul>
<li><p><code>core.randomNum(min, max, decs)</code></p>
<p> Generates a random number between given min and max (required)
Optional third parameter indicates the number of decimal points to return:</p>
<ul>
<li>If it is not given or is NaN, random number is unmodified</li>
<li>If &gt;0, then that many decimal points are returned (e.g., &quot;2&quot; -&gt; 12.52</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.now()</code></p>
<p> Return number of seconds for current time</p>
</li>
</ul>

<ul>
<li><p><code>core.strftime(date, fmt, utc)</code></p>
<p> Format date object</p>
</li>
</ul>

<ul>
<li><p><code>core.strSplit(str, sep)</code></p>
<p> Split string into array, ignore empty items</p>
</li>
</ul>

<ul>
<li><p><code>core.strSplitUnique(str, sep)</code></p>
<p> Split as above but keep only unique items</p>
</li>
</ul>

<ul>
<li><p><code>core.arrayUnique(list, key)</code></p>
<p> Returns only unique items in the array, optional <code>key</code> specified the name of the column to use when determining uniqueness if items are objects.</p>
</li>
</ul>

<ul>
<li><p><code>core.toBase64(data)</code></p>
<p> Stringify JSON into base64 string</p>
</li>
</ul>

<ul>
<li><p><code>core.toJson(data)</code></p>
<p> Parse base64 JSON into JavaScript object, in some cases this can be just a number then it is passed as it is</p>
</li>
</ul>

<ul>
<li><p><code>core.parseLocalAddress(str)</code></p>
<p> Given a string with list of urls try to find if any points to our local server using IP address or host name, returns the url
in format: protocol://*:port, mostly to be used with nanomsg sockets</p>
</li>
</ul>

<ul>
<li><p><code>core.moveFile(src, dst, overwrite, callback)</code></p>
<p> Copy file and then remove the source, do not overwrite existing file</p>
</li>
</ul>

<ul>
<li><p><code>core.copyFile(src, dst, overwrite, callback)</code></p>
<p> Copy file, overwrite is optional flag, by default do not overwrite</p>
</li>
</ul>

<ul>
<li><p><code>core.runProcess(cmd, callback)</code></p>
<p> Run the process and return all output to the callback</p>
</li>
</ul>

<ul>
<li><p><code>core.killBackend(name, callback)</code></p>
<p> Kill all backend processes that match name and not the current process</p>
</li>
</ul>

<ul>
<li><p><code>core.shutdown()</code></p>
<p> Shutdown the machine now</p>
</li>
</ul>

<ul>
<li><p><code>core.statSync(file)</code></p>
<p> Non-exception version, returns empty object,
mtime is 0 in case file does not exist or number of seconds of last modified time
mdate is a Date object with last modified time</p>
</li>
</ul>

<ul>
<li><p><code>core.findFileSync(file, filter)</code></p>
<p> Return list of files than match filter recursively starting with given path</p>
<ul>
<li>file - starting path</li>
<li>filter - a function(file, stat) that return 1 if the given file matches, stat is a object returned by fs.statSync</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.makePathSync(dir)</code></p>
<p> Recursively create all directories, return 1 if created or 0 on error, no exceptions are raised, error is logged only</p>
</li>
</ul>

<ul>
<li><p><code>core.makePath(dir, callback)</code></p>
<p> Async version of makePath, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>core.unlinkPath(dir, callback)</code></p>
<p> Recursively remove all files and folders in the given path, returns an error to the callback if any</p>
</li>
</ul>

<ul>
<li><p><code>core.unlinkPathSync(dir)</code></p>
<p> Recursively remove all files and folders in the given path, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>core.chownSync()</code></p>
<p> Change file owner, multiples files can be specified, do not report errors about non existent files</p>
</li>
</ul>

<ul>
<li><p><code>core.mkdirSync()</code></p>
<p> Create a directories if do not exist, multiple dirs can be specified</p>
</li>
</ul>

<ul>
<li><p><code>core.dropPrivileges()</code></p>
<p> Drop root privileges and switch to regular user</p>
</li>
</ul>

<ul>
<li><p><code>core.setTimeout(name, callback, timeout)</code></p>
<p> Set or reset a timer</p>
</li>
</ul>

<ul>
<li><p><code>core.iconPath(id, options)</code></p>
<p> Full path to the icon, perform necessary hashing and sharding, id can be a number or any string</p>
</li>
</ul>

<ul>
<li><p><code>core.getIcon(uri, id, options, callback)</code></p>
<p> Download image and convert into JPG, store under core.path.images
Options may be controlled using the properties:</p>
<ul>
<li>force - force rescaling for all types even if already exists</li>
<li>type - type for the icon, prepended to the icon id</li>
<li>prefix - where to store all scaled icons</li>
<li>verify - check if the original icon is the same as at the source</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.putIcon(file, id, options, callback)</code></p>
<p> Put original or just downloaded file in the proper location according to the types for given id,
this function is used after downloading new image or when moving images from other places. On success
the callback will be called with the second argument set to the output file name where the image has been saved.
Valid properties in the options:</p>
<ul>
<li>type - icon type, this will be prepended to the name of the icon</li>
<li>prefix - top level subdirectory under images/</li>
<li>force - to rescale even if it already exists</li>
<li>width, height, filter, ext, quality for backend.resizeImage function</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.scaleIcon(infile, options, callback)</code></p>
<p> Scale image using ImageMagick, return err if failed</p>
<ul>
<li>infile can be a string with file name or a Buffer with actual image data</li>
<li>options can specify image properties:<ul>
<li>outfile - if not empty is a file name where to store scaled image or if empty the new image contents will be returned in the callback as a buffer</li>
<li>width, height - new image dimensions<ul>
<li>if width or height is negative this means do not perform upscale, keep the original size if smaller than given positive value,</li>
<li>if any is 0 that means keep the original size</li>
</ul>
</li>
<li>filter - ImageMagick image filters, default is lanczos</li>
<li>quality - 0-99 percent, image scaling quality</li>
<li>ext - image format: png, gif, jpg</li>
<li>flip - flip horizontally</li>
<li>flop - flip vertically</li>
<li>blue_radius, blur_sigma - perform adaptive blur on the image</li>
<li>crop_x, crop_y, crop_width, crop_height - perform crop using given dimensions</li>
<li>sharpen_radius, sharpen_sigma - perform sharpening of the image</li>
<li>brightness - use thing to change brightness of the image</li>
<li>contrast - set new contrast of the image</li>
<li>rotate - rotation angle</li>
<li>bgcolor - color for the background, used in rotation</li>
<li>quantized - set number of colors for quantize</li>
<li>treedepth - set tree depth for quantixe process</li>
<li>dither - set 0 or 1 for quantixe and posterize processes</li>
<li>posterize - set number of color levels</li>
<li>normalize - normalize image</li>
<li>opacity - set image opacity</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.domainName(host)</code></p>
<p> Extract domain from local host name</p>
</li>
</ul>

<ul>
<li><p><code>core.typeName(v)</code></p>
<p> Return object type, try to detect any distinguished type</p>
</li>
</ul>

<ul>
<li><p><code>core.isEmpty(val)</code></p>
<p> Return true of the given value considered empty</p>
</li>
</ul>

<ul>
<li><p><code>core.exists(obj, name)</code></p>
<p> Return true if a variable or property in the object exists, just a syntax sugar</p>
</li>
</ul>

<ul>
<li><p><code>core.cloneObj()</code></p>
<p> Deep copy of an object,</p>
<ul>
<li>first argument is the object to clone</li>
<li>second argument can be an object that acts as a filter to skip properties:<ul>
<li>_skip_null - to skip all null properties</li>
<li>_empty_to_null - convert empty strings into null objects</li>
<li>_skip_cb - a callback that returns true to skip a property, arguments are property name and value</li>
<li>name - a property name to skip, the value is treated depending on the type of the property:<ul>
<li>boolean - skip if true</li>
<li>integer - skip only if the object&#39;s property is a string and greater in length that this value</li>
</ul>
</li>
</ul>
</li>
<li>if the second arg is not an object then it is assumed that filter is not given and the arguments are treated as additional property to be added to the cloned object</li>
<li>all additional arguments are treated as name value pairs and added to the cloned object as additional properties
Example:<pre><code>  core.cloneObj({ 1: 2 }, { 1: 1 }, &quot;3&quot;, 3, &quot;4&quot;, 4)
  core.cloneObj({ 1 : 2 }, &quot;3&quot;, 3, &quot;4&quot;, 4)
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.newObj()</code></p>
<p> Return new object using arguments as name value pairs for new object properties</p>
</li>
</ul>

<ul>
<li><p><code>core.extendObj()</code></p>
<p> Add properties to existing object, first arg is the object, the rest are pairs: name, value,....</p>
</li>
</ul>

<ul>
<li><p><code>core.delObj()</code></p>
<p> Delete properties from the object, first arg is an object, the rest are properties to be deleted</p>
</li>
</ul>

<ul>
<li><p><code>core.searchObj(obj, options)</code></p>
<p> Return an object consisting of properties that matched given criteria in the given object.
optins can define the following properties:</p>
<ul>
<li>name - search by property name, return all objects that contain given property</li>
<li>value - search by value, return all objects that have a property with given value</li>
<li>sort if true then sort found columns by the property value.</li>
<li>names - if true just return list of column names</li>
<li>flag - if true, return object with all properties set to flag value</li>
</ul>
<p>Example</p>
<pre><code>    core.searchObj({id:{index:1},name:{index:3},type:{index:2},descr:{}}, { name: &#39;index&#39;, sort: 1 });
    { id: { index: 1 }, type: { index: 2 }, name: { index: 3 } }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.mergeObj(obj, options)</code></p>
<p> Merge an object with the options, all properties in the options override existing in the object, returns a new object</p>
<p>Example</p>
<pre><code>var o = core.mergeObject({ a:1, b:2, c:3 }, { c:5, d:1 })
o = { a:1, b:2, c:5, d:1 }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.stringify(obj)</code></p>
<p> JSON stringify without empty properties</p>
</li>
</ul>

<ul>
<li><p><code>core.cookieGet(domain, callback)</code></p>
<p> Return cookies that match given domain</p>
</li>
</ul>

<ul>
<li><p><code>core.cookieSave(cookiejar, setcookies, hostname, callback)</code></p>
<p> Save new cookies arrived in the request,
merge with existing cookies from the jar which is a list of cookies before the request</p>
</li>
</ul>

<ul>
<li><p><code>core.addContext()</code></p>
<p> Adds reference to the objects in the core for further access, specify module name, module reference pairs</p>
</li>
</ul>

<ul>
<li><p><code>core.createRepl(options)</code></p>
<p> Create REPL interface with all modules available</p>
</li>
</ul>

<ul>
<li><p><code>core.watchTmp(dirs, secs, pattern)</code></p>
<p> Watch temp files and remove files that are older than given number of seconds since now, remove only files that match pattern if given
This function is not async-safe, it uses sync calls</p>
</li>
</ul>

<ul>
<li><p><code>core.watchFiles(dir, pattern, callback)</code></p>
<p> Watch files in a dir for changes and call the callback</p>
</li>
</ul>

<ul>
<li><p><code>core.watchLogs(callback)</code></p>
<p> Watch log files for errors and report via email</p>
</li>
</ul>

<h2 id="module-db">Module: DB</h2>
<ul>
<li><p><code>db</code></p>
<p> The Database API, a thin abstraction layer on top of SQLite, PostgreSQL, DynamoDB and Cassandra.
The idea is not to introduce new abstraction layer on top of all databases but to make
the API usable for common use cases. On the source code level access to all databases will be possible using
this API but any specific usage like SQL queries syntax or data types available only for some databases will not be
unified or automatically converted but passed to the database directly. Only conversion between JavaScript types and
database types is unified to some degree meaning JavaScript data type will be converted into the corresponding
data type supported by any particular database and vice versa.</p>
<p>Basic operations are supported for all database and modelled after NoSQL usage, this means no SQL joins are supported
by the API, only single table access. SQL joins can be passed as SQL statements directly to the database using low level db.query
API call, all high level operations like add/put/del perform SQL generation for single table on the fly.</p>
<p>Before the DB functions can be used the <code>core.init</code> MUST be called first, the typical usage:</p>
<pre><code>    var backend = require(&quot;backendjs&quot;), core = backend.core, db = backend.db;
    core.init(function(err) {
        db.add(...
        ...
    });
</code></pre><p>All database methods can use default db pool or any other available db pool by using pool: name in the options. If not specified,
then default db pool is used, sqlite is default if not -db-pool config parameter specified in the command line or the config file.</p>
<p>Also, to spread functionality between different databases it is possible to assign some tables to the specific pools using <code>db-pool-tables</code> parameters
thus redirecting the requests to one or another databases depending on the table, this for example can be useful when using fast but expensive
database like DynamoDB for real-time requests and slower SQL database running on some slow instance for rare requests, reports or statistics processing.</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      bk_property: { name: { primary: 1 },
                     value: {},
                     mtime: { type: &quot;bigint&quot;, now: 1 }
      },

      bk_cookies: { id: { primary: 1 },
                    name: {},
                    domain: {},
                    path: {},
                    value: { type: &quot;text&quot; },
                    expires: { type:&quot; bigint&quot; }
      },

      bk_queue: { id: { primary: 1 },
                  url: {},
                  postdata: { type: &quot;text&quot; },
                  counter: { type: &#39;int&#39; },
                  mtime: { type: &quot;bigint&quot;, now: 1 }
      },

      bk_jobs: { id: { primary: 1 },
                 tag: { primary: 1 },
                 type: { value: &quot;local&quot; },
                 job: { type: &quot;json&quot; },
                 cron: {},
                 args: {},
                 mtime: { type: &#39;bigint&#39;, now: 1 }
      },
</code></pre></li>
</ul>

<ul>
<li><p><code>db.init(options, callback)</code></p>
<p> Initialize database pools</p>
</li>
</ul>

<ul>
<li><p><code>db.initTables(tables, options, callback)</code></p>
<p> Create tables in all pools</p>
</li>
</ul>

<ul>
<li><p><code>db.initPoolTables(name, tables, options, callback)</code></p>
<p> Init the pool, create tables and columns:</p>
<ul>
<li>name - db pool to create the tables in</li>
<li>tables - an object with list of tables to create or upgrade</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.dropPoolTables(name, tables, options, callback)</code></p>
<p> Remove all registered tables from the pool</p>
</li>
</ul>

<ul>
<li><p><code>db.getPoolTables(name)</code></p>
<p> Return all tables know to the given pool, returned tables are in the object with
column information merged from cached columns from the database with description columns
given by the application. Property fake: 1 in any column signifies not a real column but
a column described by the application and not yet created by the database driver or could not be added
due to some error.</p>
</li>
</ul>

<ul>
<li><p><code>db.createPool(options)</code></p>
<p> Create a new database pool with default methods and properties</p>
<ul>
<li>options - an object with default pool properties<ul>
<li>pool - pool name</li>
<li>pooling - create generic pool for connection caching</li>
<li>watchfile - file path to be watched for changes, all clients will be destroyed gracefully
The following pool callback can be assigned to the pool object:</li>
</ul>
</li>
<li>connect - a callback to be called when actual database client needs to be created, the callback signature is
function(pool, callback) and will be called with first arg an error object and second arg is the database instance, required for pooling</li>
<li>bindValue - a callback function(val, info) that returns the value to be used in binding, mostly for SQL drivers, on input value and col info are passed, this callback
may convert the val into something different depending on the DB driver requirements, like timestamp as string into milliseconds</li>
<li>convertError - a callback function(table, op, err, options) that converts native DB driver error into other human readable format</li>
<li>resolveTable - a callback function(op, table, obj, options) that returns poosible different table at the time of the query, it is called by the <code>db.prepare</code> method
and if exist it must return the same or new table name for the given query parameters.</li>
</ul>
<p>The db methods cover most use cases but in case native driver needs to be used this is how to get the client and use it with its native API,
it is required to call pool.free at the end to return the connection back to the connection pool.</p>
<pre><code>    var pool = db.getPool(&quot;&quot;, { pool: &quot;mongodb&quot; });
    pool.get(function(err, client) {
        var collection = client.collection(&#39;bk_account&#39;);
        collection.findOne({ id: &#39;123&#39; }, function() {
            pool.free(client);
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.query(req, options, callback)</code></p>
<p> Execute query using native database driver, the query is passed directly to the driver.</p>
<ul>
<li>req - can be a string or an object with the following properties:<ul>
<li>text - SQL statement or other query in the format of the native driver, can be a list of statements</li>
<li>values - parameter values for SQL bindings or other driver specific data</li>
<li>op - operations to be performed, used by non-SQL drivers</li>
<li>obj - actual object with data for non-SQL drivers</li>
<li>table - table name for the operation</li>
</ul>
</li>
<li><p>options may have the following properties:</p>
<ul>
<li><p>pool - name of the database pool where to execute this query.</p>
<p>The difference with the high level functions that take a table name as their firt argument, this function must use pool
explicitely if it is different from the default. Other functions can resolve
the pool by table name if some tables are assigned to any specific pool by configuration parameters <code>db-pool-tables</code>.</p>
</li>
<li>filter - function to filter rows not to be included in the result, return false to skip row, args are: (row, options)</li>
</ul>
</li>
<li>callback(err, rows, info) where<ul>
<li>info is an object with information about the last query: inserted_oid,affected_rows,next_token</li>
<li>rows is always returned as a list, even in case of error it is an empty list</li>
</ul>
</li>
</ul>
<p>Example with SQL driver</p>
<pre><code>    db.query({ text: &quot;SELECT a.id,c.type FROM bk_account a,bk_connection c WHERE a.id=c.id and a.id=?&quot;, values: [&#39;123&#39;] }, { pool: &#39;pgsql&#39; }, function(err, rows, info) {
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.add(table, obj, options, callback)</code></p>
<p> Insert new object into the database</p>
<ul>
<li>obj - an JavaScript object with properties for the record, primary key properties must be supplied</li>
<li>options may contain the following properties:<ul>
<li>all_columns - do not check for actual columns defined in the pool tables and add all properties from the obj, only will work for NoSQL dbs,
by default all properties in the obj not described in the table definition for the given table will be ignored.</li>
<li>skip_columns - ignore properties by name listed in the this array</li>
<li>mtime - if set, mtime column will be added automatically with the current timestamp, if mtime is a
string then it is used as a name of the column instead of default mtime name</li>
<li>skip_null - if set, all null values will be skipped, otherwise will be written into the DB as NULLs</li>
</ul>
</li>
</ul>
<p>Example</p>
<pre><code> db.add(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.put(table, obj, options, callback)</code></p>
<p> Add/update an object in the database, if object already exists it will be replaced with all new properties from the obj</p>
<ul>
<li>obj - an object with record properties, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.put(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.update(table, obj, options, callback)</code></p>
<p> Update existing object in the database.</p>
<ul>
<li>obj - is an actual record to be updated, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method with the following additional properties:<ul>
<li>keys - list of properties to use as keys for the update condition, if not specified then primary keys will be used</li>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
</ul>
</li>
</ul>
<p>Note: SQL databases can update more than one record with the corresponding condition but non-SQL databases like
DynamoDB or Cassandra only update one existing record at a time</p>
<p>Example</p>
<pre><code>    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });

    db.update(&quot;bk_account&quot;, { gender: &#39;m&#39;, prefix: &#39;Mr&#39; }, { pool: pgsql&#39;, keys: [&#39;gender&#39;] }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.updateAll(table, obj, options, callback)</code></p>
<p> Update all records that match given condition, one by one, the input is the same as for <code>db.select</code> and every record
returned will be updated using <code>db.update</code> call by the primary key, so make sure options.select include the primary key for every row found by the select.
The callback will receive on completion the err and all rows found and updated. This is mostly for non-SQL databases and for very large range it may take a long time
to finish due to sequential update every record one by one.
Special properties that can be in the options for this call:</p>
<ul>
<li>concurrency - how many update queries to execute at the same time, default is 1, this is done by using async.forEachLimit.</li>
<li>process - a function callback that will be called for each row before updating it, this is for some transformations of the record properties
in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
as <code>options.process(row, options)</code></li>
</ul>
<p>Example, update birthday format if not null</p>
<pre><code>    db.updateAll(&quot;bk_account&quot;, { birthday: 1 }, { keys: [&quot;birthday&quot;], ops: { birthday: &quot;not null&quot; }, concurrency: 2, process: function(r, o) { r.birthday = core.strftime(new Date(r.birthday, &quot;%Y-%m-D&quot;)) } }, function(err, rows) {
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.incr(table, obj, options, callback)</code></p>
<p> Counter operation, increase or decrease column values, similar to update but all specified columns except primary
key will be incremented, use negative value to decrease the value.</p>
<p><em>Note: The record must exist already for SQL databases, for DynamoDB and Cassandra a new record will be created
if does not exist yet.</em></p>
<p>Example</p>
<pre><code> db.incr(&quot;bk_counter&quot;, { id: &#39;123&#39;, like0: 1, invite0: 1 }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.del(table, obj, options, callback)</code></p>
<p> Delete object in the database, no error if the object does not exist</p>
<ul>
<li>obj - an object with primary key properties only, other properties will be ignored</li>
<li>options - same properties as for <code>db.update</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.del(&quot;bk_account&quot;, { id: &#39;123&#39; }, function(err, rows, info) {
     console.log(&#39;updated:&#39;, info.affected_rows);
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.delAll(table, obj, options, callback)</code></p>
<p> Delete all records that match given condition, one by one, the input is the same as for <code>db.select</code> and every record
returned will be deleted using <code>db.del</code> call. The callback will receive on completion the err and all rows found and deleted.
Special properties that can be in the options for this call:</p>
<ul>
<li>concurrency - how many delete requests to execute at the same time by using async.forEachLimit.</li>
<li>process - a function callback that will be called for each row before deleting it, this is for some transformations of the record properties
in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
as <code>options.process(row, options)</code></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.replace(table, obj, options, callback)</code></p>
<p> Add/update the object, check existence by the primary key or by other keys specified. This is not equivalent of REPLACE INTO, it does <code>db.get</code>
to check if the object exists in the database and performs <code>db.add</code> or <code>db.update</code> depending on the existence.</p>
<ul>
<li>obj is a JavaScript object with properties that correspond to the table columns</li>
<li>options define additional flags that may<ul>
<li>keys - is list of column names to be used as primary key when looking for updating the record, if not specified
then default primary keys for the table will be used, only keys columns will be used for condition, i.e. WHERE clause</li>
<li>check_mtime - defines a column name to be used for checking modification time and skip if not modified, must be a date value</li>
<li>check_data - verify every value in the given object with actual value in the database and skip update if the record is the same,
if it is an array then check only specified columns</li>
</ul>
</li>
</ul>
<p>Example: updates record 123 only if gender is not &#39;m&#39; or adds new record</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, { check_data: true });
</code></pre><p>Example: updates record 123 only if mtime of the record is less or equal yesterday</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, mtime: Date.now() - 86400000 }, { check_mtime: &#39;mtime&#39; });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.select(table, obj, options, callback)</code></p>
<p> Select objects from the database that match supplied conditions.</p>
<ul>
<li>obj - can be an object with primary key properties set for the condition, all matching records will be returned</li>
<li>obj - can be a list where each item is an object with primary key condition. Only records specified in the list must be returned.</li>
<li><p>options can use the following special properties:</p>
<ul>
<li><p>keys - a list of columns for condition or all primary keys will be used for query condition, only keys will be used in WHERE part of the SQL statement.</p>
<p>By default primary keys are used only but if any other columns specified they will be treated as primary keys, some databases like DynamoDB may restrict which
columns can be used, for example for DynamoDB keys must contain hash, range keys first and then any other columns can be added which will be filtered by the backend
after all records are received from the database using has,range combination.</p>
<p>NOTE: keys can refer only to the columns in the table, any artificial or computed properties can be filtered by using .filter callback</p>
</li>
<li>ops - operators to use for comparison for properties, an object with column name and operator. The follwoing operators are available:
 <code>&gt;, gt, &lt;, lt, =, !=, &lt;&gt;, &gt;=, ge, &lt;=, le, in, between, regexp, iregexp, begins_with, like%, ilike%</code></li>
<li>opsMap - operator mapping between supplied operators and actual operators supported by the db</li>
<li>typesMap - type mapping between supplied and actual column types, an object</li>
<li>select - a list of columns or expressions to return or all columns if not specified</li>
<li>start - start records with this primary key, this is the next_token passed by the previous query</li>
<li>count - how many records to return</li>
<li>sort - sort by this column</li>
<li>check_public - value to be used to filter non-public columns (marked by .pub property), compared to primary key column</li>
<li>desc - if sorting, do in descending order</li>
<li>page - starting page number for pagination, uses count to find actual record to start</li>
</ul>
</li>
</ul>
<p>On return, the callback can check third argument which is an object with the following properties:</p>
<ul>
<li>affected_rows - how many records this operation affected, for add/put/update</li>
<li>inserted_oid - last created auto generated id</li>
<li>next_token - next primary key or offset for pagination by passing it as .start property in the options, if null it means there are no more pages availabe for this query</li>
</ul>
<p>Example: get by primary key, refer above for default table definitions</p>
<pre><code>    db.select(&quot;bk_message&quot;, { id: &#39;123&#39; }, { count: 2 }, function(err, rows) {

    });
</code></pre><p>Example: get all icons with type greater or equal to 2</p>
<pre><code>    db.select(&quot;bk_icon&quot;, { id: &#39;123&#39;, type: &#39;2&#39; }, { select: &#39;id,type&#39;, ops: { type: &#39;ge&#39; } }, function(err, rows) {

    });
</code></pre><p>Example: get unread msgs sorted by time, recent first</p>
<pre><code>    db.select(&quot;bk_message&quot;, { id: &#39;123&#39;, status: &#39;N:&#39; }, { sort: &quot;status&quot;, desc: 1, ops: { status: &quot;begins_with&quot; } }, function(err, rows) {

    });
</code></pre><p>Example: allow all accounts icons to be visible</p>
<pre><code>    db.select(&quot;bk_account&quot;, {}, function(err, rows) {
        rows.forEach(function(row) {
            row.acl_allow = &#39;auth&#39;;
            db.update(&quot;bk_icon&quot;, row);
        });
    });
</code></pre><p>Example: scan accounts with custom filter, not by primary key: all females</p>
<pre><code>    db.select(&quot;bk_account&quot;, { gender: &#39;f&#39; }, { keys: [&#39;gender&#39;] }, function(err, rows) {

    });
</code></pre><p>Example: select connections using primary key and other filter columns: all likes for the last day</p>
<pre><code>    db.select(&quot;bk_connection&quot;, { id: &#39;123&#39;, type: &#39;like&#39;, mtime: Date.now()-86400000 }, { keys: [&#39;id&#39;, &#39;type&#39;, &#39;mtime&#39;], ops: { type: &quot;begins_with&quot;, mtime: &quot;gt&quot; } }, function(err, rows) {

    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.list(table, obj, options, callback)</code></p>
<p> Convenient helper to retrieve all records by primary key, the obj must be a list with key property or a string with list of primary key column
Example</p>
<pre><code>db.list(&quot;bk_account&quot;, [&quot;id1&quot;, &quot;id2&quot;], function(err, rows) { console.log(err, rows) });
db.list(&quot;bk_account&quot;, &quot;id1,id2&quot;, function(err, rows) { console.log(err, rows) });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.scan(table, obj, options, rowCallback, callback)</code></p>
<p> Convenient helper for scanning a table for some processing, rows are retrieved in batches and passed to the callback until there are no more
records matching given criteria. The obj is the same as passed to the <code>db.select</code> method which defined a condition which records to get.
The rowCallback must be present and is called for every rows batch retrieved and second parameter which is the function to be called
once the processing is complete. At the end, the callback will be called just with 1 argument, err, this indicates end of scan operation.
Basically, db.scan is the same as db.select but can be used to retrieve large number of records in batches and allows async processing of such records.</p>
<p>Parameters:</p>
<ul>
<li>table - table to scan</li>
<li>obj - an object with query conditions, same as in <code>db.select</code></li>
<li>options - same as in <code>db.select</code>, the only required property is <code>count</code> to specify sixe of every batch, default is 100</li>
<li>rowCallback - process records when called like this `callback(rows, next)</li>
<li>endCallback - end of scan when called like this: `callback(err)</li>
</ul>
<p>Example:</p>
<pre><code>    db.scan(&quot;bk_account&quot;, {}, { count:10, pool:&quot;dynamodb&quot; }, function(rows, next) {
        async.forEachSeries(rows, function(row, next2) {
            // Copy all accounts from one db into another
            db.add(&quot;bk_account&quot;, row, { pool: &quot;pgsql&quot; }, next2)
        }, next);
    }, function(err) { });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.migrate(table, options, callback)</code></p>
<p> Migrate a table via temporary table, copies all records into a temp table, then re-create the table with up-to-date definitions and copies all records back into the new table.
The following options can be used:</p>
<ul>
<li>preprocess - a callback function(row, options, next) that is called for every row on the original table, next must be called to move to the next row, if err is returned as first arg then the processing will stop</li>
<li>postprocess - a callback function(row, options, next) that is called for every row on the destination table, same rules as for preprocess</li>
<li>tmppool - the db pool to be used for temporary table</li>
<li>tpmdrop - if 1 then the temporary table willbe dropped at the end in case of success, by default it is kept</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.search(table, obj, options, callback)</code></p>
<p> Perform full text search on the given table, the database implementation may ignore table name completely
in case of global text index. Options takes same properties as in the select method. Without full text support
this works the same way as the <code>select</code> method.</p>
</li>
</ul>

<ul>
<li><p><code>db.getLocations(table, options, callback)</code></p>
<p> Geo locations search, paginate all results until the end.
table must be defined with the following required columns:</p>
<ul>
<li>geohash - location as primary key hash,</li>
<li>id or other column name to be used as a RANGE key for DynamoDB/Cassandra or part of the compsoite primary key for SQL, the result will be sorted by this column for all databases</li>
<li>latitude and longitude as floating numbers</li>
</ul>
<p>other optional properties:</p>
<ul>
<li>round - a number that defines the &quot;precision&quot; of  the distance, it rounds the distance to the nearest
round number and uses decimal point of the round number to limit decimals in the distance</li>
<li>sort - sorting order, by default the RANGE key is used for DynamoDB , it is possinle to specify Local Index as well,
for SQL the second part of the primary key if exists or id</li>
<li>unique - specified the column name to be used in determinint unique records, if for some reasons there are multiple record in the location
table for the same id only one instance will be returned</li>
</ul>
<p>On first call, options must contain latitude and longitude of the center and optionally distance for the radius. On subsequent calls options must be the
the next_token returned by the previous call</p>
<p>On return, the callback&#39;s third argument contains the object that must be provided for subsequent searches until rows array is empty.</p>
<p>Example</p>
<pre><code>    db.getLocations(&quot;bk_location&quot;, { latitude: -118, longitude: 30, distance: 10 }, function(err, rows, info) {
        ...
        // Get next page using previous info object
        db.getLocations(&quot;bk_location&quot;, info, function(err, rows, info) {
            ...
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.get(table, obj, options, callback)</code></p>
<p> Retrieve one record from the database by primary key, returns found record or null if not found
Options can use the following special properties:</p>
<ul>
<li>keys - a list of columns to be used instead of primary keys, this can be useful in case of another
  unique index which is different than the primary key</li>
<li>select - a list of columns or expressions to return, default is to return all columns</li>
<li>op - operators to use for comparison for properties, see <code>db.select</code></li>
<li>cached - if specified it runs getCached version</li>
</ul>
<p>Example</p>
<pre><code>    db.get(&quot;bk_account&quot;, { id: &#39;12345&#39; }, function(err, row) {
       if (row) console.log(row.name);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.getCached(table, obj, options, callback)</code></p>
<p> Retrieve cached result or put a record into the cache prefixed with table:key[:key...]
Options accept the same parameters as for the usual get action but it is very important that all the options
be the same for every call, especially <code>select</code> parameters which tells which columns to retrieve and cache.
Additional options:</p>
<ul>
<li>prefix - prefix to be used for the key instead of table name</li>
</ul>
<p>Example:</p>
<pre><code>db.getCache(&quot;bk_account&quot;, { id: req.query.id }, { select: &quot;latitude,longitude&quot; }, function(err, row) {
    var distance = backend.geoDistance(req.query.latitude, req.query.longitude, row.latitude, row.longitudde);
});
</code></pre></li>
</ul>

<ul>
<li><p><code>db.clearCached(table, obj, options)</code></p>
<p> Notify or clear cached record, this is called after del/update operation to clear cached version by primary keys</p>
</li>
</ul>

<ul>
<li><p><code>db.getCachedKey(table, obj, options)</code></p>
<p> Returns concatenated values for the primary keys, this is used for caching records by primary key</p>
</li>
</ul>

<ul>
<li><p><code>db.create(table, columns, options, callback)</code></p>
<p> Create a table using column definitions represented as a list of objects. Each column definition can
contain the following properties:</p>
<ul>
<li>name - column name</li>
<li>type - column type, one of: int, real, string, counter or other supported type</li>
<li>primary - column is part of the primary key</li>
<li>unique - column is part of an unique key</li>
<li>index - column is part of an index</li>
<li>value - default value for the column</li>
<li>len - column length</li>
<li>pub - columns is public, <em>this is very important property because it allows anybody to see it when used in the default API functions, i.e. anybody with valid
credentials can retrieve all public columns from all other tables, and if one of the other tables is account table this may expose some personal infoamtion,
so by default only a few columns are marked as public in the bk_account table</em></li>
<li>semipub - column is not public but still retrieved to support other public columns, must be deleted after use</li>
<li>now - means on every add/put/update set this column with current time as Date.now()</li>
<li>autoincr - for counter tables, mark the column to be auto-incremented by the connection API if the connection type has the same name as the column name</li>
</ul>
<p><em>Some properties may be defined multiple times with number suffixes like: unique1, unique2, index1, index2 to create more than one index for the table, same
properties define a composite key in the order of definition or sorted by the property value, for example: <code>{ a: {index:2 }, b: { index:1 } }</code> will create index (b,a)
because of the index: property value being not the same.</em></p>
<p>NOTE: Index creation is not required and all index properties can be omitted, it can be done more effectively using native tools for any specific database,
this format is for simple and common use cases without using any other tools but it does not cover all possible variations for every database. But all indexes and
primary keys created outside of the backend application still be be detected properly by <code>db.cacheColumns</code> method for every database.</p>
<p>Each database pool also can support native options that are passed directly to the driver in the options, these properties are
defined in the object with the same name as the db driver, for example to define Projection for the DynamoDB index:</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index: 1, dynamodb: { ProvisionedThroughput: { ReadCapacityUnits: 50, WriteCapacityUnits: 50 } } },
                              type: { primary: 1, pub: 1 },
                              name: { index: 1, pub: 1, dynamodb: { projection: [&#39;type&#39;] } }
                            });
</code></pre><p>Create DynamoDB table with global secondary index, first index property if not the same as primary key hash defines global index, if it is the same then local,
below we create global secondary index on property &#39;name&#39; only, in the example above it was local secondary index for id and name. also local secondary index is
created on id,title.</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index1: 1 },
                              type: { primary: 1 },
                              name: { index: 1, dynamodb: { projection: [&#39;type&#39;] } }
                              title: { index1: 1 } }
                            });
</code></pre><p>Pass MongoDB options directly:</p>
<pre><code>  db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, mongodb: { w: 1, capped: true, max: 100, size: 100 } },
                            type: { primary: 1, pub: 1 },
                            name: { index: 1, pub: 1, mongodb: { sparse: true, min: 2, max: 5 } }
                          });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.upgrade(table, columns, options, callback)</code></p>
<p> Upgrade a table with missing columns from the definition list</p>
</li>
</ul>

<ul>
<li><p><code>db.drop(table, options, callback)</code></p>
<p> Drop a table</p>
</li>
</ul>

<ul>
<li><p><code>db.prepare(op, table, obj, options)</code></p>
<p> Prepare for execution for the given operation: add, del, put, update,...
Returns prepared object to be passed to the driver&#39;s .query method. This method is a part of the driver
helpers and is not used directly in the applications.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPool(table, options)</code></p>
<p> Return database pool by name or default pool</p>
</li>
</ul>

<ul>
<li><p><code>db.getOptions(table, options)</code></p>
<p> Return combined options for the pool including global pool options</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumns(table, options)</code></p>
<p> Return cached columns for a table or null, columns is an object with column names and objects for definition</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumn(table, name, options)</code></p>
<p> Return the column definition for a table</p>
</li>
</ul>

<ul>
<li><p><code>db.getSelectedColumns(table, options)</code></p>
<p> Return list of selected or allowed only columns, empty list if no options.select is specified</p>
</li>
</ul>

<ul>
<li><p><code>db.skipColumn(name, val, options, columns)</code></p>
<p> Verify column against common options for inclusion/exclusion into the operation, returns 1 if the column must be skipped</p>
</li>
</ul>

<ul>
<li><p><code>db.filterColumns(obj, rows, options)</code></p>
<p> Given object with data and list of keys perform comparison in memory for all rows, return only rows that match all keys. This method is usee
by custom filters in <code>db.select</code> by the drivers which cannot perform comparisons with non-indexes columns like DynamoDb, Cassandra.
The rows that satisfy primary key conditions are retunred and then called this function to eliminate the records that do not satisfy non-indexed column conditions.</p>
<p>Options support the following propertis:</p>
<ul>
<li>keys - list of columns to check, these may or may not be the primary keys, any columns to be compared</li>
<li>ops - operations for columns</li>
<li>typesMap - types for the columns if different from the actual Javascript type</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getKeys(table, options)</code></p>
<p> Return cached primary keys for a table or null</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchKeys(table, options)</code></p>
<p> Return keys for the table search, if options.keys provided and not empty it will be used otherwise
table&#39;s primary keys will be returned. This is a wrapper that makes sure that valid keys are used and
deals with input errors like empty keys list to be consistent between different databases.
This function always returns an Array even if it is empty.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchQuery(table, obj, options)</code></p>
<p> Return query object based on the keys specified in the options or primary keys for the table, only search properties
will be returned in the query object</p>
</li>
</ul>

<ul>
<li><p><code>db.getBindValue(table, options, val, info)</code></p>
<p> Return possibly converted value to be used for inserting/updating values in the database,
is used for SQL parametrized statements</p>
<p>Parameters:</p>
<ul>
<li>options - standard pool parameters with pool: property for specific pool</li>
<li>val - the JavaScript value to convert into bind parameter</li>
<li>info - column definition for the value from the cached columns</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getColumnValue(table, options, val, info)</code></p>
<p> Return transformed value for the column value returned by the database, same parameters as for getBindValue</p>
</li>
</ul>

<ul>
<li><p><code>db.convertError(table, op, err, options)</code></p>
<p> Convert native database error in some generic human readable string</p>
</li>
</ul>

<ul>
<li><p><code>db.cacheColumns(options, callback)</code></p>
<p> Reload all columns into the cache for the pool</p>
</li>
</ul>

<ul>
<li><p><code>db.mergeColumns(pool)</code></p>
<p> Merge JavaScript column definitions with the db cached columns</p>
</li>
</ul>

<ul>
<li><p><code>db.getPublicColumns(table, options)</code></p>
<p> Columns that are allowed to be visible, used in select to limit number of columns to be returned by a query</p>
<ul>
<li>pub property means public column</li>
<li>semipub means not allowed but must be returned for calculations in the select to produce another public column</li>
</ul>
<p>options may be used to define the following properties:</p>
<ul>
<li>columns - list of public columns to be returned, overrides the public columns in the definition list</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.processRows(pool, table, rows, options)</code></p>
<p> Custom row handler that is called for every row in the result, this assumes that pool.processRow callback has been assigned previously by db.setProcessRow.
This function is called automatically by the db.query but can be called manually for rows that are not received from the database, for example on
adding new records and returning them back to the client. In such case, the <code>pool</code> argument can be passed as null, it will be found by the table name.
<code>rows</code> can be list of records or single record.</p>
</li>
</ul>

<ul>
<li><p><code>db.setProcessRow(table, options, callback)</code></p>
<p> Assign processRow callback for a table, this callback will be called for every row on every result being retrieved from the
specified table thus providing an opportunity to customize the result.</p>
<p>All assigned callback to this table will be called in the order of the assignment.</p>
<p>The callback accepts 3 arguments: function(row, options, columns)
 where - row is a row from the table, options are the obj passed to the db called and columns is an object with table&#39;s columns</p>
<p>Example</p>
<pre><code>db.setProcessRow(&quot;bk_account&quot;, function(row, opts, cols) {
    if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
    delete row.birthday;
});
</code></pre></li>
</ul>

<ul>
<li><p><code>db.sqlInitPool(options)</code></p>
<p> Create a database pool for SQL like databases</p>
<ul>
<li>options - an object defining the pool, the following properties define the pool:<ul>
<li>pool - pool name/type, of not specified SQLite is used</li>
<li>max - max number of clients to be allocated in the pool</li>
<li>idle - after how many milliseconds an idle client will be destroyed</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlCacheColumns(options, callback)</code></p>
<p> Cache columns using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlPrepare(op, table, obj, options)</code></p>
<p> Prepare SQL statement for the given operation</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlQuote(val)</code></p>
<p> Quote value to be used in SQL expressions</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValue(value, type, dflt, min, max)</code></p>
<p> Return properly quoted value to be used directly in SQL expressions, format according to the type</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValueIn(list, type)</code></p>
<p> Return list in format to be used with SQL IN ()</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlExpr(name, value, options)</code></p>
<p> Build SQL expressions for the column and value
options may contain the following properties:</p>
<ul>
<li>op - SQL operator, default is =</li>
<li>type - can be data, string, number, float, expr, default is string</li>
<li>value - default value to use if passed value is null or empty</li>
<li>min, max - are used for numeric values for validation of ranges</li>
<li>expr - for op=expr, contains sprintf-like formatted expression to be used as is with all &#39;%s&#39; substituted with actual value</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlTime(d)</code></p>
<p> Return time formatted for SQL usage as ISO, if no date specified returns current time</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlFilter(columns, values, params)</code></p>
<p> Given columns definition object, build SQL query using values from the values object, all conditions are joined using AND,</p>
<ul>
<li>columns is a list of objects with the following properties:<ul>
<li>name - column name, also this is the key to use in the values object to get value by</li>
<li>col - actual column name to use in the SQL</li>
<li>alias - optional table prefix if multiple tables involved</li>
<li>value - default value</li>
<li>type - type of the value, this is used for proper formatting: boolean, number, float, date, time, string, expr</li>
<li>op - any valid SQL operation: =,&gt;,&lt;, between, like, not like, in, not in, ~*,.....</li>
<li>group - for grouping multiple columns with OR condition, all columns with the same group will be in the same ( .. OR ..)</li>
<li>always - only use default value if true</li>
<li>required - value default or supplied must be in the query, otherwise return empty SQL</li>
<li>search - additional name for a value, for cases when generic field is used for search but we search specific column</li>
</ul>
</li>
<li>values - actual values for the condition as an object, usually req.query</li>
<li>params - if given will contain values for binding parameters</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlLimit(options)</code></p>
<p> Build SQL orderby/limit/offset conditions, config can define defaults for sorting and paging</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlWhere(table, obj, keys, options)</code></p>
<p> Build SQL where condition from the keys and object values, returns SQL statement to be used in WHERE</p>
<ul>
<li>obj - an object record properties</li>
<li>keys - a list of primary key columns</li>
<li>options may contains the following properties:<ul>
<li>pool - pool to be used for driver specific functions</li>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlCreate(table, obj, options)</code></p>
<p> Create SQL table using table definition</p>
<ul>
<li>table - name of the table to create</li>
<li>obj - object with properties as column names and each property value is an object:<ul>
<li>name - column name</li>
<li>type - type of the column, default is TEXT, options: int, real or other supported types</li>
<li>value - default value for the column</li>
<li>primary - part of the primary key</li>
<li>index - indexed column, part of the composite index</li>
<li>unique - must be combined with index property to specify unique composite index</li>
<li>len - max length of the column</li>
<li>notnull - true if should be NOT NULL</li>
<li>auto - true for AUTO_INCREMENT column</li>
</ul>
</li>
<li>options may contains:<ul>
<li>upgrade - perform alter table instead of create</li>
<li>typesMap - type mapping, convert lowercase type into other type supported by any specific database</li>
<li>noDefaults - ignore default value if not supported (Cassandra)</li>
<li>noNulls - NOT NULL restriction is not supported (Cassandra)</li>
<li>noMultiSQL - return as a list, the driver does not support multiple SQL commands</li>
<li>noLengths - ignore column length for columns (Cassandra)</li>
<li>noIfExists - do not support IF EXISTS on table or indexes</li>
<li>noCompositeIndex - does not support composite indexes (Cassandra)</li>
<li>noauto - no support for auto increment columns</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpgrade(table, obj, options)</code></p>
<p> Create ALTER TABLE ADD COLUMN statements for missing columns</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDrop(table, obj, options)</code></p>
<p> Create SQL DROP TABLE statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlSelect(table, obj, options)</code></p>
<p> Select object from the database,
options may define the following properties:</p>
<ul>
<li>keys is a list of columns for condition</li>
<li>select is list of columns or expressions to return</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlInsert(table, obj, options)</code></p>
<p> Build SQL insert statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpdate(table, obj, options)</code></p>
<p> Build SQL statement for update</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDelete(table, obj, options)</code></p>
<p> Build SQL statement for delete</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlInitPool(options)</code></p>
<p> Setup PostgreSQL pool driver</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlConnect(options, callback)</code></p>
<p> Open PostgreSQL connection, execute initial statements</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlCacheIndexes(options, callback)</code></p>
<p> Cache indexes using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlBindValue(val, info)</code></p>
<p> Convert JS array into db PostgreSQL array format: {..}</p>
</li>
</ul>

<ul>
<li><p><code>db.sqliteInitPool(options)</code></p>
<p> Initialize local SQLite cache database by name, the db files are open in read only mode and are watched for changes,
if new file got copied from the master, we reopen local database</p>
</li>
</ul>

<ul>
<li><p><code>db.sqliteConnect(options, callback)</code></p>
<p> Common code to open or create local SQLite databases, execute all required initialization statements, calls callback
with error as first argument and database object as second</p>
</li>
</ul>

<ul>
<li><p><code>db.mysqlInitPool(options)</code></p>
<p> Setup MySQL database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.dynamodbInitPool(options)</code></p>
<p> Setup DynamoDB database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.mongodbInitPool(options)</code></p>
<p> MongoDB pool</p>
</li>
</ul>

<ul>
<li><p><code>db.cassandraInitPool(options)</code></p>
<p> Cassandra pool</p>
</li>
</ul>

<ul>
<li><p><code>db.leveldbInitPool(options)</code></p>
<p> Setup LevelDB database driver, this is simplified driver which supports only basic key-value operations,
table parameter is ignored, the object only supports the following properties name and value.
Options are passed to the LevelDB backend low level driver which is native LevelDB options using the same names:
<a href="http://leveldb.googlecode.com/svn/trunk/doc/index.html">http://leveldb.googlecode.com/svn/trunk/doc/index.html</a>
The database can only be shared by one process so if no unique options.db is given, it will create a unique database as using core.processId()</p>
</li>
</ul>

<ul>
<li><p><code>db.lmdbInitPool(options)</code></p>
<p> Setup LMDB database driver, this is simplified driver which supports only basic key-value operations,
table parameter is ignored, the object only supports the properties name and value in the record objects.
Options are passed to the LMDB backend low level driver as MDB_ flags, see <a href="http://symas.com/mdb/doc/">http://symas.com/mdb/doc/</a></p>
<ul>
<li>select and search actions support options.end property which defines the end condition for a range retrieval starting
with obj.name property. If not end is given, all records till the end will be returned.</li>
</ul>
</li>
</ul>

<h2 id="module-logger">Module: LOGGER</h2>
<ul>
<li><p><code>logger</code></p>
<p> Simple logger utility for debugging</p>
</li>
</ul>

<ul>
<li><p><code>logger.setSyslog (on)</code></p>
<p> Set or close syslog mode</p>
</li>
</ul>

<ul>
<li><p><code>logger.setFile(file)</code></p>
<p> Redirect logging into file</p>
</li>
</ul>

<ul>
<li><p><code>logger.setChannel(name)</code></p>
<p> Assign output channel to system logger, default is stdout</p>
</li>
</ul>

<ul>
<li><p><code>logger.printSyslog(level, msg)</code></p>
<p> syslog allows facility to be specified after log level like info:local0 for LOG_LOCAL0</p>
</li>
</ul>

<ul>
<li><p><code>logger.debug()</code></p>
<p> Make it one line to preserve space, syslog cannot output very long lines</p>
</li>
</ul>

<ul>
<li><p><code>logger.edebug()</code></p>
<p> Display error if first argument is an Error object or debug</p>
</li>
</ul>

<ul>
<li><p><code>logger.elog()</code></p>
<p> Display error if first argument is an Error object or log</p>
</li>
</ul>

<ul>
<li><p><code>logger.trace()</code></p>
<p> Print stack backtrace as error</p>
</li>
</ul>

<ul>
<li><p><code>logger.print()</code></p>
<p> Default write handler</p>
</li>
</ul>

<ul>
<li><p><code>logger.write(str)</code></p>
<p> Stream emulation</p>
</li>
</ul>

<h2 id="module-server">Module: SERVER</h2>
<ul>
<li><p><code>server.start()</code></p>
<p> Start the server process, call the callback to perform some initialization before launchng any server, just after core.init</p>
</li>
</ul>

<ul>
<li><p><code>server.startMonitor()</code></p>
<p> Start process monitor, running as root</p>
</li>
</ul>

<ul>
<li><p><code>server.startMaster()</code></p>
<p> Setup worker environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWeb(callback)</code></p>
<p> Create Express server, setup worker environment, call supplied callback to set initial environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebProcess()</code></p>
<p> Spawn web server from the master as a separate master with web workers, it is used when web and master processes are running on the same server</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebProxy()</code></p>
<p> Spawn web proxy from the master as a separate master with web workers</p>
</li>
</ul>

<ul>
<li><p><code>server.startProxy()</code></p>
<p> Start http proxy as standalone server process</p>
</li>
</ul>

<ul>
<li><p><code>server.startProcess()</code></p>
<p> Restart process with the same arguments and setup as a monitor for the spawn child</p>
</li>
</ul>

<ul>
<li><p><code>server.startWatcher()</code></p>
<p> Watch source files for modifications and restart</p>
</li>
</ul>

<ul>
<li><p><code>server.startRepl(port, bind)</code></p>
<p> Start command prompt on TCP socket, context can be an object with properties assigned with additional object to be accessible in the shell</p>
</li>
</ul>

<ul>
<li><p><code>server.startDaemon()</code></p>
<p> Create daemon from the current process, restart node with -daemon removed in the background</p>
</li>
</ul>

<ul>
<li><p><code>server.sleep(options, callback)</code></p>
<p> Sleep and keep a worker busy</p>
</li>
</ul>

<ul>
<li><p><code>server.shutdown(options, callback)</code></p>
<p> Shutdown the system immediately, mostly to be used in the remote jobs as the last task</p>
</li>
</ul>

<ul>
<li><p><code>server.respawn(callback)</code></p>
<p> If respawning too fast, delay otherwise schedule new process after short timeout</p>
</li>
</ul>

<ul>
<li><p><code>server.spawnProcess(args, skip, opts)</code></p>
<p> Start new process reusing global process arguments, args will be added and args in the skip list will be removed</p>
</li>
</ul>

<ul>
<li><p><code>server.runJob(job)</code></p>
<p> Run all jobs from the job spec at the same time, when the last job finishes and it is running in the worker process, the process
terminates.</p>
</li>
</ul>

<ul>
<li><p><code>server.execJob(job)</code></p>
<p> Execute job in the background by one of the workers, object must be known exported module
and method must be existing method of the given object. The method function must take options
object as its first argument and callback as its second argument.
More than one job can be specified, property of the object defines name for the job to run:
Example: { &#39;scraper.run&#39;: {}, &#39;server.shutdown&#39;: {} }
If the same object.method must be executed several times, prepend subsequent jobs with $
Example: { &#39;scraper.run&#39;: { &quot;arg&quot;: 1 }, &#39;$scraper.run&#39;: { &quot;arg&quot;: 2 }, &#39;$$scraper.run&#39;: { &quot;arg&quot;: 3 } }
Supported options by the server:</p>
<ul>
<li>runalways - no checks for existing job wth the same name should be done</li>
<li>runlast - run when no more pending or running jobs</li>
<li>runafter - specifies another job in canoncal form obj.method which must finish and not be pending in
order for this job to start, this implements chaining of jobs to be executed one after another
but submitted at the same time
Exampe: submit 3 jobs to run sequentially:<pre><code>        &#39;scraper.import&#39;
        { &#39;scraper.sync&#39;: { runafter: &#39;scraper.import&#39; } }
        { &#39;server.shutdown&#39;: { runafter: &#39;scraper.sync&#39; } }
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>server.launchJob(job, options, callback)</code></p>
<p> Remote mode, launch remote instance to perform scraping or other tasks
By default, shutdown the instance after job finishes unless noshutdown:1 is specified in the options</p>
</li>
</ul>

<ul>
<li><p><code>server.queueJob(job)</code></p>
<p> Run a job, the string is in the format:
object/method/name/value/name/value....
All spaces must be are replaced with %20 to be used in command line parameterrs</p>
</li>
</ul>

<ul>
<li><p><code>server.execQueue()</code></p>
<p> Process pending jobs, submit to idle workers</p>
</li>
</ul>

<ul>
<li><p><code>server.scheduleCronjob(spec, obj)</code></p>
<p> Create a new cron job, for remote jobs additonal property args can be used in the cron object to define
arguments to the instance backend process, properties must start with -</p>
<p>Example:</p>
<pre><code>    { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 */10 * * * *&quot;, &quot;job&quot;: &quot;server.processJobs&quot; },
    { &quot;type&quot;: &quot;local&quot;, &quot;cron&quot;: &quot;0 10 7 * * *&quot;, &quot;id&quot;: &quot;processQueue&quot;, &quot;job&quot;: &quot;api.processQueue&quot; }
    { &quot;type&quot;: &quot;remote&quot;, &quot;cron&quot;: &quot;0 5 * * * *&quot;, &quot;args&quot;: { &quot;-workers&quot;: 2 }, &quot;job&quot;: { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host1&quot; }, &quot;$scraper.run&quot;: { &quot;url&quot;: &quot;host2&quot; } } }
</code></pre></li>
</ul>

<ul>
<li><p><code>server.runCronjob(id)</code></p>
<p> Execute a cronjob by id now, it must have been scheduled already and id property must be specified in the crontab
When REPL is activated on the master server with -repl-port then connecting to the running master server via telnet it is possible to execute
cron jobs manually</p>
<p>Example:</p>
<pre><code>// Start the backend with repl-port like `bkjs run-backend -repl-port 2080`

# telnet localhost 2080
&gt; server.runCronjob(&quot;processQueue&quot;)
</code></pre></li>
</ul>

<ul>
<li><p><code>server.doJob(type, job, options)</code></p>
<p> Perform execution according to type</p>
</li>
</ul>

<ul>
<li><p><code>server.checkJob(type, job)</code></p>
<p> Verify job structure and permissions and return as an object if the job is a string</p>
</li>
</ul>

<ul>
<li><p><code>server.loadSchedules()</code></p>
<p> Load crontab from JSON file as list of job specs:</p>
<ul>
<li>type - local, remote, server<ul>
<li>local means spawn a worker to run the job function</li>
<li>remote means launch an AWS instance</li>
<li>server means run inside the master process, do not spawn a worker</li>
</ul>
</li>
<li>cron - cron time interval spec: &#39;second&#39; &#39;minute&#39; &#39;hour&#39; &#39;dayOfMonth&#39; &#39;month&#39; &#39;dayOfWeek&#39;</li>
<li>job - a string as obj.method or an object with job name as property name and the value is an object with<pre><code> additional options for the job passed as first argument, a job callback always takes options and callback as 2 arguments
</code></pre></li>
<li>args - additional arguments passwed to the backend in the command line for the remote jobs</li>
</ul>
<p>Example:</p>
<pre><code>    [ { &quot;type&quot;: &quot;local&quot;, cron: &quot;0 0 * * * *&quot;, job: &quot;scraper.run&quot; }, ..]
</code></pre></li>
</ul>

<ul>
<li><p><code>server.submitJob(options, callback)</code></p>
<p> Submit job for execution, it will be saved in the server queue and the master will pick it up later
options can specify:</p>
<ul>
<li>tag - job tag for execution, default is current jobTag, this can be used to run on specified servers only</li>
<li>job - an object with job spec</li>
<li>type - job type: local, remote, server</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>server.processJobs(options, callback)</code></p>
<p> Run submitted jobs, usually called from the crontab file in case of shared database, requires connection to the PG database
To run it from crontab add line(to run every 5 mins):</p>
<pre><code>    { type: &quot;server&quot;, cron: &quot;0 */5 * * * *&quot;, job: &quot;server.processJobs&quot; }
</code></pre></li>
</ul>


