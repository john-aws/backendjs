<head><title>Backendjs Documentation</title><link rel="stylesheet" href="css/pages.css"></head>
<h1 id="-backend-img-logo-png-backendjs-documentation"><img src="img/logo.png" alt="Backend"> Backendjs Documentation</h1>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#backend-platform-for-node-js"> Backend platform for node.js</a></li>
<li><a href="#requirements-and-dependencies"> Requirements and dependencies</a></li>
<li><a href="#installation"> Installation</a></li>
<li><a href="#quick-start"> Quick start</a></li>
<li><a href="#backend-runtime"> Backend runtime</a></li>
<li><a href="#application-structure"> Application structure</a><ul>
<li><a href="#modules"> Modules</a></li>
</ul>
</li>
<li><a href="#database-schema-definition"> Database schema definition</a></li>
<li><a href="#api-endpoints-provided-by-the-backend"> API endpoints provided by the backend</a><ul>
<li><a href="#accounts"> Accounts</a></li>
<li><a href="#status-enquiry"> Status enquiry</a></li>
<li><a href="#public-images-endpoint"> Public Images endpoint</a></li>
<li><a href="#icons"> Icons</a></li>
<li><a href="#file-api"> File API</a></li>
<li><a href="#connections"> Connections</a></li>
<li><a href="#locations"> Locations</a></li>
<li><a href="#messages"> Messages</a></li>
<li><a href="#counters"> Counters</a></li>
<li><a href="#status"> Status</a></li>
<li><a href="#data"> Data</a></li>
<li><a href="#pages"> Pages</a></li>
<li><a href="#system-api"> System API</a></li>
</ul>
</li>
<li><a href="#backend-directory-structure"> Backend directory structure</a></li>
<li><a href="#internal-backend-functions"> Internal backend functions</a></li>
<li><a href="#cache-configurations"> Cache configurations</a><ul>
<li><a href="#nanomsg"> nanomsg</a></li>
<li><a href="#memcached"> memcached</a></li>
<li><a href="#redis"> Redis</a></li>
</ul>
</li>
<li><a href="#pub-sub-configurations"> PUB/SUB configurations</a><ul>
<li><a href="#nanomsg"> nanomsg</a></li>
<li><a href="#redis"> Redis</a></li>
<li><a href="#rabbitmq"> RabbitMQ</a></li>
</ul>
</li>
<li><a href="#security-configurations"> Security configurations</a><ul>
<li><a href="#api-only"> API only</a></li>
<li><a href="#secure-web-site-client-verification"> Secure Web site, client verification</a></li>
<li><a href="#secure-web-site-backend-verification"> Secure Web site, backend verification</a></li>
</ul>
</li>
<li><a href="#websockets-connections"> WebSockets connections</a></li>
<li><a href="#the-backend-provisioning-utility-bkjs"> The backend provisioning utility: bkjs</a></li>
<li><a href="#deployment-use-cases"> Deployment use cases</a><ul>
<li><a href="#custom-aws-instance-setup"> Custom AWS instance setup</a></li>
<li><a href="#custom-aws-instance"> Custom AWS instance</a></li>
<li><a href="#aws-beanstalk-deployment"> AWS Beanstalk deployment</a></li>
<li><a href="#configure-http-port"> Configure HTTP port</a></li>
</ul>
</li>
<li><a href="#security"> Security</a></li>
<li><a href="#backend-framework-development-mac-os-x-developers-"> Backend framework development (Mac OS X, developers)</a></li>
<li><a href="#author"> Author</a></li>
<li><a href="#configuration-parameters">Configuration parameters</a></li>
<li>Javascript API functions<ul>
<li><a href="#module-api">api</a></li>
<li><a href="#module-app">app</a></li>
<li><a href="#module-aws">aws</a></li>
<li><a href="#module-core">core</a></li>
<li><a href="#module-db">db</a></li>
<li><a href="#module-ipc">ipc</a></li>
<li><a href="#module-logger">logger</a></li>
<li><a href="#module-metrics">metrics</a></li>
<li><a href="#module-msg">msg</a></li>
<li><a href="#module-server">server</a></li>
</ul>
</li>
</ul>
<h1 id="backend-platform-for-node-js">Backend platform for node.js</h1>
<p>General purpose backend framework. The primary goal is to have a scalable platform for running and managing node.js
servers for Web services implementation.</p>
<p>This framework only covers the lower portion of the Web services system:
node.js processes, HTTP servers, basic API functinality, database access, caching, messaging between processes,
metrics and monitoring, a library of tools for developing node.js servers.</p>
<p>For the UI and presentation layer there are no restrictions what to use as long as it can run on top of the Express server.</p>
<p>Features:</p>
<ul>
<li>Exposes a set of Web service APIs over HTTP(S) using Express framework.</li>
<li>Database API supports Sqlite, PostgreSQL, MySQL, DynamoDB, Cassandra, MongoDB, Redis with all basic operations behaving the
same way allowing to switch databases without changing the code.</li>
<li>Database driver for LevelDB, LMDB, CouchDB, Riak, ElasticSearch support only a subset of all database operations</li>
<li>Easily extendable to support any kind of database, provides a database driver on top of Redis with all supported methods.</li>
<li>Provides accounts, connections, locations, messaging and icons APIs with basic functionality for a qucik start.</li>
<li>Supports crontab-like and on-demand scheduling for local and remote(AWS) jobs.</li>
<li>Authentication is based on signed requests using API key and secret, similar to Amazon AWS signing requests.</li>
<li>Runs web server as separate processes to utilize multiple CPU cores.</li>
<li>Local jobs are executed by spawned processes</li>
<li>Supports WebSockets connections and process them with the same Express routes as HTTP requests</li>
<li>Supports several cache modes(Redis, memcached, LRU) for the database operations.</li>
<li>Supports several PUB/SUB modes of operations using nanomsg, Redis, RabbitMQ.</li>
<li>Supports common database operations (Get, Put, Del, Update, Select) for all databases using the same DB API.</li>
<li>ImageMagick is compiled as C++ module for in-process image scaling.</li>
<li>nanomsg interface for messaging between processes and servers.</li>
<li>REPL(command line) interface for debugging and looking into server internals.</li>
<li>Geohash based location searches supported by all databases drivers.</li>
<li>Supports push notifications for mobile devices, APN and GCM</li>
<li>Supports HTTP(S) reverse proxy mode where multiple Web workers are load-balanced by the proxy
server running in the master process instead of relying on the OS scheduling between processes listening on the same port.</li>
<li>Can be used with any MVC or other types of frameworks that work on top or with the Express server.</li>
<li>Hosted on <a href="https://github.com/vseryakov/backendjs">github</a>, <a href="http://backendjs.io">http://backendjs.io</a> or <a href="http://vseryakov.github.io/backendjs">http://vseryakov.github.io/backendjs</a>, BSD licensed.</li>
</ul>
<p>Check out the <a href="http://backendjs.io">Documentation</a> for more details.</p>
<h1 id="requirements-and-dependencies">Requirements and dependencies</h1>
<p>The module supports several databases and includes ImageMagick interface. In order for such interfaces to be compiled the software must be installed
on the system before installing the backendjs. Not everything is required, if not available the interface will be skipped.</p>
<p>The optional packages that the backendjs uses if available(resolving packages is done with <em>pkg-config</em>):</p>
<ul>
<li>nanomsg - messaging, caching and pub/sub services</li>
<li>ImageMagick - image manipulation</li>
<li>libpq - PostgreSQL database driver</li>
<li>libmysql - MySQL database driver</li>
</ul>
<p>Installing dependencies on CentOS:</p>
<pre><code>    yum -y install libpng-devel libjpeg-turbo-devel postgresql-devel mysql-devel
</code></pre><p>Installing dependencies on Mac OS X using macports:</p>
<pre><code>    port install libpng jpeg mysql56 postgresql93
</code></pre><h1 id="installation">Installation</h1>
<p>To install the module with all optional dependencies if they are available in the system</p>
<p>Note: if for example ImageMagick is not istalled it will be skipped, same goes to all database drivers(PostgreSQL, MySQL) and nanomsg.</p>
<pre><code>    npm install backendjs
</code></pre><p>To force internal nanomsg and ImageMagick to be compiled in the module the following command must be used:</p>
<pre><code>    npm install backendjs --backendjs_nanomsg --backendjs_imagemagick
</code></pre><p>This may take some time because of compiling required dependencies like ImageMagick, nanomsg. They are not required in all
applications but still part of the core of the system to be available once needed.</p>
<p>To install from the git</p>
<pre><code>    npm install git+https://github.com/vseryakov/backendjs.git
</code></pre><p>or simply</p>
<pre><code>    npm install vseryakov/backendjs
</code></pre><h1 id="quick-start">Quick start</h1>
<ul>
<li><p>Simplest way of using the backendjs, it will start the server listening on port 8000</p>
<pre><code>  # node
  &gt; var bk = require(&#39;backendjs&#39;)
  &gt; bk.server.start()
</code></pre></li>
<li><p>Same but using the helper tool, by default it will use embedded Sqlite database and listen on port 8000</p>
<pre><code>  bkjs run-backend
</code></pre></li>
<li><p>To start the server and connect to the DynamoDB (command line parameters can be saved in the etc/config file, see below about config files)</p>
<pre><code>  bkjs run-backend -db-pool dynamodb -db-dynamodb-pool default -aws-key XXXX -aws-secret XXXX
</code></pre></li>
<li><p>or to the PostgreSQL server, database backend</p>
<pre><code>  bkjs run-backend -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend
</code></pre></li>
<li><p>All commands above will behave exactly the same, all required tables will be automatically created</p>
</li>
<li><p>While the local backendjs is runnning the documentation is always available when the backend Web server is running at <a href="http://localhost:8000/doc.html">http://localhost:8000/doc.html</a></p>
</li>
<li><p>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> for the Web console to test API requests.
For this example let&#39;s create a couple of accounts, type and execute the following URLs in the Web console</p>
<pre><code>  /account/add?name=test1&amp;secret=test1&amp;login=test1@test.com
  /account/add?name=test2&amp;secret=test2&amp;login=test2@test.com&amp;gender=m&amp;alias=Test%20User&amp;birthday=1980-01-01
</code></pre></li>
</ul>
<ul>
<li>Now login with any of the accounts above, click on <em>Login</em> at the top-right corner and enter &#39;test1&#39; as login and &#39;test1&#39; as secret in the login popup dialog.</li>
<li><p>If no error message appeared after the login, try to get your current account details:</p>
<pre><code>  /account/get
</code></pre></li>
<li><p>To see all public fields for all accounts just execute</p>
<pre><code>  /account/select
</code></pre></li>
<li><p>Shutdown the backend by pressing Ctrl-C</p>
</li>
<li><p>To make your own custom Web app, create a new directory (somewhere else) to store your project and run the following command from that directory:</p>
<pre><code>  bkjs init-app
</code></pre></li>
<li><p>The app.js file is created in your project directory with 2 additional API endpoints <code>/test/add</code> and <code>/test/[0-9]</code> to show the simplest way
of adding new tables and API commands.</p>
</li>
<li>The app.sh script is created for convenience in the development process, it specifies common arguments and can be customized as needed.</li>
<li><p>Run new application now, it will start the Web server on port 8000:</p>
<pre><code>  ./app.sh
</code></pre></li>
</ul>
<ul>
<li>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> and issue command <code>/test/add?id=1&amp;name=1</code> and then <code>/test/1</code> commands in the console to see it in action</li>
<li><p>Change in any of the source files will make the server restart automatically letting you focus on the source code and not server management, this mode
is only enabled by default in development mode, check app.sh for parameters before running it in the production.</p>
</li>
<li><p>To start node.js shell with backendjs loaded and initialized, all command line parameters apply to the shell as well</p>
<pre><code>  ./app.sh -shell
</code></pre></li>
<li><p>To access the database while in the shell</p>
<pre><code>  &gt; db.select(&quot;bk_account&quot;, {}, function(err, rows) { console.log(rows) });
  &gt; db.select(&quot;bk_account&quot;, {}, db.showResult);
  &gt; db.add(&quot;bk_account&quot;, { login: &#39;test2&#39;, secret: &#39;test2&#39;, name&#39; Test 2 name&#39;, gender: &#39;f&#39; }, db.showResult);
  &gt; db.select(&quot;bk_account&quot;, { gender: &#39;m&#39; }, db.showResult);
</code></pre></li>
<li><p>To add users from the command line</p>
<pre><code>  bksh -add-user login test sectet test name TestUser email test@test.com
</code></pre></li>
<li><p>To see current metrics run the command in the console &#39;/system/stats/get&#39;</p>
</li>
<li><p>To see charts about accumulated metrics go to <a href="http://localhist:8000/metrics.html">http://localhist:8000/metrics.html</a></p>
</li>
</ul>
<h1 id="backend-runtime">Backend runtime</h1>
<p>When the backendjs server starts it spawns several processes the perform different tasks.</p>
<p>There are 2 major tasks of the backend that can be run at the same time or in any combination:</p>
<ul>
<li>a Web server (server) with Web workers (web)</li>
<li>a job scheduler (master)</li>
</ul>
<p>These features can be run standalone or under the guard of the monitor which tracks all running processes and restarted any failed one.</p>
<p>This is the typical output from the ps command on Linux server:</p>
<pre><code>        root       891  0.0  0.6 1071632 49504 ?       Ssl  14:33   0:01 backendjs: monitor
        backend    899  0.0  0.6 1073844 52892 ?       Sl   14:33   0:01 backendjs: master
        root       908  0.0  0.8 1081020 68780 ?       Sl   14:33   0:02 backendjs: server
        backend    917  0.0  0.7 1072820 59008 ?       Sl   14:33   0:01 backendjs: web
        backend    919  0.0  0.7 1072820 60792 ?       Sl   14:33   0:02 backendjs: web
</code></pre><p>To enable any task a command line parameter must be provided, it cannot be specified in the config file. The <code>bkjs</code> utility supports several
commands that simplify running the backend in different modes.</p>
<ul>
<li><code>bkjs run-backend</code> - runs the Web server and the jobs scheduler in debug mode with watching source files for changes, this is the common command to be used
 in development, it passes the command line switches: <code>-debug -watch -web -master</code></li>
<li><code>bkjs run-server</code> - this command is supposed to be run at the server startup, it runs in the backgroud and the monitors all tasks,
 the command line parameters are: <code>-daemon -monitor -master -syslog</code></li>
<li><code>bkjs run</code> - this command runs the Web server and the job scheduler without any other parameters, all aditional parameters can be added in the command line, this command
 is a barebone elper to be used with any other custom settings.</li>
<li><code>bkjs run-shell</code> or <code>bksh</code> - start backendjs shell, no API or Web server is initialized, only the database pools</li>
</ul>
<h1 id="application-structure">Application structure</h1>
<p>The main puspose of the backendjs is to provide API to access the data, the data can be stored in the database or some other way
but the access to that data will be over HTTP and returned back as JSON. This is default functionality but any custom application
may return data in whatever format is required.</p>
<p>Basically the backendjs is a Web server with ability to perform data processing using local or remote jobs which can be scheduled similar to Unix cron.</p>
<p>The principle behind the system is that nowadays the API services just return data which Web apps or mobiles apps can render to
the user without the backend involved. It does not mean this is simple gateway between the database, in many cases it is but if special
processing of the data is needed before sending it to the user, it is possible to do and backendjs provides many convenient helpers and tools for it.</p>
<p>When the API layer is initialized, the api module contains <code>app</code> object which is an Express server.</p>
<p>Special module <code>app</code> or namespace is designated to be used fpr application developent. This module is available the same way as api or core
which makes it easy to refer and extend with additional methods and structures.</p>
<p>The typical structure of a backendjs application is the following (created by the bkjs init-app command):</p>
<pre><code>        var bkjs = require(&#39;backendjs&#39;);
        var api = bkjs.api;
        var app = bkjs.app;
        var db = bkjs.db;

        // Describe the tables or data model
        api.describeTables({
            ...
        });

        // Optionally customize the Express environment, setup MVC routes or else, options.app is the Express server
        app.configureMiddleware = function(options, callback)
        {
            ...
            callback()
        }

        // Register API endpoints, i.e. url callbacks
        app.configureWeb = function(options, callback)
        {
            api.app.get(&#39;/some/api/endpoint&#39;, function(req, res) { ... });
            ...
            callback();
        }

        // Optionally register post processing of the returned data from the default calls
        api.registerPostProcess(&#39;&#39;, /^\/account\/([a-z\/]+)$/, function(req, res, rows) { ... });
        ...

        // Optionally register access permissions callbacks
        api.registerAccessCheck(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
        api.registerPreProcess(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
        ...

        bkjs.server.start();
</code></pre><p>Except the <code>app.configureWeb</code> and <code>server.start()</code> all other functions are optional, they are here for the sake of completness of the example. Also
because running the backend involves more than just running web server many things can be setup using the configuration options like common access permissions,
configuration of the cron jobs so the amount of code to be written to have fully functionaning production API server is not that much, basically only
request endpoint callbacks must be provided in the application.</p>
<p>As with any node.js application, node modules are the way to build and extend the functionality, backendjs does not restrict how
the application is structured.</p>
<h2 id="modules">Modules</h2>
<p>Another way to add functionality to the backend is via external modules specific to the backend, these modules are loaded on startup from the backend
home subdirectory <code>modules/</code>. The format is the same as for regular node.js modules and only top level .js files are loaded on the backend startup.</p>
<p>Once loaded they have the same access to the backend as the rest of the code, the only difference is that they reside in the backend home and
can be shipped regardless of the npm, node nodules and other env setup. These modules are exposed in the <code>core.modules</code> the same way as all other core submodules.
methods.</p>
<p>Let&#39;s assuming the modules/ contains file facebook.js which implements custom FB logic:</p>
<pre><code>        var bkjs = require(&quot;backendjs&quot;);
        var fb = {}
        module.exports = fb;
        fb.configureWeb = function(options, callback) {
        }
</code></pre><p>This is the main app code:</p>
<pre><code>        var bkjs = require(&quot;backendjs&quot;);
        var core = bkjs.core;
        var fb;

        // Using facebook module in the main app
        api.app.get(&quot;some url&quot;, function(req, res) {

            fb = core.modules.facebook;
            fb.makeRequest(function(err, data) {
                ...
            });
        });

        bkj.server.start()
</code></pre><h1 id="database-schema-definition">Database schema definition</h1>
<p>The backend support multiple databases and provides the same db layer for access. Common operations are supported and all other specific usage can be achieved by
using SQL directly or other query language supported by any particular database.
The database operations supported in the unified way provide simple actions like <code>db.get, db.put, db.update, db.del, db.select</code>. The <code>db.query</code> method provides generic
access to the database driver and executes given query directly by the db driver, it can be SQL or other driver specific query request.</p>
<p>Before the tables can be queried the schema must be defined and created, the backend db layer provides simple functions to do it:</p>
<ul>
<li><p>first the table needs to be described, this is achieved by creating a Javascript object with properties describing each column, multiple tables can be described
at the same time, for example lets define album table and make sure it exists when we run our application:</p>
<pre><code>      api.describeTables({
          album: {
              id: { primary: 1 },                         // Primary key for an album
              name: { pub: 1 },                           // Album name, public column
              mtime: { type: &quot;bigint&quot; },                  // Modification timestamp
          },
          photo: {
              album_id: { primary: 1 },                   // Combined primary key
              id: { primary: 1 },                         // consiting of album and photo id
              name: { pub: 1, index: 1 },                 // Photo name or description, public column with the index for faster search
              mtime: { type: &quot;bigint&quot; }
          }
       });
</code></pre></li>
<li><p>the system will automatically create the album and photos tables, this definition must remain in the app source code
and be called on every app startup. This allows 1) to see the db schema while working with the app and 2) easily maintain it by adding new columns if
necessary, all new columns will be detected and the database tables updated accordingly. And it is all Javascript, no need to learn one more language or syntax
to maintain database tables.</p>
</li>
</ul>
<p>Each database may restrict how the schema is defined and used, the db layer does not provide an artificial layer hiding all specifics, it just provides the same
API and syntax, for example, DynamoDB tables must have only hash primary key or combined hash and range key, so when creating table to be used with DynamoDB, only
one or two columns can be marked with primary property while for SQL databases the composite primary key can conisit of more than 2 columns.</p>
<p>The backendjs always creates several tables in the configured database pools by default, these tables are required to support default API functionality and some
are required for backend opertions. Refer below for the Javascript modules documenttion that described which tables are created by default. In the custom applications
same <code>api.describeTables</code> method can modify columns in the default table and add more columns if needed.</p>
<p>For example, to make age and some other columns in the accounts table public and visible by other users with additional columns the following can be
done in the <code>api.initApplication</code> method. It will extend the bk_account table and the application can use new columns the same way as the already existing columns.
Using the birthday column we make &#39;age&#39; property automatically calculated and visible in the result, this is done by the internal method <code>api.processAccountRow</code> which
is registered as post process callback for the bk_account table. The computed property <code>age</code> will be returned because it is not present in the table definition
and all properties not defined and configured are passed as is.</p>
<p>The cleanup of the public columns is done by the <code>api.sendJSON</code> which is used by all API routes when redy to send data back to the client. If any postprocess
hooks are registered and return data itself then it is the hook responsibility to cleanup non-public columns.</p>
<pre><code>        api.describeTables({
                bk_account: {
                       gender: { pub: 1 },
                       birthday: {},
                       ssn: {},
                       salary: { type: &quot;int&quot; },
                       occupation: {},
                       home_phone: {},
                       work_phone: {},
        });

        app.configureWeb = function(options, callback)
        {
            db.setProcessRow(&quot;bk_account&quot;, this.processAccountRow);
            ...
            callback();
        }
        app.processAccountRow = function(row, options, cols)
        {
            if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
            return row;
        }
</code></pre><h1 id="api-endpoints-provided-by-the-backend">API endpoints provided by the backend</h1>
<h2 id="accounts">Accounts</h2>
<p>The accounts API manages accounts and authentication, it provides basic user account features with common fields like email, name, address.</p>
<ul>
<li><p><code>/account/get</code></p>
<p>Returns information about current account or other accounts, all account columns are returned for the current account and only public columns
returned for other accounts. This ensures that no private fields ever be exposed to other API clients. This call also can used to login into the service or
verifying if the given login and secret are valid, there is no special login API call because each call must be signed and all calls are stateless and independent.</p>
<p>Parameters:</p>
<ul>
<li>no id is given, return only one current account record as JSON</li>
<li>id=id,id,... - return information about given account(s), the id parameter can be a single account id or list of ids separated by comma</li>
<li>_session - after successful login setup a session with cookies so the Web app can perform requests without signing every request anymore</li>
<li>_accesstoken - after successful login, return new access token that ca be used to make requests without signing every request, it can be
 passed in the query or headers with the name <code>bk-access-token</code></li>
</ul>
<p>Note: When retrieving current account, all properties will be present including the location, for other accounts only the properties marked as <code>pub</code> in the
<code>bk_account</code> table will be returned.</p>
<p>Response:</p>
<pre><code>      { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
        &quot;alias&quot;: &quot;Test User&quot;,
        &quot;name&quot;: &quot;Real Name&quot;,
        &quot;mtime&quot;: 1391824028,
        &quot;latitude&quot;: 34,
        &quot;longitude&quot;: -118,
        &quot;geohash&quot;: &quot;9qh1&quot;,
        &quot;login&quot;: &quot;testuser&quot;,
      }
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/add</code></p>
<p>Add new account, all parameters are the columns from the <code>bk_account</code> table, required columns are: <strong>name, secret, login</strong>.</p>
<p>By default, this URL is in the list of allowed paths that do not need authentication, this means that anybody can add an account. For the real
application this may not be a good choice so the simplest way to disable it to add api-disallow-path=^/account/add$ to the config file or
specify in the command line. More complex ways to perform registration will require adding pre and.or post callbacks to handle account registration
for example with invitation codes....</p>
<p>In the table <code>bk_auth</code>, the column type is used to distinguish between account roles, by default only account with type <code>admin</code> can
add other accounts with this type specified, this column can also be used in account permissions implementations. Because it is in the bk_auth table,
all columns of this table are available as <code>req.account</code> object after the successful authentication where req is Express request object used in the middleware
parameters.</p>
<p><em>Note: secret and login can be anything, the backend does not require any specific formats and does not process the contents of the login/sectet fields. In the
Web client if Backendjs.scramble is set to true then login and secret are replaced by HMAC values derived from the login and sent to the server, no actual login/secret
are ever saved, only used in the login form</em></p>
<p>Example:</p>
<pre><code>      /account/add?name=test&amp;login=test@test.com&amp;secret=test123&amp;gender=f&amp;phone=1234567
</code></pre></li>
</ul>
<p>  How to make an account as admin</p>
<pre><code>        # Run backend shell
        bkjs run-shell

        # Update record by login
        &gt; db.update(&quot;bk_auth&quot;, { login: &#39;login@name&#39;, type: &#39;admin&#39; });
</code></pre><ul>
<li><p><code>/account/select</code></p>
<p>Return list of accounts by the given condition, calls <code>db.select</code> for bk_account table. Parameters are the column values to be matched and
all parameters starting with underscore are control parameters that goes into options of the <code>db.select</code> call with underscore removed. This will work for SQL
databases only because DynamoDB or Cassandra will not search by non primary keys. In the DynamoDB case this will run ScanTable action which will be very expensive for
large tables. Supports special query parameters <code>_select,_ops</code>, see docs about <code>db.select</code> for more info.</p>
<p>Example:</p>
<pre><code>      /account/search?email=test&amp;_ops=email,begins_with
      /account/search?name=test
</code></pre></li>
</ul>
<p>  Response:</p>
<pre><code>        {  &quot;data&quot;: [{
                      &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
                      &quot;alias&quot;: &quot;Test User1&quot;,
                      &quot;name&quot;: &quot;User1&quot;,
                      &quot;mtime&quot;: 1391824028,
                      &quot;login&quot;: &quot;test1&quot;,
                    },
                    {
                      &quot;id&quot;: &quot;57d07a4e2824fc43bd669f6c8e04d6c3&quot;,
                      &quot;alias&quot;: &quot;Test User2&quot;,
                      &quot;name&quot;: &quot;User2&quot;,
                      &quot;mtime&quot;: 1391824028,
                      &quot;login&quot;: &quot;test2&quot;,
                    }],
            &quot;next_token&quot;: &quot;&quot;
        }
</code></pre><ul>
<li><p><code>/account/del</code></p>
<p>Delete current account, after this call no more requests will be authenticated with the current credentials</p>
</li>
<li><p><code>/account/update</code></p>
<p>Update current account with new values, the parameters are columns of the table <code>bk_account</code>, only columns with non empty values will be updated.</p>
<p>Example:</p>
<pre><code>      /account/update?name=New%2BName&amp;alias=Hidden%2BName&amp;gender=m
</code></pre></li>
<li><p><code>/account/put/secret</code></p>
<p>Change account secret for the current account, no columns except the secret will be updated and expected.</p>
<p>Parameters:</p>
<ul>
<li>secret - new secret for the account</li>
</ul>
<p>Example:</p>
<pre><code>      /account/put/secret?secret=blahblahblah
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/subcribe</code></p>
<p>Subscribe to account events delivered via HTTP Long Poll, a client makes the connection and waits for events to come, whenever
somebody updates the account&#39;s counter or send a message or creates a connection to this account the event about it will be sent to this HTTP
connection and delivered as JSON object. This is not a persistent queue so if not listening, all events will just be ignored, only events published
since the connect will be delivered. To specify what kind of events needs to be delivered, <code>match</code> query parameter can be specified which is a
RegExp of the whole event body string.</p>
<p><em>Note: On the server side there is a config parameter <code>api-subscribe-interval</code> which defines how often to deliver notifications, by default it is 5 seconds which means
only every 5 seconds new events will be delivered to the Web client, if more than one event happened, they all accumulate and will be sent as a JSON list.</em></p>
<p>Example:</p>
<pre><code>  /account/subscribe
  /account/subscribe?match=connection/add.*type:*like

  // To run in the browser:
  (function poll() {
      Backendjs.send({ url: &quot;/account/subscribe&quot;, complete: poll }, function(data) {
          console.log(&quot;received event:&quot;, data);
       });
   })();
</code></pre><p>Response:</p>
<pre><code>  [ { &quot;path&quot;: &quot;/message/add&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;1&quot; },
    { &quot;path&quot;: &quot;/counter/incr&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;like,invite&quot; } },
    { &quot;path&quot;: &quot;/connection/add&quot;, &quot;mtime&quot;: 1223345545, &quot;type&quot;: &quot;like&quot; } ]
</code></pre></li>
<li><p><code>/account/select/icon</code></p>
<p>Return a list of available account icons, icons that have been uploaded previously with /account/put/icon calls. The <code>url</code> property is an URL to retrieve this particular icon.</p>
<p>Parameters:</p>
<ul>
<li>id - if specified then icons for the given account will be returned</li>
</ul>
<p>Example:</p>
<pre><code>  /account/select/icon?id=12345
</code></pre><p>Response:</p>
<pre><code>  [ { id: &#39;12345&#39;, type: &#39;1&#39;, url: &#39;/account/get/icon?id=12345&amp;type=1&#39; },
    { id: &#39;12345&#39;, type: &#39;2&#39;, url: &#39;/account/get/icon?id=12345&amp;type=2&#39; } ]
</code></pre></li>
<li><p><code>/account/get/icon</code></p>
<p>Return an account icon, <em>the icon is returned in the body as binary BLOB</em>, if no icon with specified type exists, i.e. never been uploaded then 404 is returned.</p>
<p>Parameters:</p>
<ul>
<li>type - a number from 0 to 9 or any single letter a..z which defines which icon to return, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/get/icon?type=2
</code></pre></li>
</ul>
<ul>
<li><p><code>/account/put/icon</code></p>
<p>Upload an account icon, once uploaded, the next <code>/account/get</code> call will return propertis in the format <code>iconN</code> wheer N is any of the
type query parameters specified here, for example if we uploaded an icon with type 5, then /account/get will return property icon5 with the URL
to retrieve this icon.
<em>By default all icons uploaded only accessible for the account which uploaded them.</em></p>
<p>Parameters:</p>
<ul>
<li>type - icon type, a number between 0 and 9 or any single letter a..z, if not specified 0 is used</li>
<li>icon - can be passed as base64 encoded image in the query,<ul>
<li>can be passed as base64 encoded string in the body as JSON, like: { type: 0, icon: &#39;iVBORw0KGgoA...&#39; },
for JSON the Content-Type HTTP headers must be set to <code>application/json</code> and data should be sent with POST request</li>
<li>can be uploaded from the browser using regular multi-part form</li>
</ul>
</li>
<li>acl_allow - icon access permissions:<ul>
<li>&quot;&quot; (empty) - only own account can access</li>
<li>all - public, everybody can see this icon</li>
<li>auth - only authenticated users can see this icon</li>
<li>id,id.. - list of account ids that can see this account</li>
</ul>
</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if th eimage width is less than given keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg, jp2</li>
</ul>
<p>Example:</p>
<pre><code>  /account/put/icon?type=1&amp;icon=iVBORw0KGgoAAAANSUhEUgAAAAcAAAAJCAYAAAD+WDajAAAABGdBTUEAALGPC/xhBQAAAAlwSFlzAAAOwgAADs....
</code></pre></li>
<li><p><code>/account/del/icon</code></p>
<p>Delete account icon</p>
<p>Parameters:</p>
<ul>
<li>type - what icon to delete, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/icon/del?type=1
</code></pre></li>
</ul>
<h3 id="status-enquiry">Status enquiry</h3>
<p>When running with AWS load balancer there should be a url that a load balancer polls all the time and this must be very quick and lightweight request. For this
purpose there is an API endpoint <code>/ping</code> that just responds with status 200. It is not open by default, the <code>allow-path</code> or other way to allow non-authenticted access
needs to be configured. This is to be able to control how pinging can be perform in the apps in cae it is not simple open access.</p>
<h2 id="public-images-endpoint">Public Images endpoint</h2>
<p>This endpoint can server any icon uploaded to the server for any account, it is supposed to be a non-secure method, i.e. no authentication will be performed and no signagture
will be needed once it is confgiured which prefix can be public using <code>api-allow</code> or <code>api-allow-path</code> config parameters.</p>
<p>The format of the endpoint is:</p>
<pre><code>/image/prefix/id/type

Example:

    # Configure accounts icons to be public in the etc/config
    api-allow-path=/image/account/

    # Or pass in the command line
    ./app.sh -api-allow-path /image/account/

    # Make requests
    /image/account/12345/0
    /image/account/12345/1

    #Return icons for account 12345 for types 0 and 1
</code></pre><h2 id="icons">Icons</h2>
<p>The icons API provides ability for an account to store icons of different types. Each account keeps its own icons separate form other
accounts, within the account icons can be separated by <code>prefix</code> which is just a namespace assigned to the icons set, for example to keep messages
icons separate from albums, or use prefix for each separate album. Within the prefix icons can be assigned with unique type which can be any string.</p>
<p>Prefix and type can consist from alphabetical characters and numbers, dots, underscores and dashes: [a-z0-9._-]. This means, they are identificators, not real titles or names,
a special mapping between prefix/type and album titles for example needs to be created separately.</p>
<p>The supposed usage for type is to concatenate common identifiers first with more specific to form unique icon type which later can be queried
by prefix or exactly by icon type. For example album id can be prefixed first, then sequential con number like album1:icon1, album1:icon2....
then retrieving all icons for an album would be only query with album1: prefix.</p>
<ul>
<li><p><code>/icon/get</code></p>
<p> Return icon for the current account in the given prefix, icons are kept on the local disk in the directory
 configured by <code>-api-images-dir</code> parameter(default is images/ in the backend directory). Current account id is used to keep icons
 separate from other accounts. Icon presense is checked in the bk_icon table before returning it and if any permissions are set in
 the <code>acl_allow</code> column it will be checked if this icon can be returned.</p>
<p>The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be specified, this defines the icons namespace</li>
<li><code>type</code> is used to specify unique icon created with such type which can be any string.</li>
</ul>
</li>
<li><p><code>/icon/put</code></p>
<p>Upload new icon for the given account in the folder prefix, if type is specified it creates an icons for this type to separate
multiple icons for the same prefix. <code>type</code> can be any string consisting from alpha and digits characters. It creates a record in the bk_icon
table with all the paramaters passed.</p>
<p>The following parameters can be used:</p>
<ul>
<li>prefix - prefix for the icons, requried</li>
<li>descr - optional description of the icon</li>
<li>latitude, longitude - optional coordinates for the icon</li>
<li>acl_allow - allow access permissions, see <code>/account/put/icon</code> for the format and usage</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if th eimage width is less than given keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg</li>
</ul>
</li>
<li><p><code>/icon/upload</code></p>
<p> Upload a new image and store on the server, no record is created in bk_icon table, just simple image upload,
 but all the same query parameters as for /icon/put are accepted. Returns an JSON object with url property being the full path
 to the uploaded image.</p>
</li>
<li><p><code>/icon/del</code></p>
<p> Delete the default icon for the current account in the folder prefix or by type</p>
</li>
<li><p><code>/icon/select</code></p>
<p>Return list of available icons for the given prefix adn type, all icons starting with prefix/type will be returned,
the <code>url</code> property will provide full URL to retrieve the icon contents</p>
<p>Example:</p>
<pre><code>  /icon/select?prefix=album&amp;type=me
  /icon/select?prefix=album&amp;type=12345
</code></pre><p>Responses:</p>
<pre><code>  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-1&#39;, icon: &#39;/icon/get?prefix=album&amp;type=me1&#39; },
    { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-2&#39;, icon: &#39;/icon/get?prefix=album&amp;type=me2&#39; } ]

  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;12345-f0deaa81a&#39;, icon: &#39;/icon/get?prefix=album&amp;type=12345-f0deaa81a&#39; } ]
</code></pre></li>
</ul>
<h2 id="file-api">File API</h2>
<p>The file API provides ability to store and retrieve files. The operations are similar to the Icon API.</p>
<ul>
<li><p><code>/file/get</code></p>
<p>  Return a file with given prefix and name, the contents are returned in the response body.</p>
<p>  The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be provided, defines the namescape where the file is stored</li>
<li><code>name</code> - name of the file, required</li>
</ul>
</li>
<li><p><code>/file/put</code></p>
<p>  Store a file on the backend, the file can be sent using form multipart upload or as JSON</p>
<p>  The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be provided, defines the namescape where the file is stored</li>
<li><code>name</code> - name of the file, required</li>
<li><code>_name</code> - name of the property that contaibs the file contents, for use with JSON or defines the name of the file attribute for multipart upload</li>
<li><code>_tm</code> - append the current timestamp to the file name</li>
<li><code>_ext</code> - extention to be assign to the file, otherwise the actual extension from the file name is used</li>
</ul>
</li>
<li><p><code>/file/del</code></p>
<p>  Delete file, prefix and name must be given</p>
</li>
</ul>
<h2 id="connections">Connections</h2>
<p>The connections API maintains two tables <code>bk_connection</code> and <code>bk_reference</code> for links between accounts of any type. bk_connection table maintains my
links, i.e. when i make explicit connection to other account, and bk_reference table is automatically updated with reference for that other account that i made
a connection with it. No direct operations on bk_reference is allowed.</p>
<ul>
<li><code>/connection/add</code></li>
<li><p><code>/connection/put</code>
Create or replace a connection between two accounts, required parameters are:</p>
<ul>
<li><code>id</code> - id of account to connect to</li>
<li><code>type</code> - type of connection, like,dislike,....</li>
<li>_connected - the reply will contain a connection record if the other side of our connection is connected to us as well</li>
<li>_publish - notify another account about this via pub/sub messaging system if it is active</li>
<li>_noreference - do not create the reference record for this connection</li>
<li>_nocounter - do not auto increment any counters</li>
</ul>
<p>This call automatically creates a record in the bk_reference table which is reversed connection for easy access to information like
&#39;&#39;who is connected to me&#39;&#39; and auto-increment like0, like1 counters for both accounts in the bk_counter table.</p>
<p>Also, this call updates the counters in the <code>bk_counter</code> table for my account which match the connection type, for example if the type of
connection is &#39;invite&#39; and the <code>bk_counter</code> table contain 2 columns <code>invite0</code> and <code>invite1</code>, then both counters will be increased.</p>
<p>Example:</p>
<pre><code>  /connection/add?id=12345&amp;type=invite&amp;state=sent
</code></pre></li>
<li><p><code>/connection/update</code></p>
</li>
<li><p><code>/connection/incr</code>
Update other properties of the existing connection, for connections that may take more than i step or if a connection has other data associated with it beside
the type of the connection.</p>
<p>Example:</p>
<pre><code>  /connection/update?id=12345&amp;type=invite&amp;state=accepted
</code></pre></li>
<li><p><code>/connection/del</code>
Delete existing connection(s), <code>id</code> and/or <code>type</code> may be be specified, if not all existing connections will be deleted.</p>
<p>Example:</p>
<pre><code>  /connection/del?type=invite&amp;id=12345
</code></pre></li>
<li><p><code>/connection/get</code>
Return a single connection for given id</p>
<p>Parameters:</p>
<ul>
<li>id - account id of the connection, required</li>
<li>type - connection type, required</li>
</ul>
<p>Example:</p>
<pre><code>  /connection/get?id=12345&amp;type=like
</code></pre><p>Response:</p>
<pre><code>  { &quot;id&quot;: &quot;12345&quot;,
    &quot;type: &quot;like&quot;,
    &quot;mtime&quot;: &quot;2434343543543&quot; }
</code></pre></li>
<li><p><code>/reference/get</code>
Return a single reference record for given account id, works the same way as <code>/connection/get</code></p>
</li>
</ul>
<ul>
<li><p><code>/connection/select</code>
Receive all my connections of the given type, i.e. connection(s) i made, if <code>id</code> is given only one record for the specified connection will be returned. Supports special
query parameters <code>_select,_ops,_desc</code>, see docs about <code>db.select</code> for more info. All <code>db.select</code> options can be passed in the query with prepended underscore.</p>
<p>By default only connection columns will be returned, specifying <code>_details=1</code> will return public account columns as well.</p>
<p>Example:</p>
<pre><code>  # Return all accounts who i invited
  /connection/select?type=invite
  # Return connection for specific type and account id
  /connection/select?type=invite&amp;id=12345
  # Return accounts who i invited me after specified mtime
  /connection/select?type=invite&amp;_ops=mtime,gt&amp;mtime=12334312543
  # Return accounts who i invited before specified mtime
  /connection/select?type=invite&amp;_ops=mtime,le&amp;_desc=1&amp;mtime=12334312543
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;12345&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }
</code></pre></li>
<li><p><code>/reference/select</code>
Receive all references that connected with my account, i.e. connections made by somebody else with me, works the same way as for connection query call</p>
<p>Example:</p>
<pre><code>  # Return all accounts who invited me
  /reference/select?type=invite
  # Return accounts who invited me after specified mtime
  /reference/select?type=invite&amp;_ops=mtime,gt&amp;mtime=12334312543
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }
</code></pre></li>
</ul>
<h2 id="locations">Locations</h2>
<p>The location API maintains a table <code>bk_location</code> with geolocation coordinates for accounts and allows searching it by distance. The configuration parameter
<code>min-distance</code> defines the radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. By default min-distance is 5 km
which means all geohashes in bk_location table will have geohash of size 4. Once min-distance is set it cannot be changed without rebuilding the bk_location table with new geohash size.</p>
<p>The location search is implemented by using geohash as a primary key in the bk_location table with the account id as the second part of the primary key, for DynamoDB this is the range key.
When request comes for all matches for the location for example 37.7, -122.4, the search that is executed looks like this:</p>
<ul>
<li>geohash for latitude 37.7 and longitude -122.4 and radius 10 km will be <code>9q8y</code></li>
<li>all neoghboring ares around this point within 10 km radius will be &#39;9q8z&#39;, &#39;9q8v&#39;, &#39;9q8w&#39;, &#39;9q8x&#39;, &#39;9q8t&#39;, &#39;9q9n&#39;, &#39;9q9p&#39;, &#39;9q9j&#39;</li>
<li>we start the search on the bk_location table by the primary key geohash with the value 9q8y</li>
<li>filter out all records beyond our radius by calculating the difference between our point and the candidate record</li>
<li>if total number of results expcted is still less than required, continue to the next neighbor area</li>
<li>continue untill we visit all neighbors or received required number of macthed records</li>
<li><p>on return the next_token opaque value will be provided if we want to continue the search for more matched for the same location</p>
</li>
<li><p><code>/location/put</code>
Store currenct location for current account, latitude and longitude parameters must be given, this call will update the bk_account table as well with
these coordinates</p>
<p>Example:</p>
<pre><code>  /location/put?latitude=-188.23232&amp;longitude=23.4545454
</code></pre></li>
<li><p><code>/location/get</code>
Return matched accounts within the distance(radius) specified by <code>distance=</code> parameter in kilometers and current position specified by latitude/longitude paraemeters. This
call returns results in chunks and requires navigation through all pages to receive all matched records. Records returned will start with the closest to the current
point. If there are more matched records than specified by the <code>_count</code>, the <code>next_token</code> property is set with the token to be used in the subsequent call,
it must be passed as is as <code>_token=</code> parameter with all original query parameters.</p>
<p>By default only locations with account ids will be returned, specifying <code>_details=1</code> will return public account columns as well.</p>
<p>Note: The current account will not be present in the results  even if it is within the range, to know my own location use <code>/account/get</code> call.</p>
<p>Example:</p>
<pre><code>      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25
      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25&amp;_token=FGTHTRHRTHRTHTTR.....
</code></pre><p>Response:</p>
<pre><code>     { &quot;data&quot;: [ { &quot;id&quot;: &quot;12345&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.123,
                   &quot;longitude&quot;: 23.45
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 },
                 { &quot;id&quot;: &quot;45678&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.133,
                   &quot;longitude&quot;: 23.5
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 }],
       &quot;next_token&quot;: &quot;&quot;
     }
</code></pre></li>
</ul>
<h2 id="messages">Messages</h2>
<p>The messaging API allows sending and recieving messages between accounts, it supports text and images. All new messages arrive into the bk_messsage table, the inbox. The client
may keep messages there as new, delete or archive them. Archiving means transfering messages into the bk_archive table. All sent messages are kept in the bk_sent table.</p>
<ul>
<li><p><code>/message/get</code>
Read all new messages, i.e. the messages that never been read or issued <code>/message/archive</code> call.</p>
<p>Parameters:</p>
<ul>
<li><code>_archive</code> - if set to 1, all returned messages will be archived automatically, so no individual /message/read call needed</li>
<li><code>_trash</code> - if set to 1, all returned messages will be deleted, not archived</li>
<li><code>_details</code> - if set to 1, return associated account details for the sender</li>
</ul>
<p>Example:</p>
<pre><code>  # Get all new messages
  /message/get

  # Get all new messages and archive them
  /message/get?_archive=1

  # Get all new messages from the specific sender
  /message/get?sender=12345
</code></pre></li>
<li><p><code>/message/get/archive</code>
Receive archived messages. The images are not returned, only link to the image in <code>icon</code> property of reach record,
the actual image data must be retrieved separately.</p>
<p>Parameters:</p>
<ul>
<li><code>mtime</code> - if specified then only messages received since that time will be retirned, it must be in milliseconds since midnight GMT on January 1, 1970, this is what
Date.now() return in Javascript.<ul>
<li><code>sender</code> - if specified then all messages from the given sender will be returned.</li>
</ul>
</li>
</ul>
<p>NOTE: The <code>mtime</code> is when the backend server received the message, if client and the server clocks are off this may return wrong data or not return anything at all,
also because the arrival order of the messages cannot be guaranteed, sending fast multiple messages may be received in different order by the backend and this will
result in mtimes that do not correspond to actual times when the message has been sent.</p>
<p>Example:</p>
<pre><code>  # Get all messages
  /message/get/archive

  # Get all messages received after given mtime
  /message/get/archive?mtime=123475658690

  # Get all messages received before given mtime
  /message/get/archive?mtime=123475658690&amp;_ops=mtime,lt

  # Get all messages with custom filter: if msg text contains Hi
  /message/get/archive?_ops=msg,iregexp&amp;msg=Hi

  # Get all messages from the specific sender
  /message/get/archive?sender=12345
</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;sender&quot;: &quot;12345&quot;,
                &quot;msg&quot;: &quot;Hi, how r u?&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              },
              { &quot;sender&quot;: &quot;45678&quot;,
                &quot;msg&quot;: &quot;check this out!&quot;,
                &quot;icon&quot;: &quot;/message/image?sender=45678&amp;mtime=12334312543&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              }],
       &quot;next_token&quot;: &quot;&quot;
     }
</code></pre></li>
<li><p><code>/message/get/sent</code>
 Return all messages i sent out. All the same query rules apply as for the archived messages API call.</p>
<p>Parameters:</p>
<ul>
<li><code>recipient</code> - id of the recipient where i have sent messages</li>
<li><code>mtime</code> - time before or after messages sent, defined by _ops parametrs</li>
</ul>
<p>Example:</p>
<pre><code>  /message/get/sent?id=123
  /message/get/sent?id=123&amp;mtime=123475658690&amp;_ops=mtime,le
</code></pre></li>
<li><p><code>/message/add</code>
Send a message to an account, the following parameters must be specified:</p>
<ul>
<li><code>id</code> - account id of the receiver</li>
<li><code>msg</code> - text of the message, can be empty if <code>icon</code> property exists</li>
<li><code>icon</code> - icon of the message, it can be base64 encoded image in the query or JSON string if the whole message is posted as JSON or
can be a multipart file upload if submitted via browser, can be omitted if <code>msg/connection/get?type=invite&amp;id=12345</code> property exists.</li>
<li>_nosent - do not save this message in my sent messages</li>
<li>_publish - notify another account about this via pub/sub messaging system if it is active</li>
</ul>
<p>Example:</p>
<pre><code>  /message/add?id=12345&amp;msg=Hello
  /message/add?id=12345&amp;msg=this%2Bis%2Bthe%2Bpic&amp;icon=KHFHTDDKH7676758JFGHFDRDEDET....TGJNK%2D
</code></pre></li>
<li><p><code>/message/archive</code>
Move a new message to the archive. The required query parameters are <code>sender</code> and <code>mtime</code>.</p>
<p>Example:</p>
<pre><code>  /message/read?sender=12345&amp;mtime=12366676434
</code></pre></li>
<li><p><code>/message/del</code>
Delete new message(s) by <code>sender</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del?sender=12345&amp;mtime=124345656567676
</code></pre></li>
<li><p><code>/message/del/archive</code>
Delete archived message(s) by <code>sender</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del/archive?sender=12345&amp;mtime=124345656567676
</code></pre></li>
<li><p><code>/message/del/sent</code>
Delete the message(s) by <code>recipient</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages to the given recipient will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del/sent?recipient=12345&amp;mtime=124345656567676
</code></pre></li>
<li><p><code>/message/image</code>
Return the image data for the given message, the required parameters are:</p>
<ul>
<li>sender - id of the sender returned in the by <code>/message/get</code> reply results for every message</li>
<li>mtime - exact timestamp of the message</li>
</ul>
</li>
</ul>
<h2 id="counters">Counters</h2>
<p>The counters API maintains realtime counters for every account records, the counters record may contain many different counter columns for different purposes and
is always cached with whatever cache service is used, by default it is cached by the Web server process on every machine. Web worker processes ask the master Web server
process for the cached records thus only one copy of the cache per machine even in the case of multiple CPU cores.</p>
<ul>
<li><p><code>/counter/get</code>
Return counter record for current account with all available columns of if <code>id</code> is given return public columns for given account, it works with <code>bk_counter</code> table
which by default defines some common columns:</p>
<ul>
<li>ping - a counter for general use, can be used to send a notification event to any acount by increasing this counter for an account</li>
<li>like0 - how many i liked, how many time i liked someone, i.e. made a new record in bk_connection table with type &#39;like&#39;</li>
<li>like1 - how many liked me, reverse counter, who connected to me with type &#39;like&#39;
More columns can be added to the bk_counter table.</li>
</ul>
<p>NOTE: The columns with suffixes 0 and 1 are special columns that support the Connections API, every time a new connection is created, the type of new connection
is checked against any columns in the bk_counter table, if a property type0 exists and marked in the table descriptnio as <code>autoincr</code> then the corresponding
counter property is increased, this is how every time new connectio like/dislike/invite/follow is added, the counters in the bk_counter table are increased.</p>
</li>
<li><p><code>/counter/put</code>
Replace my counters record, all values if not specified will be set to 0</p>
</li>
<li><p><code>/counter/incr</code>
Increase one or more counter fields, each column can provide a numeric value and it will be added to the existing value, negative values will be substracted.
if <code>id</code> parameter is specified, only public columns will be increased for other account.</p>
<p>Example:</p>
<pre><code>  /counter/incr?msg_read=5&amp;
  /counter/incr?id=12345&amp;ping=1
</code></pre></li>
</ul>
<h2 id="status">Status</h2>
<p>The status API maintains account status with the timestamp to be used for presence or any other purposes. This table can be cached with any available
caching system like Redis, memcache, nanomsg to be very fast presence state system.</p>
<ul>
<li><p><code>/status/put</code>
Set the status of the current account, requires status parameter, automatically updates the timestamp</p>
<p>Example:</p>
<pre><code>  /status/put?status=online
</code></pre></li>
<li><p><code>/status/get</code>
Return status for the account by id, if no id is psecified return statrus for the current account</p>
<p>Example:</p>
<pre><code>  /status/get?id=12345
</code></pre></li>
<li><p><code>/status/del</code>
Delete current account status, mostly for clearing the cache or marking offline status</p>
</li>
</ul>
<h2 id="data">Data</h2>
<p>The data API is a generic way to access any table in the database with common operations, as oppose to the any specific APIs above this API only deals with
one table and one record without maintaining any other features like auto counters, cache...</p>
<p><em>Because it exposes the whole database to anybody who has a login it is a good idea to disable this endpoint in the production or provide access callback that verifies
who can access it.</em></p>
<ul>
<li>To disable this endpoint completely in the config: api-disable=data</li>
<li><p>To allow admins to access it only:</p>
<pre><code>api.registerPreProcess(&#39;GET&#39;, &#39;/data&#39;, function(req, status, cb) { if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &#39;access denied&#39; }; cb(status)); });
</code></pre></li>
</ul>
<ul>
<li><code>/data/columns</code></li>
<li><p><code>/data/columns/TABLE</code>
Return columns for all tables or the specific TABLE</p>
</li>
<li><p><code>/data/keys/TABLE</code>
Return primary keys for the given TABLE</p>
</li>
<li><p><code>/data/(select|search|list|get|add|put|update|del|incr|replace)/TABLE</code>
Perform database operation on the given TABLE, all options for the <code>db</code> functiobns are passed as query parametrrs prepended with underscore,
regular parameters are the table columns.</p>
<p>By default the API does not allow table scans without a condition to avoid expensive and long queries, to enable a scan pass <code>_noscan=0</code>.
For this to work the Data API must be configured as unsecure in the config file using the parameter <code>api-unsecure=data</code>.</p>
<p>Some tables like messages and connections perform data convertion before returning the results, mostly splitting combined columns like type into
separate fields. To return raw data pass the parameter <code>_noprocessrows=1</code>.</p>
<p>Example:</p>
<pre><code>  /data/get/bk_account?id=12345
  /data/put/bk_counter?id=12345&amp;like0=1
  /data/select/bk_account?name=john&amp;_ops=name,gt&amp;_select=name,alias,email
  /data/select/bk_connection?_noscan=0&amp;_noprocessrows=1
</code></pre></li>
</ul>
<h2 id="pages">Pages</h2>
<p>The pages API provides a simple Wiki like system with Markdown formatting. It keeps all pages in the database table <code>bk_pages</code> and
exposes an API to manage and render pages.</p>
<p>The pages support public mode, all pages with <code>pub</code> set to true will be returning without an account, this must be enabled with <code>api-allow-path=^/pages/(get|select|show)</code>
to work.</p>
<p>All .md files will be rendered into html automatically if there is not _raw=1 query parameter and pages view exists (api-pages-view=pages.html by default).</p>
<ul>
<li><p><code>/pages/get/ID</code>
Return a page with given id or the main page if id is empty. If the query parameter <code>_render=1</code> is given, the content will be rendered into html from markdown, otherwie
returns all data as is.</p>
</li>
<li><p><code>/pages/select</code>
Return all pages or only ones which match the query criteria. This potentially scans the whole table to return all pages and
is used to show pages index.</p>
</li>
<li><p><code>/pages/put</code>
Replace or add a new page.</p>
</li>
<li><p><code>/pages/del</code>
Delete a page from the database</p>
</li>
<li><p><code>/pages/show/ID</code>
Render a page with given id, markdown is converted into html using <code>marked</code>. A view must be condfigured in order to render to work, by default pages.html view
is provided to simply wrap the markdown in the page layout.</p>
</li>
</ul>
<h2 id="system-api">System API</h2>
<p>The system API returns information about the backend statistics, allows provisioning and configuration commands and other internal maintenance functions. By
default is is open for access to all users but same security considerations apply here as for the Data API.</p>
<ul>
<li><p><code>/system/restart</code>
  Perform restart of the Web processes, this will be done gracefully, only one Web worker process will be restarting while the other processes will keep
  serving requests. The intention is to allow code updates on live systems without service interruption.</p>
</li>
<li><p><code>/system/cache/(init|stats|keys|get|set|put|incr|del|clear)</code>
  Access to the caching functions</p>
</li>
<li><p><code>/system/msg/(msg)</code>
  Access to the messaging functions</p>
</li>
<li><p><code>/system/stats/get</code>
Database pool statistics and other diagnostics</p>
<ul>
<li>latency - how long a pending request waits in queue at this moment</li>
<li>busy - how many busy error responses have been returned so far</li>
<li>pool - database metrics<ul>
<li>response - stats about how long it takes between issuing the db request and till the final moment all records are ready to be sent to the client</li>
<li>queue - stats about db requests at any given moment queued for the execution</li>
<li>cache - db cache response time and metrics</li>
</ul>
</li>
<li>api - Web requests metrics, same structure as for the db pool metrics</li>
<li>url - metrics per url endpoints</li>
</ul>
<p>Individual sub-objects:</p>
<ul>
<li>meter - Things that are measured as events / interval.<ul>
<li>rmean: The average rate since the meter was started.</li>
<li>rcnt: The total of all values added to the meter.</li>
<li>rate: The rate of the meter since the last toJSON() call.</li>
<li>r1m: The rate of the meter biased towards the last 1 minute.</li>
<li>r5m: The rate of the meter biased towards the last 5 minutes.</li>
<li>r15m: The rate of the meter biased towards the last 15 minutes.</li>
</ul>
</li>
<li>queue or histogram - Keeps a resevoir of statistically relevant values biased towards the last 5 minutes to explore their distribution<ul>
<li>hmin: The lowest observed value.</li>
<li>mmax: The highest observed value.</li>
<li>hsum: The sum of all observed values.</li>
<li>hvar: The variance of all observed values.</li>
<li>hmean: The average of all observed values.</li>
<li>hdev: The standard deviation of all observed values.</li>
<li>hcnt: The number of observed values.</li>
<li>hmed: median, 50% of all values in the resevoir are at or below this value.</li>
<li>hp75: See median, 75% percentile.</li>
<li>hp95: See median, 95% percentile.</li>
<li>hp99: See median, 99% percentile.</li>
<li>hp999: See median, 99.9% percentile.</li>
</ul>
</li>
</ul>
<p>Response:</p>
<pre><code>       {
            &quot;id&quot;: &quot;172.31.31.85-25170&quot;,
            &quot;ip&quot;: &quot;172.31.31.85&quot;,
            &quot;mtime&quot;: 1417500027321,
            &quot;ctime&quot;: 1416941754760,
            &quot;type&quot;: &quot;&quot;,
            &quot;host&quot;: &quot;&quot;,
            &quot;pid&quot;: 25170,
            &quot;instance&quot;: &quot;i-d4c89eff&quot;,
            &quot;worker&quot;: 27,
            &quot;latency&quot;: 0,
            &quot;cpus&quot;: 4,
            &quot;mem&quot;: 15774367744,
            &quot;rss_hmin&quot;: 66879488,
            &quot;rss_hmax&quot;: 151891968,
            &quot;rss_hsum&quot;: 2451506479104,
            &quot;rss_hvar&quot;: 254812067010902.66,
            &quot;rss_hmean&quot;: 118895507.98312236,
            &quot;rss_hdev&quot;: 15962833.92793719,
            &quot;rss_hcnt&quot;: 20619,
            &quot;rss_hmed&quot;: 147644416,
            &quot;rss_h75p&quot;: 149262336,
            &quot;rss_h95p&quot;: 150834585.6,
            &quot;rss_h99p&quot;: 151550033.92000002,
            &quot;rss_h999p&quot;: 151886266.368,
            &quot;heap_hmin&quot;: 25790920,
            &quot;heap_hmax&quot;: 72316184,
            &quot;heap_hsum&quot;: 1029889929504,
            &quot;heap_hvar&quot;: 54374337037311.65,
            &quot;heap_hmean&quot;: 49948587.68630874,
            &quot;heap_hdev&quot;: 7373895.648658967,
            &quot;heap_hcnt&quot;: 20619,
            &quot;heap_hmed&quot;: 57480704,
            &quot;heap_h75p&quot;: 61934254,
            &quot;heap_h95p&quot;: 67752391.2,
            &quot;heap_h99p&quot;: 70544797.92,
            &quot;heap_h999p&quot;: 72315029.104,
            &quot;avg_hmin&quot;: 0.04541015625,
            &quot;avg_hmax&quot;: 0.06005859375,
            &quot;avg_hsum&quot;: 938.234375,
            &quot;avg_hvar&quot;: 4.491222722966496e-7,
            &quot;avg_hmean&quot;: 0.04550338886463941,
            &quot;avg_hdev&quot;: 0.0006701658543201448,
            &quot;avg_hcnt&quot;: 20619,
            &quot;avg_hmed&quot;: 0.04541015625,
            &quot;avg_h75p&quot;: 0.04541015625,
            &quot;avg_h95p&quot;: 0.04541015625,
            &quot;avg_h99p&quot;: 0.05078125,
            &quot;avg_h999p&quot;: 0.05997363281250001,
            &quot;free_hmin&quot;: 12879872000,
            &quot;free_hmax&quot;: 13228994560,
            &quot;free_hsum&quot;: 268429937405952,
            &quot;free_hvar&quot;: 5839592954606286,
            &quot;free_hmean&quot;: 13018572064.889277,
            &quot;free_hdev&quot;: 76417229.43555522,
            &quot;free_hcnt&quot;: 20619,
            &quot;free_hmed&quot;: 12908707840,
            &quot;free_h75p&quot;: 12915716096,
            &quot;free_h95p&quot;: 12919331430.4,
            &quot;free_h99p&quot;: 12922073088,
            &quot;free_h999p&quot;: 12922164563.968,
            &quot;util_hmin&quot;: 0.05905642141342145,
            &quot;util_hmax&quot;: 0.0607655708794173,
            &quot;util_hsum&quot;: 1230.6298386264643,
            &quot;util_hvar&quot;: 2.1530671850148948e-7,
            &quot;util_hmean&quot;: 0.059684263961708346,
            &quot;util_hdev&quot;: 0.0004640115499656118,
            &quot;util_hcnt&quot;: 20619,
            &quot;util_hmed&quot;: 0.05920415878947068,
            &quot;util_h75p&quot;: 0.059217278415661254,
            &quot;util_h95p&quot;: 0.05934395790869296,
            &quot;util_h99p&quot;: 0.059361851867105964,
            &quot;util_h999p&quot;: 0.0593659827984017,
            &quot;pool_name&quot;: &quot;dynamodb&quot;,
            &quot;pool_que_rate&quot;: 0,
            &quot;pool_que_rcnt&quot;: 1989,
            &quot;pool_que_rmean&quot;: 0.0035627883554577716,
            &quot;pool_que_r1m&quot;: 0,
            &quot;pool_que_r5m&quot;: 0,
            &quot;pool_que_r15m&quot;: 0,
            &quot;pool_que_hmin&quot;: 0,
            &quot;pool_que_hmax&quot;: 230,
            &quot;pool_que_hsum&quot;: 45843,
            &quot;pool_que_hvar&quot;: 366.86587852909315,
            &quot;pool_que_hmean&quot;: 23.048265460030166,
            &quot;pool_que_hdev&quot;: 19.15374319889178,
            &quot;pool_que_hcnt&quot;: 1989,
            &quot;pool_que_hmed&quot;: 21,
            &quot;pool_que_h75p&quot;: 23,
            &quot;pool_que_h95p&quot;: 33,
            &quot;pool_que_h99p&quot;: 126.42000000000007,
            &quot;pool_que_h999p&quot;: 225.971,
            &quot;pool_req_hmin&quot;: 1,
            &quot;pool_req_hmax&quot;: 2,
            &quot;pool_req_hsum&quot;: 1991,
            &quot;pool_req_hvar&quot;: 0.001005024617286425,
            &quot;pool_req_hmean&quot;: 1.0010055304172951,
            &quot;pool_req_hdev&quot;: 0.03170212322994195,
            &quot;pool_req_hcnt&quot;: 1989,
            &quot;pool_req_hmed&quot;: 1,
            &quot;pool_req_h75p&quot;: 1,
            &quot;pool_req_h95p&quot;: 1,
            &quot;pool_req_h99p&quot;: 1,
            &quot;pool_req_h999p&quot;: 1.9710000000000036,
            &quot;pool_count&quot;: 0,
            &quot;pool_req_0&quot;: 2,
            &quot;pool_cache_rate&quot;: 0.1303780964797914,
            &quot;pool_cache_rcnt&quot;: 284,
            &quot;pool_cache_rmean&quot;: 0.0005087436344326025,
            &quot;pool_cache_r1m&quot;: 0,
            &quot;pool_cache_r5m&quot;: 0,
            &quot;pool_cache_r15m&quot;: 0,
            &quot;pool_cache_hmin&quot;: 0,
            &quot;pool_cache_hmax&quot;: 2,
            &quot;pool_cache_hsum&quot;: 70,
            &quot;pool_cache_hvar&quot;: 0.19345045538247163,
            &quot;pool_cache_hmean&quot;: 0.24647887323943662,
            &quot;pool_cache_hdev&quot;: 0.4398300301053483,
            &quot;pool_cache_hcnt&quot;: 284,
            &quot;pool_cache_hmed&quot;: 0,
            &quot;pool_cache_h75p&quot;: 0,
            &quot;pool_cache_h95p&quot;: 1,
            &quot;pool_cache_h99p&quot;: 1,
            &quot;pool_cache_h999p&quot;: 2,
            &quot;pool_hits&quot;: 239,
            &quot;pool_misses&quot;: 45,
            &quot;cache_inserted&quot;: 484,
            &quot;cache_deleted&quot;: 310,
            &quot;cache_cleanups&quot;: 0,
            &quot;cache_hits&quot;: 7642,
            &quot;cache_misses&quot;: 1411,
            &quot;cache_max&quot;: 1000000,
            &quot;cache_size&quot;: 61586,
            &quot;cache_count&quot;: 174,
            &quot;api_que_hmin&quot;: 1,
            &quot;api_que_hmax&quot;: 6,
            &quot;api_que_hsum&quot;: 13237,
            &quot;api_que_hvar&quot;: 0.005674280465987009,
            &quot;api_que_hmean&quot;: 1.0024992426537414,
            &quot;api_que_hdev&quot;: 0.07532782000022972,
            &quot;api_que_hcnt&quot;: 13204,
            &quot;api_que_hmed&quot;: 1,
            &quot;api_que_h75p&quot;: 1,
            &quot;api_que_h95p&quot;: 1,
            &quot;api_que_h99p&quot;: 1,
            &quot;api_que_h999p&quot;: 2,
            &quot;api_nreq&quot;: 1,
            &quot;api_req_rate&quot;: 0,
            &quot;api_req_rcnt&quot;: 13203,
            &quot;api_req_rmean&quot;: 0.02365120609256502,
            &quot;api_req_r1m&quot;: 0,
            &quot;api_req_r5m&quot;: 0,
            &quot;api_req_r15m&quot;: 0,
            &quot;api_req_hmin&quot;: 0,
            &quot;api_req_hmax&quot;: 536,
            &quot;api_req_hsum&quot;: 20115,
            &quot;api_req_hvar&quot;: 89.12554520926801,
            &quot;api_req_hmean&quot;: 1.5235173824130879,
            &quot;api_req_hdev&quot;: 9.440632669968046,
            &quot;api_req_hcnt&quot;: 13203,
            &quot;api_req_hmed&quot;: 1,
            &quot;api_req_h75p&quot;: 1,
            &quot;api_req_h95p&quot;: 1,
            &quot;api_req_h99p&quot;: 33.13000000000011,
            &quot;api_req_h999p&quot;: 99.36200000000008,
            &quot;url_message_get_rate&quot;: 0,
            &quot;url_message_get_rcnt&quot;: 24,
            &quot;url_message_get_rmean&quot;: 0.00004299242196761214,
            &quot;url_message_get_r1m&quot;: 0,
            &quot;url_message_get_r5m&quot;: 0,
            &quot;url_message_get_r15m&quot;: 0,
            &quot;url_message_get_hmin&quot;: 16,
            &quot;url_message_get_hmax&quot;: 71,
            &quot;url_message_get_hsum&quot;: 792,
            &quot;url_message_get_hvar&quot;: 208.34782608695653,
            &quot;url_message_get_hmean&quot;: 33,
            &quot;url_message_get_hdev&quot;: 14.434258764722092,
            &quot;url_message_get_hcnt&quot;: 24,
            &quot;url_message_get_hmed&quot;: 30.5,
            &quot;url_message_get_h75p&quot;: 40.75,
            &quot;url_message_get_h95p&quot;: 68,
            &quot;url_message_get_h99p&quot;: 71,
            &quot;url_message_get_h999p&quot;: 71,
            &quot;url_message_get_0&quot;: 0,
            &quot;api_req_0&quot;: 20,
            &quot;url_ping_rate&quot;: 0,
            &quot;url_ping_rcnt&quot;: 12407,
            &quot;url_ping_rmean&quot;: 0.022226981327796796,
            &quot;url_ping_r1m&quot;: 0,
            &quot;url_ping_r5m&quot;: 0,
            &quot;url_ping_r15m&quot;: 0,
            &quot;url_ping_hmin&quot;: 0,
            &quot;url_ping_hmax&quot;: 4,
            &quot;url_ping_hsum&quot;: 6915,
            &quot;url_ping_hvar&quot;: 0.25785489698686204,
            &quot;url_ping_hmean&quot;: 0.5573466591440316,
            &quot;url_ping_hdev&quot;: 0.5077941482400737,
            &quot;url_ping_hcnt&quot;: 12407,
            &quot;url_ping_hmed&quot;: 1,
            &quot;url_ping_h75p&quot;: 1,
            &quot;url_ping_h95p&quot;: 1,
            &quot;url_ping_h99p&quot;: 1,
            &quot;url_ping_h999p&quot;: 2,
            &quot;url_ping_0&quot;: 5,
            &quot;url_image_account_rate&quot;: 0,
            &quot;url_image_account_rcnt&quot;: 95,
            &quot;url_image_account_rmean&quot;: 0.00017084907295404685,
            &quot;url_image_account_r1m&quot;: 0,
            &quot;url_image_account_r5m&quot;: 0,
            &quot;url_image_account_r15m&quot;: 0,
            &quot;url_image_account_hmin&quot;: 17,
            &quot;url_image_account_hmax&quot;: 121,
            &quot;url_image_account_hsum&quot;: 4295,
            &quot;url_image_account_hvar&quot;: 372.42329227323637,
            &quot;url_image_account_hmean&quot;: 45.21052631578947,
            &quot;url_image_account_hdev&quot;: 19.29827174317007,
            &quot;url_image_account_hcnt&quot;: 95,
            &quot;url_image_account_hmed&quot;: 42,
            &quot;url_image_account_h75p&quot;: 51,
            &quot;url_image_account_h95p&quot;: 89.59999999999991,
            &quot;url_image_account_h99p&quot;: 121,
            &quot;url_image_account_h999p&quot;: 121,
            &quot;url_image_account_0&quot;: 0,
            &quot;incr_follow_0&quot;: 0,
            &quot;api_bad_0&quot;: 3,
            &quot;url_account_update_rate&quot;: 0,
            &quot;url_account_update_rcnt&quot;: 6,
            &quot;url_account_update_rmean&quot;: 0.000010813705805470248,
            &quot;url_account_update_r1m&quot;: 0,
            &quot;url_account_update_r5m&quot;: 0,
            &quot;url_account_update_r15m&quot;: 0,
            &quot;url_account_update_hmin&quot;: 53,
            &quot;url_account_update_hmax&quot;: 182,
            &quot;url_account_update_hsum&quot;: 573,
            &quot;url_account_update_hvar&quot;: 2041.5,
            &quot;url_account_update_hmean&quot;: 95.5,
            &quot;url_account_update_hdev&quot;: 45.18296139032943,
            &quot;url_account_update_hcnt&quot;: 6,
            &quot;url_account_update_hmed&quot;: 82,
            &quot;url_account_update_h75p&quot;: 120.5,
            &quot;url_account_update_h95p&quot;: 182,
            &quot;url_account_update_h99p&quot;: 182,
            &quot;url_account_update_h999p&quot;: 182,
            &quot;url_account_update_0&quot;: 0,
            &quot;auth_add_0&quot;: 0,
            &quot;url_account_get_rate&quot;: 0,
            &quot;url_account_get_rcnt&quot;: 9,
            &quot;url_account_get_rmean&quot;: 0.0001993511695335063,
            &quot;url_account_get_r1m&quot;: 0,
            &quot;url_account_get_r5m&quot;: 0,
            &quot;url_account_get_r15m&quot;: 0,
            &quot;url_account_get_hmin&quot;: 2,
            &quot;url_account_get_hmax&quot;: 100,
            &quot;url_account_get_hsum&quot;: 435,
            &quot;url_account_get_hvar&quot;: 844.0000000000001,
            &quot;url_account_get_hmean&quot;: 48.333333333333336,
            &quot;url_account_get_hdev&quot;: 29.051678092667903,
            &quot;url_account_get_hcnt&quot;: 9,
            &quot;url_account_get_hmed&quot;: 46,
            &quot;url_account_get_h75p&quot;: 67,
            &quot;url_account_get_h95p&quot;: 100,
            &quot;url_account_get_h99p&quot;: 100,
            &quot;url_account_get_h999p&quot;: 100,
            &quot;url_account_get_0&quot;: 1,
            &quot;url_system_stats_rate&quot;: 0,
            &quot;url_system_stats_rcnt&quot;: 1,
            &quot;url_system_stats_rmean&quot;: 0.04501665616278023,
            &quot;url_system_stats_r1m&quot;: 0,
            &quot;url_system_stats_r5m&quot;: 0,
            &quot;url_system_stats_r15m&quot;: 0,
            &quot;url_system_stats_hmin&quot;: 3,
            &quot;url_system_stats_hmax&quot;: 3,
            &quot;url_system_stats_hsum&quot;: 3,
            &quot;url_system_stats_hmean&quot;: 3,
            &quot;url_system_stats_hdev&quot;: 0,
            &quot;url_system_stats_hcnt&quot;: 1,
            &quot;url_system_stats_hmed&quot;: 3,
            &quot;url_system_stats_h75p&quot;: 3,
            &quot;url_system_stats_h95p&quot;: 3,
            &quot;url_system_stats_h99p&quot;: 3,
            &quot;url_system_stats_h999p&quot;: 3,
            &quot;url_system_stats_0&quot;: 2
        }
</code></pre></li>
</ul>
<h1 id="backend-directory-structure">Backend directory structure</h1>
<p>When the backend server starts and no -home argument passed in the command line the backend makes its home environment in the ~/.backend directory.</p>
<p>The backend directory structure is the following:</p>
<ul>
<li><p><code>etc</code> - configuration directory, all config files are there</p>
<ul>
<li><code>etc/profile</code> - shell script loaded by the bkjs utility to customize env variables</li>
<li><p><code>etc/config</code> - config parameters, same as specified in the command line but without leading -, each config parameter per line:</p>
<p>  Example:</p>
<pre><code>  debug=1
  db-pool=dynamodb
  db-dynamodb-pool=http://localhost:9000
  db-pgsql-pool=postgresql://postgres@127.0.0.1/backend

  To specify other config file: bkjs run-backend -config-file file
</code></pre></li>
<li><p>etc/config.local - same as the config but for the cases when local environment is different than the production or for dev specific parameters</p>
</li>
<li><p>some config parameters can be condigured in DNS as TXT records, the backend on startup will try to resolve such records and use the value if not empty.
All params that  marked with DNS TXT can be configured in the DNS server for the domain where the backend is running, the config parameter name is
concatenated with the domain and queried for the TXT record, for example: <code>cache-host</code> parameter will be queried for cache-host.domain.name for TXT record type.</p>
</li>
<li><p><code>etc/crontab</code> - jobs to be run with intervals, local or remote, JSON file with a list of cron jobs objects:</p>
<p>  Example:</p>
<ol>
<li><p>Create file in ~/.backend/etc/crontab with the following contents:</p>
<pre><code> [ { &quot;type&quot;: &quot;local&quot;, &quot;cron&quot;: &quot;0 1 1 * * 1,3&quot;, &quot;job&quot;: { &quot;app.cleanSessions&quot;: { &quot;interval&quot;: 3600000 } } } ]
</code></pre></li>
<li><p>Define the function that the cron will call with the options specified, callback must be called at the end, create this app.js file</p>
<pre><code> var bkjs = require(&quot;backendjs&quot;);
 bkjs.app.cleanSessions = function(options, callback) {
      bkjs.db.delAll(&quot;session&quot;, { mtime: options.interval + Date.now() }, { ops: &quot;le&quot; }, callback);
 }
 bkjs.server.start()
</code></pre></li>
<li><p>Start the scheduler and the web server at once</p>
<pre><code> bkjs run-backend -master -web
</code></pre></li>
</ol>
</li>
<li><p>etc/crontab.local - additional local crontab that is read after the main one, for local or dev environment</p>
</li>
</ul>
</li>
<li><p><code>images</code> - all images to be served by the API server, every subfolder represent naming space with lots of subfolders for images</p>
</li>
<li><code>var</code> - database files created by the server</li>
<li><code>tmp</code> - temporary files</li>
<li><code>web</code> - Web pages served by the static Express middleware</li>
</ul>
<h1 id="internal-backend-functions">Internal backend functions</h1>
<p>The backend includes internal C++ module which provide some useful functions available in the Javascript. The module is exposed as <code>utils</code> submodule, to see
all functions for example run the below:</p>
<pre><code>    var bkjs = require(&#39;backendjs&#39;);
    console.log(bkjs.utils)
</code></pre><p>List of available functions:</p>
<ul>
<li><code>rungc()</code> - run V8 garbage collector on demand</li>
<li><code>setsegv()</code> - install SEGV signal handler to show crash backtrace</li>
<li><code>setbacktrace()</code> - install special V8-aware backtrace handler</li>
<li><code>backtrace()</code> - show V8 backtrace from current position</li>
<li><code>heapSnapshot(file)</code> - dump current memory heap snapshot into a file</li>
<li><code>splitArray(str)</code> - split a string into an array separated by commas, supports double quotes</li>
<li><code>logging([level])</code> - set or return logging level, this is internal C++ logging facility</li>
<li><code>loggingChannel(channelname)</code> - redirect logging into stdout or stderr, this is internal C++ logging</li>
<li><code>countWords(word, text)</code> - return how many time word appers in the text, uses Knuth-Morris-Pratt algorithm</li>
<li><code>countAllWords(list, text)</code> - return an object with counters for each word from the list, i.e. how many times each word appears in the text, uses Aho-Corasick algorithm</li>
<li><code>countWordsInit()</code> - clears word counting cache</li>
<li><code>resizeImage(source, options, callback)</code> - resize image using ImageMagick,<ul>
<li>source can be a Buffer or file name</li>
<li>options can have the following properties:<ul>
<li>width - output image width, if negative and the original image width is smaller than the specified, nothing happens</li>
<li>height - output image height, if negative and the original image height is smaller this the specified, nothing happens</li>
<li>quality - 0 -99</li>
<li>out - output file name</li>
<li>ext - image extention</li>
</ul>
</li>
</ul>
</li>
<li><code>resizeImageSync(name,width,height,format,filter,quality,outfile)</code> - resize an image synchronically</li>
<li><code>snappyCompress(str)</code> - compress a string</li>
<li><code>snappyUncompress(str)</code> - decompress a string</li>
<li><code>zlibCompress(str)</code> - compress a string</li>
<li><code>zlibUncompress(str)</code> - decompress a string</li>
<li><code>unzip(zipfile, outdir)</code> - extract a zip archive into directory</li>
<li><code>unzipFile(zipfile, file [, outfile])</code> - extract a file from zip archive, return contents if no outfile s specified</li>
<li><code>run(command, callback)</code> - run shell command and return all output to the callback</li>
<li><code>getUser([user])</code> - return an object with user info from the /etc/passwd file, user can be uid or name</li>
<li><code>getGroup([group])</code> - return an object with specified group info for the current user of for the given group id or name</li>
<li>Geohash support<ul>
<li><code>geoDistance(lat1, lon1, lat2, lon2)</code> - return distance between 2 coordinates in km</li>
<li><code>geoBoundingBox(lat, lon, distance)</code> - return bounding box geohash for given point around distance</li>
<li><code>geoHashEncode(lat, lon, len)</code> - return geohash for given coordinate, len defines number of bytesin geohash</li>
<li><code>geoHashDecode(hash)</code> - return coordinates for given geohash</li>
<li><code>geoHashAdjacent()</code></li>
<li><code>geoHashGrid()</code></li>
<li><code>geoHashRow()</code></li>
</ul>
</li>
<li>Generic cache outside of V8 memory pool<ul>
<li><code>cacheSave()</code> - general purpose caching functions that have no memory limits and do not use V8 heap</li>
<li><code>cachePut()</code></li>
<li><code>cacheGet()</code></li>
<li><code>cacheDel()</code></li>
<li><code>cacheKeys()</code></li>
<li><code>cacheClear()</code></li>
<li><code>cacheNames()</code></li>
<li><code>cacheSize()</code></li>
<li><code>cacheEach()</code></li>
<li><code>cacheForEach()</code></li>
<li><code>cacheForEachNext()</code></li>
<li><code>cacheBegin()</code></li>
<li><code>cacheNext()</code></li>
</ul>
</li>
<li>LRU internal cache<ul>
<li><code>lruInit(max)</code> - init LRU cache with max number of keys, this is in-memory cache which evicts older keys</li>
<li><code>lruStats()</code> - return statistics about the LRU cache</li>
<li><code>lruSize()</code> - return size of the current LRU cache</li>
<li><code>lruCount()</code> - number of keys in the LRU cache</li>
<li><code>lruPut(name, val)</code> - set/replace value by name</li>
<li><code>lruGet(name)</code> - return value by name</li>
<li><code>lruIncr(name, val)</code> - increase value by given number, non existent items assumed to be 0</li>
<li><code>lruDel(name)</code> - delete by name</li>
<li><code>lruKeys()</code> - return all cache key names</li>
<li><code>lruClear()</code> - clear LRU cache</li>
<li><code>lruServer()</code></li>
</ul>
</li>
<li>Syslog support<ul>
<li><code>syslogInit(name, priority, facility)</code> - initialize syslog client, used by the logger module</li>
<li><code>syslogSend(level, text)</code></li>
<li><code>syslogClose()</code></li>
</ul>
</li>
<li>NNSocket() - nanomsg socket object with the methods:<ul>
<li><code>subscribe</code></li>
<li><code>bind</code></li>
<li><code>close</code></li>
<li><code>setOption</code></li>
<li><code>connect</code></li>
<li><code>unsubscribe</code></li>
<li><code>send</code></li>
<li><code>recv</code></li>
<li><code>setCallback</code></li>
<li><code>setProxy</code></li>
<li><code>setForward</code></li>
</ul>
</li>
</ul>
<h1 id="cache-configurations">Cache configurations</h1>
<p>Database layer support caching of the responses using <code>db.getCached</code> call, it retrieves exactly one record from the configured cache, if no record exists it
will pull it from the database and on success will store it in the cache before returning to the client. When dealing with cached records, there is a special option
that must be passed to all put/update/del database methods in order to clear local cache, so next time the record will be retrieved with new changes from the database
and refresh the cache, that is <code>{ cached: true }</code> can be passed in the options parameter for the db methods that may modify records with cached contents. In any case
it is required to clear cache manually there is <code>db.clearCache</code> method for that.
Also there is a configuration option <code>-db-caching</code> to make any table automatically cached for all requests.</p>
<h2 id="nanomsg">nanomsg</h2>
<p>For cache management signaling, all servers maintain local cache per machine, it is called <code>LRU</code> cache. This cache is maintained in the master Web process and
serves all local Web worker processes via IPC channel. Every Web master process if compiled with nanomsg library can accept cache messages on a TCP port (<code>cache-port=20194/20195</code>)
from other backend nodes. Every time any Web worker updates the local cache, its master process re-broadcasts the same request to other connected Web master
processes on other nodes thus keeping in sync caches on all nodes.</p>
<p>In case of a single machine even with multiple CPUs there is nothing to configure, it is enabled by default. In case of multiple servers in the cluster
it requires one or multiple cache coordinators to be configured. It can be any node(s) in the cluster. The coordinator&#39;s role is to broadcast
cache requests to all nodes in the cluster.</p>
<p>For very frequent items there is no point using local cache but for items reasonable static with not so often changes this cache model will work reliably and similar to
what <code>memcached</code> or <code>Redis</code> servers would do as well.</p>
<p>The benefits of this approach is not to run any separate servers and dealing with its own configuration and support, using nanomsg
internal backend cache system is self contained and does not need additional external resources, any node can be LRU server whose only role is to make sure all other
nodes flush their caches if needed. Using redundant coordinators servers makes sure cache requests reach all nodes in the cluster and there is no single point of failure.</p>
<p>Essentually, setting <code>cache-host</code> to the list of any node(s) in the network is what needs to be done to support distributed cache with nanomsg sockets.</p>
<h2 id="memcached">memcached</h2>
<p>Setting <code>cache-type=memcache</code> and pointing <code>memcache-host</code> to one or more hosts running memcached servers is what needs to be done only, the rest of the
system works similar to the internal nanomsg caching but using memcache client instead. The great benefit using memcache is to configure more than one
server in <code>memcache-host</code> separated by comma which makes it more reliable and eliminates single point of failure if one of the memcache servers goes down.</p>
<h2 id="redis">Redis</h2>
<p>Set <code>cache-type=redis</code> and point <code>redis-host</code> to the server running Redis server. Only single Redis server can be specified.</p>
<h1 id="pub-sub-configurations">PUB/SUB configurations</h1>
<p>Publish/subscribe functionality allows clients to receive notifications without constantly polling for new events. A client can be anything but
the backend provides some partially implemented subscription notifications for Web clients using the Long Poll.
The Account API call <code>/account/subscribe</code> can use any pub/sub mode.</p>
<p>The flow of the pub/sub operations is the following:</p>
<ul>
<li>a HTTP client makes <code>/account/subscribe</code> API request, the connection is made and is kept open indefenitely or as long as configured using <code>api-subscribe-timeout</code>.</li>
<li>the API backend receives this request, and runs the <code>api.subscribe</code> method with the key being the account id, this will subscribe to the events for the current
account and registers a callback to be called if any events occured. The HTTP connection is kept open.</li>
<li>some other client makes an API call that triggers an event like makes a connectiopn or sends a message, on such event the backend API handler
always runs <code>ipc.publish</code> after the DB operation succedes. If the messaging is configured, it publishes the message for the account, the
message being a JSON object with the request API path and mtime, other properties depend on the call made.</li>
<li>the connection that initiated <code>/account/subscribe</code> receives an event</li>
</ul>
<h2 id="nanomsg">nanomsg</h2>
<p>To use publish/subcribe with nanomsg, first nanomsg must be compiled in the backend module. Usually this is done when explicitely installed with <code>--backendjs_nanomsg</code>
options to the npm install, see above how to install the package.</p>
<p>All nodes must have the same configuration, similar to the LRU cache otherwise some unexpected behaviour may happen.
The config parameter <code>msg-host</code> defines where to publish messages and from where messages can be retrieved. Having more than one hosts listed will ensure
better reliability of delivering messages, publishing will be load-balanced between all configured hosts.</p>
<h2 id="redis">Redis</h2>
<p>To configure the backend to use Redis for messaging set <code>msg-type=redis</code> and <code>redis-host=HOST</code> where HOST is IP address or hostname of the single Redis server.</p>
<h2 id="rabbitmq">RabbitMQ</h2>
<p>To configure the backend to use RabbitMQ for messaging set <code>msg-type=amqp</code> and <code>amqp-host=HOST</code> and optionally <code>amqp-options=JSON</code> with options to the amqp module.</p>
<h1 id="security-configurations">Security configurations</h1>
<h2 id="api-only">API only</h2>
<p>This is default setup of the backend when all API requests except <code>/account/add</code> must provide valid signature and all HTML, Javascript, CSS and image files
are available to everyone. This mode assumes that Web developmnt will be based on &#39;single-page&#39; design when only data is requested from the Web server and all
rendering is done using Javascript. This is how the <code>api.html</code> develpers console is implemented, using JQuery-UI and Knockout.js.</p>
<p>To see current default config parameters run any of the following commands:</p>
<pre><code>    bkjs run-backend -help | grep api-allow

    node -e &#39;require(&quot;backendjs&quot;).core.showHelp()&#39;
</code></pre><p>To disable open registration in this mode just add config parameter <code>api-disallow-path=^/account/add$</code> or if developing an application add this in the initMiddleware</p>
<pre><code>    api.initMiddleware = function(callback) {
        this.allow.splice(this.allow.indexOf(&#39;^/account/add$&#39;), 1);
    }
</code></pre><h2 id="secure-web-site-client-verification">Secure Web site, client verification</h2>
<p>This is a mode when the whole Web site is secure by default, even access to the HTML files must be authenticated. In this mode the pages must defined &#39;Backend.session = true&#39;
during the initialization on every html page, it will enable Web sessions for the site and then no need to sign every API reauest.</p>
<p>The typical client Javascript verification for the html page may look like this, it will redirect to login page if needed,
this assumes the default path &#39;/public&#39; still allowed without the signature:</p>
<pre><code>    &lt;link href=&quot;/styles/jquery-ui.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot; /&gt;
    &lt;script src=&quot;/js/jquery.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/jquery-ui.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/knockout.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/js/crypto.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/backendjs.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/backendjs-jquery-ui.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
    $(function () {
        Backendjs.session = true;
        ko.applyBindings(Backendjs);

        Backendjs.login(function(err, data) {
            if (err) window.location=&#39;/public/index.html&#39;;
        });
    });
    &lt;/script&gt;
</code></pre><h2 id="secure-web-site-backend-verification">Secure Web site, backend verification</h2>
<p>On the backend side in your application app.js it needs more secure settings defined i.e. no html except /public will be accessible and
in case of error will be redirected to the login page by the server. Note, in the login page <code>Backendjs.session</code> must be set to true for all
html pages to work after login without singing every API request.</p>
<p>First we disable all allowed paths to the html and registration:</p>
<pre><code>    app.configureMiddleware = function(options, callback) {
        self.allow.splice(self.allow.indexOf(&#39;^/$&#39;), 1);
        self.allow.splice(self.allow.indexOf(&#39;\\.html$&#39;), 1);
        self.allow.splice(self.allow.indexOf(&#39;^/account/add$&#39;), 1);
        callback();
    }
</code></pre><p>Second we define auth callback in the app and redirect to login if the reauest has no valid signature, we check all html pages, all allowed html pages from the /public
will never end up in this callback because it is called after the signature check but allowed pages are served before that:</p>
<pre><code>    api.registerPreProcess(&#39;&#39;, /^\/$|\.html$/, function(req, status, callback) {
        if (status.status != 200) {
            status.status = 302;
            status.url = &#39;/public/index.html&#39;;
        }
        callback(status);
    });
</code></pre><h1 id="websockets-connections">WebSockets connections</h1>
<p>The simplest way is to configure <code>ws-port</code> to the same value as the HTTP port. This will run WebSockets server along the regular Web server.
All requests must be properly signed with all parameters encoded as for GET requests.</p>
<p>Example:</p>
<pre><code>    wscat --connect ws://localhost:8000
    connected (press CTRL+C to quit)
    &gt; /account/get
    &lt; {
        &quot;status&quot;: 400,
        &quot;message&quot;: &quot;Invalid request: no host provided&quot;
      }
    &gt;
</code></pre><h1 id="the-backend-provisioning-utility-bkjs">The backend provisioning utility: bkjs</h1>
<p>The purpose of the <code>bkjs</code> shell script is to act as a helper tool in configuring and managing the backend environment
and as well to be used in operations on production systems. It is not required for the backend operations and provided as a convenience tool
which is used in the backend development and can be useful for others running or testing the backend.</p>
<p>Running without arguments will bring help screen with description of all available commands.</p>
<p>The tool is multi-command utility where the first argument is the command to be executed with optional additional arguments if needed.
On Linux, when started the bkjs tries to load and source the following config files:</p>
<pre><code>    /etc/sysconfig/bkjs
    $BKJS_HOME/etc/profile
</code></pre><p>Any of the following config files can redefine any environmnt variable thus pointing to the correct backend environment directory or
customize the running environment, these should be regular shell scripts using bash syntax.</p>
<p>Most common used commands are:</p>
<ul>
<li>bkjs run-backend - run the backend or the app for development purposes, uses local app.js if exists otherwise runs generic server</li>
<li>bkjs run-shell - start REPL shell with the backend module loaded and available for use, all submodules are availablein the shell as well like core, db, api</li>
<li>bkjs init-app - create the app skeleton</li>
<li>bkjs put-backend [-path path] [-host host] [-user user] - sync sources of the app with the remote site, uses BKJS_HOST env variable for host if not specified in the command line, this is for developent version of the backend only</li>
<li><p>bkjs init-server [-home path] [-user user] [-host name] [-domain name] - initialize Linux instance(Amazon,CentOS) for backend use, optional -home can be specified where the backend
 home will be instead of ~/.bkjs, optional -user tells to use existing user instead of the current user.</p>
<p> <strong>This command will create <code>/etc/sysconfig/bkjs</code> file with BKJS_HOME set to the home of the
 backendjs app which was pased in the command line. This makes the bkjs or bksh run globally regardless of the current directory.</strong></p>
</li>
</ul>
<h1 id="deployment-use-cases">Deployment use cases</h1>
<h2 id="custom-aws-instance-setup">Custom AWS instance setup</h2>
<p>Here is the example how to setup new custom AWS server, it is not required and completely optional but bkjs provies some helpful commands that may simplify
new image configuration.</p>
<ul>
<li>start new AWS instance via AWS console, use Amazon Linux</li>
<li>login as <code>ec2-user</code></li>
<li><p>install commands</p>
<pre><code>  yum-config-manager --enable epel
  sudo yum install npm
  npm install backendjs --backendjs_nanomsg --backendjs_imagemagick
  sudo bkjs init-service
  bkjs restart
</code></pre></li>
<li><p>try to access the instance via HTTP port 8000 for the API console or documentation</p>
</li>
<li>after reboot the server will be started automatically</li>
</ul>
<h2 id="custom-aws-instance">Custom AWS instance</h2>
<p>Run the backendjs on the AWS instance as user ec2-user with the backend in the user home</p>
<ul>
<li>start new AWS instance via AWS console, use Amazon Linux or CentOS 6</li>
<li>login as <code>ec2-user</code></li>
<li><p>install commands</p>
<pre><code>  curl -L -o /tmp/bkjs http://backendjs.io/bkjs &amp;&amp; chmod 755 /tmp/bkjs
  /tmp/bkjs install -user ec2-user -prefix ec2-user
  bkjs restart
</code></pre></li>
<li><p>run <code>ps agx</code>, it should show several backend processes running</p>
</li>
<li>try to access the instance via HTTP port for the API console or documentation</li>
</ul>
<h2 id="aws-beanstalk-deployment">AWS Beanstalk deployment</h2>
<p>As with any node.js module, the backendjs app can be packaged into zip file according to AWS docs and deployed the same way as any other node.js app.
Inside the app package etc/config file can be setup for any external connections.</p>
<h2 id="configure-http-port">Configure HTTP port</h2>
<p>The first thing when deploying the backend into production is to change API HTTP port, by default is is 8000, but we would want port 80 so regardless
how the environment is setup it is ultimatley 2 ways to specify the port for HTTP server to use:</p>
<ul>
<li><p>config file</p>
<p>The config file is always located in the etc/ folder in the backend home directory, how the home is specified depends on the system but basically it can be
defined via command line arguments as <code>-home</code> or via environment variables when using bkjs. See bkjs documentation but on AWS instances created with bkjs
<code>init-server</code> command, for non-standard home use <code>/etc/sysconfig/bkjs</code> profile, specify <code>BKJS_HOME=/home/backend</code> there and the rest will be taken care of</p>
</li>
<li><p>command line arguments</p>
<p>When running node scripts which use the backend, just specify <code>-home</code> command line argument with the directory where yor backend should be and the backend will use it</p>
<p>Example:</p>
<pre><code>  node app.js -home $HOME -port 80
</code></pre></li>
<li><p>config database</p>
<p>If <code>-db-config</code> is specified in the command line or <code>db-config=</code> in the local config file, this will trigger loading additional
config parameters from the specified databae pool, it will load all records from tbe bk_config table on that db pool. <code>db-config-type</code> defines the
configuration group or type to load, by default all records will be use for config parameters if not specified. Using the database to store
configuration make it easier to maintain dynamic environment for example in case of auto scaling or lanching on demand, this way
a new instance will query current config from the database and this eliminates supporting text files and distributing them to all instances.</p>
</li>
<li><p>DNS records
Some config options may be kept in the DNS TXT records and every time a instance is started it will query the local DNS for such parameters. Only a small subset of
all config parameters support DNS store. To see which parmeteres can be stored in the DNS run <code>bkjs show-help</code> and look for &#39;DNS TXT configurable&#39;.</p>
</li>
</ul>
<h1 id="security">Security</h1>
<p>All requests to the API server must be signed with account login/secret pair.</p>
<ul>
<li>The algorithm how to sign HTTP requests (Version 1, 2):<ul>
<li>Split url to path and query parameters with &quot;?&quot;</li>
<li>Split query parameters with &quot;&amp;&quot;</li>
<li>&#39;&#39;&#39;ignore parameters with empty names&#39;&#39;&#39;</li>
<li>&#39;&#39;&#39;Sort&#39;&#39;&#39; list of parameters alphabetically</li>
<li>Join sorted list of parameters with &quot;&amp;&quot;<ul>
<li>Make sure all + are encoded as %2B</li>
</ul>
</li>
<li>Form canonical string to be signed as the following:<ul>
<li>Line1: The HTTP method(GET), followed by a newline.</li>
<li>Line2: the host, lowercase, followed by a newline.</li>
<li>Line3: The request URI (/), followed by a newline.</li>
<li>Line4: The sorted and joined query parameters as one string, followed by a newline.</li>
<li>Line5: The expiration value in milliseconds, required, followed by a newline</li>
<li>Line6: The Content-Type HTTP header, lowercase, followed by a newline</li>
</ul>
</li>
<li>Computed HMAC-SHA1 digest from the canonical string and encode it as BASE64 string, preserve trailing = if any</li>
<li>Form BK-Signature HTTP header as the following:<ul>
<li>The header string consist of multiple fields separated by pipe |<ul>
<li>Field1: Signature version:<ul>
<li>version 1, normal signature</li>
<li>version 2, only used in session cookies, not headers</li>
<li>version 3, same as 1 but uses SHA256</li>
</ul>
</li>
<li>Field2: Application version or other app specific data</li>
<li>Field3: account login or whatever it might be in the login column</li>
<li>Field4: HMAC-SHA digest from the canonical string, version 1 o 3 defines SHA1 or SHA256</li>
<li>Field5: expiration value in milliseconds, same as in the canonical string</li>
<li>Field6: SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query paremeters</li>
<li>Field7: empty, reserved for future use</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The resulting signature is sent as HTTP header bk-signature: string</p>
<p>For JSON content type, the method must be POST and no query parameters specified, instead everything should be inside the JSON object
which is placed in the body of the request. For additional safety, SHA1 checksum of the JSON paylod can be calculated and passed in the signature,
this is the only way to ensure the body is not modified when not using query parameters.</p>
<p>See web/js/backendjs.js for function Backendjs.sign or function core.signRequest in the core.js for the Javascript implementation.</p>
<h1 id="backend-framework-development-mac-os-x-developers-">Backend framework development (Mac OS X, developers)</h1>
<ul>
<li><p>for DB drivers and ImageMagick to work propely it needs some dependencies to be installed:</p>
<pre><code>  port install libpng jpeg tiff lcms2 mysql56 postgresql93
</code></pre></li>
<li><p>make sure there is no openjpeg15 installed, it will conflict with ImageMagick jp2 codec</p>
</li>
<li><p><code>git clone https://github.com/vseryakov/backendjs.git</code> or <code>git clone git@github.com:vseryakov/backendjs.git</code></p>
</li>
<li><p>cd backendjs</p>
</li>
<li><p>if node.js is already installed skip to the next section</p>
<ul>
<li><p>node.js can be compiled by the bkjs and installed into default location, on Darwin it is /opt/local</p>
</li>
<li><p>to install node.js in $BKJS_PREFIX/bin run command:</p>
<pre><code>  ./bkjs build-node
</code></pre></li>
<li><p>to specify a different install path for the node run</p>
<pre><code>  ./bksj build-node -prefix $HOME
</code></pre></li>
<li><p><strong>Important</strong>: Add NODE_PATH=$BKJS_PREFIX/lib/node_modules to your environment in .profile or .bash_profile so
node can find global modules, replace $BKJS_PREFIX with the actual path unless this variable is also set in the .profile</p>
</li>
</ul>
</li>
<li><p>to compile the binary module and all required dependencies just type <code>make</code> or <code>npm build .</code></p>
<ul>
<li><p>to see the actual compiler settings during compilation the following helps:</p>
<pre><code>  make V=1
</code></pre></li>
<li><p>to compile with internal nanomsg and ImageMagick use:</p>
<pre><code>  make force V=1
</code></pre></li>
</ul>
</li>
<li><p>to install all dependencies and make backendjs module and bkjs globally available:</p>
<pre><code>      npm link backendjs
</code></pre></li>
<li><p>to run local server on port 8000 run command:</p>
<pre><code>      ./bkjs run-backend
</code></pre></li>
<li><p>to start the backend in command line mode, the backend environment is prepared and initialized including all database pools.
 This command line access allows you to test and run all functions from all modules of the backend without running full server
 similar to node.js REPL functionality. All modules are accessible from the command line.</p>
<pre><code>      $ ./bkjs run-shell
      &gt; core.version
      &#39;2013.10.20.0&#39;
      &gt; logger.setDebug(2)
</code></pre></li>
</ul>
<h1 id="author">Author</h1>
<p>  Vlad Seryakov</p>
<p>Check out the <a href="http://backendjs.io">Documentation</a> for more details.</p>
<h2 id="configuration-parameters">Configuration parameters</h2>
<ul>
<li><code>help</code> - Print help and exit</li>
<li><code>debug</code> - Enable debugging messages, short of -log debug, -debug 0 will disable debugging, otherwise enable</li>
<li><code>debug-filter</code> - Enable debug filters, format is: +label,... to enable, and -label,... to disable. Only first argument is used for label in logger.debug</li>
<li><code>debug-run-segv</code> - On SEGV crash keep the process spinning so attaching with gdb is possible</li>
<li><code>debug-set-segv</code> - Set default SEGV handler which shows backtrace of calls if debug info is available</li>
<li><code>debug-set-backtrace</code> - Set alternative backtrace on SEGV crashes, including backtrace of V8 calls as well</li>
<li><code>log</code> - Set debugging level: none, log, debug, dev</li>
<li><code>log-file</code> - Log to a file, if not specified used default logfile, disables syslog. Default: &quot;log/message.log&quot;</li>
<li><code>syslog</code> - Write all logging messages to syslog, connect to the local syslog server over Unix domain socket</li>
<li><code>console</code> - All logging goes to the console resetting all previous log related settings, this is used in the development mode mostly</li>
<li><code>home</code> - Specify home directory for the server, the server will try to chdir there or exit if it is not possible, the directory must exist. Default: &quot;/Users/vlad/.bkjs&quot;</li>
<li><code>conf-file</code> - Name of the config file to be loaded instead of the default etc/config, can be relative or absolute path. Default: &quot;config&quot;</li>
<li><code>err-file</code> - Path to the error log file where daemon will put app errors and crash stacks. Default: &quot;log/error.log&quot;</li>
<li><code>etc-dir</code> - Path where to keep config files</li>
<li><code>web-dir</code> - Path where to keep web pages</li>
<li><code>views-dir</code> - Path where to keep web template views</li>
<li><code>tmp-dir</code> - Path where to keep temp files</li>
<li><code>spool-dir</code> - Path where to keep modifiable files</li>
<li><code>log-dir</code> - Path where to keep other log files, log-file and err-file are not affected by this</li>
<li><code>files-dir</code> - Path where to keep uploaded files</li>
<li><code>images-dir</code> - Path where to keep images</li>
<li><code>modules-dir</code> - Directory from where to load modules, these are the backendjs modules but in the same format and same conventions as regular node.js modules, the format of the files is NAME_{web,worker,shell}.js. The modules can load any other files or directories, this is just an entry point</li>
<li><code>uid</code> - User id or name to switch after startup if running as root, used by Web servers and job workers</li>
<li><code>gid</code> - Group id or name to switch after startup if running to root</li>
<li><code>email</code> - Email address to be used when sending emails from the backend</li>
<li><code>force-uid</code> - Drop privileges if running as root by all processes as early as possibly, this reqiures uid being set to non-root user. A convenient switch to start the backend without using any other tools like su or sudo.</li>
<li><code>umask</code> - Permissions mask for new files, calls system umask on startup, if not specified the current umask is used. Default: &quot;0002&quot;</li>
<li><code>port</code> - port to listen for the HTTP server, this is global default. Default: 8000</li>
<li><code>bind</code> - Bind to this address only, if not specified listen on all interfaces. Default: &quot;0.0.0.0&quot;</li>
<li><code>backlog</code> - The maximum length of the queue of pending connections, used by HTTP server in listen.. Default: 511</li>
<li><code>ws-port</code> - port to listen for WebSocket server, it can be the same as HTTP/S ports to co-exist on existing web servers</li>
<li><code>ws-bind</code> - Bind to this address only for WebSocket, if not specified listen on all interfaces, only when the port is different from existing web ports</li>
<li><code>ssl-port</code> - port to listen for HTTPS server, this is global default, be advised that proxy-port takes precedence</li>
<li><code>ssl-bind</code> - Bind to this address only for HTTPS server, if not specified listen on all interfaces</li>
<li><code>ssl-key</code> - Path to SSL prvate key</li>
<li><code>ssl-cert</code> - Path to SSL certificate</li>
<li><code>ssl-pfx</code> - A string or Buffer containing the private key, certificate and CA certs of the server in PFX or PKCS12 format. (Mutually exclusive with the key, cert and ca options.)</li>
<li><code>ssl-ca</code> - An array of strings or Buffers of trusted certificates in PEM format. If this is omitted several well known root CAs will be used, like VeriSign. These are used to authorize connections.</li>
<li><code>ssl-passphrase</code> - A string of passphrase for the private key or pfx</li>
<li><code>ssl-crl</code> - Either a string or list of strings of PEM encoded CRLs (Certificate Revocation List)</li>
<li><code>ssl-ciphers</code> - A string describing the ciphers to use or exclude. Consult <a href="http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT">http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT</a> for details on the format</li>
<li><code>ssl-request-cert</code> - If true the server will request a certificate from clients that connect and attempt to verify that certificate. </li>
<li><code>concurrency</code> - How many simultaneous tasks to run at the same time inside one process, this is used by async module only to perform several tasks at once, this is not multithreading but and only makes sense for I/O related tasks. Default: 2</li>
<li><code>timeout</code> - HTTP request idle timeout for servers in ms, how long to keep the connection socket open, this does not affect Long Poll requests. Default: 30000</li>
<li><code>daemon</code> - Daemonize the process, go to the background, can be specified only in the command line</li>
<li><code>shell</code> - Run command line shell, load the backend into the memory and prompt for the commands, can be specified only in the command line</li>
<li><code>monitor</code> - For production use, monitors the master and Web server processes and restarts if crashed or exited, can be specified only in the command line</li>
<li><code>master</code> - Start the master server, can be specified only in the command line, this process handles job schedules and starts Web server, keeps track of failed processes and restarts them</li>
<li><code>proxy-port</code> - Start the HTTP reverse proxy server, all Web workers will listen on different ports and will be load-balanced by the proxy, the proxy server will listen on global HTTP port and all workers will listen on ports starting with the proxy-port</li>
<li><code>proxy-ssl</code> - Start HTTPS reverse proxy to accept incoming SSL requests, ssl-key/cert must be defined</li>
<li><code>app-name</code> - Set appName and version explicitely an skip reading it from package.json, it can be just a name or name-version</li>
<li><code>instance-tag</code> - Set instance tag explicitely, skip all meta data checks for it</li>
<li><code>instance-region</code> - Set instance region explicitely, skip all meta data checks for it</li>
<li><code>instance-zone</code> - Set instance zone explicitely, skip all meta data checks for it</li>
<li><code>instance-job</code> - Enables remote job mode, it means the backendjs is running in the cloud to execute a job or other task and can be terminated during the idle timeout</li>
<li><code>run-mode</code> - Running mode for the app, used to separate different running environment and configurations. Default: &quot;development&quot;. DNS TXT configurable.</li>
<li><code>web</code> - Start Web server processes, spawn workers that listen on the same port, for use without master process which starts Web servers automatically</li>
<li><code>no-web</code> - Disable Web server processes, without this flag Web servers start by default</li>
<li><code>repl-port-web</code> - Web server REPL port, if specified it initializes REPL in the Web server processes, in workers port is port+workerid+1</li>
<li><code>repl-bind-web</code> - Web server REPL listen address. Default: &quot;127.0.0.1&quot;</li>
<li><code>repl-port</code> - Port for REPL interface in the master, if specified it initializes REPL in the master server process</li>
<li><code>repl-bind</code> - Listen only on specified address for REPL server in the master process. Default: &quot;127.0.0.1&quot;</li>
<li><code>repl-file</code> - User specified file for REPL history. Default: &quot;.history&quot;</li>
<li><code>lru-max</code> - Max number of items in the LRU cache, this cache is managed by the master Web server process and available to all Web processes maintaining only one copy per machine, Web proceses communicate with LRU cache via IPC mechanism between node processes. Default: 100000</li>
<li><code>no-queue</code> - Disable nanomsg queue sockets</li>
<li><code>queue-port</code> - Ports to use for nanomsg sockets for publish/subscribe queues, 2 ports will be used, this one and the next. Default: 20110</li>
<li><code>queue-type</code> - One of the redis, amqp or nanomsg to use for PUB/SUB queues, default is nanomsg sockets. Default: &quot;nanomsg&quot;</li>
<li><code>queue-host</code> - Server(s) where clients publish and subscribe with nanomsg sockets, IPs or hosts separated by comma, TCP port is optional, msg-port is used. Default: &quot;127.0.0.1&quot;. DNS TXT configurable.</li>
<li><code>queue-bind</code> - Listen only on specified address for queue sockets in the master process</li>
<li><code>memcache-host</code> - List of memcached servers for cache messages: IP[:port],host[:port]... DNS TXT configurable.</li>
<li><code>memcache-options</code> - JSON object with options to the Memcached client, see npm doc memcached</li>
<li><code>redis-port</code> - Port to Redis server for cache and messaging. DNS TXT configurable.</li>
<li><code>redis-host</code> - Address to Redis server for cache and messaging. DNS TXT configurable.</li>
<li><code>redis-options</code> - JSON object with options to the Redis client, see npm doc redis</li>
<li><code>amqp-host</code> - Host running RabbitMQ</li>
<li><code>amqp-options</code> - JSON object with options to the AMQP client, see npm doc amqp</li>
<li><code>cache-type</code> - One of the local, redis, memcache or nanomsg to use for caching in API requests. Default: &quot;nanomsg&quot;</li>
<li><code>cache-host</code> - Address of nanomsg cache servers, IPs or hosts separated by comma: IP:[port],host[:[port], if TCP port is not specified, cache-port is used. Default: &quot;127.0.0.1&quot;. DNS TXT configurable.</li>
<li><code>cache-port</code> - Port to use for nanomsg sockets for cache requests. Default: 20100</li>
<li><code>cache-bind</code> - Listen only on specified address for cache sockets server in the master process</li>
<li><code>worker</code> - Set this process as a worker even it is actually a master, this skips some initializations</li>
<li><code>logwatcher-url</code> - The backend URL where logwatcher reports should be sent instead of sending over email</li>
<li><code>logwatcher-email</code> - Email address for the logwatcher notifications, the monitor process scans system and backend log files for errors and sends them to this email address, if not specified no log watching will happen. DNS TXT configurable.</li>
<li><code>logwatcher-from</code> - Email address to send logwatcher notifications from, for cases with strict mail servers accepting only from known addresses</li>
<li><code>logwatcher-ignore</code> - Regexp with patterns that need to be ignored by the logwatcher process, it is added to the list of ignored patterns. Default: [&quot; (NOTICE|DEBUG|DEV): &quot;]</li>
<li><code>logwatcher-match</code> - Regexp patterns that match conditions for logwatcher notifications, this is in addition to default backend logger patterns. Default: [&quot; (ERROR|WARNING|WARN|ALERT|EMERG|CRIT): &quot;,&quot;message\&quot;:\&quot;ERROR:&quot;]</li>
<li><code>logwatcher-interval</code> - How often to check for errors in the log files in minutes. Default: 60</li>
<li><code>logwatcher-file</code> - Add a file to be watched by the logwatcher, it will use all configured match patterns</li>
<li><code>user-agent</code> - Add HTTP user-agent header to be used in HTTP requests, for scrapers or other HTTP requests that need to be pretended coming from Web browsers. Default: []</li>
<li><code>backend-host</code> - Host of the master backend, can be used for backend nodes communications using core.sendRequest function calls with relative URLs, also used in tests.</li>
<li><code>backend-login</code> - Credentials login for the master backend access when using core.sendRequest</li>
<li><code>backend-secret</code> - Credentials secret for the master backend access when using core.sendRequest</li>
<li><code>host-name</code> - Hostname/domain to use for communications, default is current domain of the host machine</li>
<li><code>config-domain</code> - Domain to query for configuration TXT records, must be specified to enable DNS configuration</li>
<li><code>max-distance</code> - Max searchable distance(radius) in km, for location searches to limit the upper bound. Default: 50</li>
<li><code>min-distance</code> - Radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. Default: 5</li>
<li><code>watch</code> - Watch sources directory for file changes to restart the server, for development only, the backend module files will be added to the watch list automatically, so only app specific directores should be added. In the production -monitor must be used.</li>
<li><code>db-pool</code> - Default pool to be used for db access without explicit pool specified. Default: &quot;sqlite&quot;. DNS TXT configurable.</li>
<li><code>db-no-pools</code> - Do not use other db pools except the default pool specified by &#39;db-pool&#39;</li>
<li><code>db-no-cache-columns</code> - Do not load column definitions from the database tables on startup, keep using in-app Javascript definitions only, in most cases caching columns is not required if tables are in sync between the app and the database</li>
<li><code>db-no-init-tables</code> - Do not create tables in the database on startup and do not perform table upgrades for new columns, all tables are assumed to be created beforehand, disabling this will turn on table creation in the shell and master processes</li>
<li><code>db-cache-tables</code> - List of tables that can be cached: bk_auth, bk_counter. This list defines which DB calls will cache data with currently configured cache. This is global for all db pools.. Default: []</li>
<li><code>db-local</code> - Local database pool for properties, cookies and other local instance only specific stuff. Default: &quot;sqlite&quot;</li>
<li><code>db-config</code> - Configuration database pool to be used to retrieve config parameters from the database, must be defined to use remote db for config parameters, set to <code>default</code> to use current default pool</li>
<li><code>db-config-interval</code> - Interval between loading configuration from the database configured with -db-config-type, in seconds, 0 disables refreshing config from the db. Default: 300</li>
<li><code>db-sqlite-pool</code> - SQLite pool db name, absolute path or just a name for the db file created in var/. 3 variants: sqlite-pool-1 .. sqlite-pool-3.</li>
<li><code>db-pgsql-pool</code> - PostgreSQL pool access url in the format: postgresql://[user:password@]hostname[:port]/db. 3 variants: pgsql-pool-1 .. pgsql-pool-3.</li>
<li><code>db-mysql-pool</code> - MySQL pool access url in the format: mysql://[user:password@]hostname/db. 3 variants: mysql-pool-1 .. mysql-pool-3.</li>
<li><code>db-dynamodb-pool</code> - DynamoDB endpoint url, a region or &#39;default&#39; to use AWS account default region. 3 variants: dynamodb-pool-1 .. dynamodb-pool-3.</li>
<li><code>db-mongodb-pool</code> - MongoDB endpoint url in the format: mongodb://hostname[:port]/dbname. 3 variants: mongodb-pool-1 .. mongodb-pool-3.</li>
<li><code>db-cassandra-pool</code> - Casandra endpoint url in the format: cql://[user:password@]hostname[:port]/dbname. 3 variants: cassandra-pool-1 .. cassandra-pool-3.</li>
<li><code>db-lmdb-pool</code> - Path to the local LMDB database. 3 variants: lmdb-pool-1 .. lmdb-pool-3.</li>
<li><code>db-leveldb-pool</code> - Path to the local LevelDB database. 3 variants: leveldb-pool-1 .. leveldb-pool-3.</li>
<li><code>db-redis-pool</code> - Redis host. 3 variants: redis-pool-1 .. redis-pool-3.</li>
<li><code>db-elasticsearch-pool</code> - ElasticSearch url to the host in the format: <a href="http://hostname[:port">http://hostname[:port</a>]. 3 variants: elasticsearch-pool-1 .. elasticsearch-pool-3.</li>
<li><code>db-couchdb-pool</code> - CouchDB url to the host in the format: <a href="http://hostname[:port]/dbname">http://hostname[:port]/dbname</a>. 3 variants: couchdb-pool-1 .. couchdb-pool-3.</li>
<li><code>db-riak-pool</code> - Riak url to the host in the format: <a href="http://hostname[:port">http://hostname[:port</a>]. 3 variants: riak-pool-1 .. riak-pool-3.</li>
<li><code>db-NAME-pool-max</code> - Max number of open connections for a pool. Where NAME is the actual pool name.. 3 variants: pool-max-1 .. pool-max-3.</li>
<li><code>db-NAME-pool-min</code> - Min number of open connections for a pool. Where NAME is the actual pool name.. 3 variants: pool-min-1 .. pool-min-3.</li>
<li><code>db-NAME-pool-idle</code> - Number of ms for a db pool connection to be idle before being destroyed. Where NAME is the actual pool name.. 3 variants: pool-idle-1 .. pool-idle-3.</li>
<li><code>db-NAME-pool-tables</code> - A DB pool tables, list of tables that belong to this pool only. Where NAME is the actual pool name.. 3 variants: pool-tables-1 .. pool-tables-3.</li>
<li><code>db-NAME-pool-init-options</code> - Options for a DB pool driver passed during creation of a pool. Where NAME is the actual pool name.. 3 variants: pool-init-options-1 .. pool-init-options-3.</li>
<li><code>db-NAME-pool-options</code> - A DB pool driver options passed to every request. Where NAME is the actual pool name.. 3 variants: pool-options-1 .. pool-options-3.</li>
<li><code>db-NAME-pool-no-cache-columns</code> - disable caching table columns for this pool only. Where NAME is the actual pool name.. 3 variants: pool-no-cache-columns-1 .. pool-no-cache-columns-3.</li>
<li><code>db-NAME-pool-no-init-tables</code> - Do not crate tables for this pool only. Where NAME is the actual pool name.. 3 variants: pool-no-init-tables-1 .. pool-no-init-tables-3.</li>
<li><code>aws-key</code> - AWS access key</li>
<li><code>aws-secret</code> - AWS access secret</li>
<li><code>aws-region</code> - AWS region</li>
<li><code>aws-sdk-profile</code> - AWS SDK profile to use when reading credentials file</li>
<li><code>aws-ddb-read-capacity</code> - Default DynamoDB read capacity for all tables</li>
<li><code>aws-ddb-write-capacity</code> - Default DynamoDB write capacity for all tables</li>
<li><code>aws-sns-app-arn</code> - SNS Platform application ARN to be used for push notifications</li>
<li><code>aws-key-name</code> - AWS instance keypair name for remote job instances</li>
<li><code>aws-elb-name</code> - AWS ELB name to be registered with on start up</li>
<li><code>aws-iam-profile</code> - IAM instance profile name</li>
<li><code>aws-image-id</code> - AWS image id to be used for instances</li>
<li><code>aws-subnet-id</code> - AWS subnet id to be used for instances</li>
<li><code>aws-group-id</code> - AWS security group(s) to be used for instances</li>
<li><code>aws-instance-type</code> - AWS instance type for remote jobs launched on demand. Default: &quot;t1.micro&quot;</li>
<li><code>msg-apn-cert</code> - Certificate for APN service, pfx format, .p12 ext</li>
<li><code>msg-apn-production</code> - Enable APN production mode of operations, if not specified the mode is derived from the certificate name, presence of the word &#39;production&#39; in the cert file name will enable production mode</li>
<li><code>msg-gcm-key</code> - Google Cloud Messaging API key</li>
<li><code>msg-server-queue</code> - Name for push notification queue to initialize for receiving messages from the clients to forward to the actual gateways, it uses PUB/SUB messaging subsystem</li>
<li><code>msg-client-queue</code> - Name for push notification queue, set to make current backend send all messages to this queue, any name can be used as long as it unqiue and does not interfere with other PUB/SUB prefixes</li>
<li><code>msg-host</code> - List of hosts/IP addresses to be used for actual delivery of push notifications, all other hosts will queue notifications to these servers. DNS TXT configurable.</li>
<li><code>msg-shutdown-timeout</code> - How long to wait for messages draining out in ms on shutting down before exiting. Default: 2000</li>
<li><code>api-images-url</code> - URL where images are stored, for cases of central image server(s), must be full URL with optional path and trailing slash at the end</li>
<li><code>api-images-s3</code> - S3 bucket name where to store and retrieve images</li>
<li><code>api-images-raw</code> - Return raw urls for the images, requires images-url to be configured. The path will reflect the actual 2 level structure and account id in the image name</li>
<li><code>api-images-s3-options</code> - S3 options to sign images urls, may have expires:, key:, secret: properties</li>
<li><code>api-domain</code> - Regexp of the domains or hostnames to be served by the API, if not matched the requests will be only served by the other middleware configured in the Express</li>
<li><code>api-files-s3</code> - S3 bucket name where to store files uploaded with the File API</li>
<li><code>api-busy-latency</code> - Max time in ms for a request to wait in the queue, if exceeds this value server returns too busy error. Default: 1000</li>
<li><code>api-access-log</code> - File for access logging</li>
<li><code>api-init-tables</code> - Initialize/create API tables in the shell/worker or other non-API modules. Default: undefined</li>
<li><code>api-salt</code> - Salt to be used for scrambling credentials or other hashing activities</li>
<li><code>api-notifications</code> - Initialize notifications in the API Web worker process to allow sending push notifications from the API handlers</li>
<li><code>api-no-access-log</code> - Disable access logging in both file or syslog</li>
<li><code>api-no-static</code> - Disable static files from /web folder, no .js or .html files will be served by the server</li>
<li><code>api-no-templating</code> - Disable templating engine completely</li>
<li><code>api-templating</code> - Templating engne to use, see consolidate.js for supported engines, default is ejs. Default: &quot;ejs&quot;</li>
<li><code>api-no-session</code> - Disable cookie session support, all requests must be signed for Web clients</li>
<li><code>api-session-age</code> - Session age in milliseconds, for cookie based authentication. Default: 1209600000</li>
<li><code>api-session-secret</code> - Secret for session cookies, session support enabled only if it is not empty</li>
<li><code>api-query-token-secret</code> - Name of the property to be used for encrypting tokens for pagination..., any property from bk_auth can be used, if empty no secret is used, if not a valid property then it is used as the secret</li>
<li><code>api-access-token-secret</code> - A secret to be used for access token signatures, additional enryption on top of the signature to use for API access without signing requests</li>
<li><code>api-access-token-age</code> - Access tokens age in milliseconds, for API requests with access tokens only. Default: 604800000</li>
<li><code>api-disable</code> - Disable default API by endpoint name: account, message, icon...... Default: []</li>
<li><code>api-disable-session</code> - Disable access to API endpoints for Web sessions, must be signed properly. Default: {}</li>
<li><code>api-allow-connection</code> - Map of connection type to operations to be allowed only, once a type is specified, all operations must be defined, the format is: type:op,type:op.... Default: {}</li>
<li><code>api-allow-admin</code> - URLs which can be accessed by admin accounts only, can be partial urls or Regexp, this is a convenient option which registers AuthCheck callback for the given endpoints. Default: {}</li>
<li><code>api-allow-account-</code> - URLs which can be accessed by specific account type only, can be partial urls or Regexp, this is a convenient option which registers AuthCheck callback for the given endpoints and only allow access to the specified account types</li>
<li><code>api-icon-limit</code> - Set the limit of how many icons by type can be uploaded by an account, type:N,type:N..., type * means global limit for any icon type. Default: {}</li>
<li><code>api-express-enable</code> - Enable/set Express config option(s), can be a list of options separated by comma or pipe |, to set value user name=val,... to just enable use name,..... Default: []</li>
<li><code>api-allow</code> - Regexp for URLs that dont need credentials, replace the whole access list. Default: {&quot;list&quot;:[&quot;^/$&quot;,&quot;\.html$&quot;,&quot;\.ico$&quot;,&quot;\.gif$&quot;,&quot;\.png$&quot;,&quot;\.jpg$&quot;,&quot;\.svg$&quot;,&quot;\.ttf$&quot;,&quot;\.eof$&quot;,&quot;\.woff$&quot;,&quot;\.js$&quot;,&quot;\.css$&quot;,&quot;^/public&quot;,&quot;^/account/add$&quot;,&quot;^/ping&quot;],&quot;rx&quot;:{}}</li>
<li><code>api-allow-path</code> - Add to the list of allowed URL paths without authentication, return result before even checking for the signature</li>
<li><code>api-disallow-path</code> - Remove from the list of allowed URL paths that dont need authentication, most common case is to to remove ^/account/add$ to disable open registration</li>
<li><code>api-allow-anonymous</code> - Add to the list of allowed URL paths that can be served with or without valid account, the difference with <code>allow-path</code> is that it will check for signature and an account but will continue if no login is provided, return error in case of wrong account or not account found. Default: {}</li>
<li><code>api-allow-ssl</code> - Add to the list of allowed URL paths using HTTPs only, plain HTTP requests to these urls will be refused. Default: {}</li>
<li><code>api-redirect-ssl</code> - Add to the list of the URL paths to be redirected to the same path but using HTTPS protocol, for proxy mode the proxy server will perform redirects. Default: {}</li>
<li><code>api-deny</code> - Regexp for URLs that will be denied access, replaces the whole access list. Default: {}</li>
<li><code>api-deny-path</code> - Add to the list of URL paths to be denied without authentication</li>
<li><code>api-subscribe-timeout</code> - Timeout for Long POLL subscribe listener, how long to wait for events before closing the connection, milliseconds. Default: 1800000</li>
<li><code>api-subscribe-interval</code> - Interval between delivering events to subscribed clients, milliseconds. Default: 3000</li>
<li><code>api-status-interval</code> - Number of milliseconds between status record updates, presence is considered offline if last access was more than this interval ago. Default: 900000</li>
<li><code>api-mime-body</code> - Collect full request body in the req.body property for the given MIME type in addition to json and form posts, this is for custom body processing. Default: []</li>
<li><code>api-pages-view</code> - A view template to be used when rendering markdown pages using Express render engine, for /pages/show command and .md files</li>
<li><code>api-pages-main</code> - A template for the main page to be created when starting the wiki engine for the first time, if not given a default simple welcome message will be used</li>
<li><code>api-collect-host</code> - The backend URL where all collected statistics should be sent over, if set to <code>pool</code> then each web worker will save metrics directly into the statistics database pool</li>
<li><code>api-collect-pool</code> - Database pool where to save collected statistics</li>
<li><code>api-collect-interval</code> - How often to collect statistics and metrics in seconds. Default: 30</li>
<li><code>api-collect-send-interval</code> - How often to send collected statistics to the master server in seconds. Default: 300</li>
<li><code>api-signature-age</code> - Max age for request signature in milliseconds, how old the API signature can be to be considered valid, the &#39;expires&#39; field in the signature must be less than current time plus this age, this is to support time drifts</li>
<li><code>api-select-limit</code> - Max value that can be passed in the _count parameter, limits how many records can be retrieved in one API call from the database</li>
<li><code>api-upload-limit</code> - Max size for uploads, bytes. Default: 10485760</li>
<li><code>server-max-processes</code> - Max number of processes to launch for Web servers, 0 means NumberofCPUs-2. Default: 1</li>
<li><code>server-max-workers</code> - Max number of worker processes to launch for jobs. Default: 1</li>
<li><code>server-idle-time</code> - If set and no jobs are submitted the backend will be shutdown, for instance mode only. Default: 120000</li>
<li><code>server-job-max-time</code> - Max number of seconds a job can run before being killed, for instance mode only. Default: 3600</li>
<li><code>server-crash-delay</code> - Delay between respawing the crashed process. Default: 30000</li>
<li><code>server-restart-delay</code> - Delay between respawning the server after changes. Default: 1000</li>
<li><code>server-log-errors</code> - If true, log crash errors from child processes by the logger, otherwise write to the daemon err-file. The reason for this is that the logger puts everything into one line thus breaking formatting for stack traces.</li>
<li><code>server-job</code> - Job specification, JSON encoded as base64 of the job object</li>
<li><code>server-proxy-url</code> - URL regexp to be passed to other web server running behind, it uses the proxy-host config parameters where to forward matched requests. Default: {}</li>
<li><code>server-proxy-reverse</code> - Reverse the proxy logic, proxy all that do not match the proxy-url pattern</li>
<li><code>server-proxy-host</code> - A Web server IP address or hostname where to proxy matched requests, can be just a host or host:port</li>
<li><code>server-proxy-target</code> - An object with virtual host names as property and targets as value, if host: header match any property, use its value as full target in the form <a href="http://host:port">http://host:port</a>. Default: {}</li>
<li><code>server-process-name</code> - Path to the command to spawn by the monitor instead of node, for external processes guarded by this monitor</li>
<li><code>server-process-args</code> - Arguments for spawned processes, for passing v8 options or other flags in case of external processes. Default: []</li>
<li><code>server-worker-args</code> - Node arguments for workers, job and web processes, for passing v8 options. Default: []</li>
<li><code>server-jobs-tag</code> - This server executes jobs that match this tag, if empty then execute all jobs, if not empty execute all that match current IP address and this tag</li>
<li><code>server-job-queue</code> - Name of the queue to process, this is a generic queue name that can be used by any queue provider</li>
<li><code>server-jobs-count</code> - How many jobs to execute at any iteration, this relates to the bk_queue queue processing only. Default: 1</li>
<li><code>server-jobs-interval</code> - Interval between executing job queue, must be set to enable jobs, 0 disables job processing, in seconds, min interval is 60 secs</li>
</ul>
<h2 id="module-api">Module: API</h2>
<ul>
<li><p><code>api</code></p>
<p> HTTP API to the server from the clients, this module implements the basic HTTP(S) API functionality with some common features. The API module
incorporates the Express server which is exposed as api.app object, the master server spawns Web workers which perform actual operations and monitors
the worker processes if they die and restart them automatically. How many processes to spawn can be configured via <code>-server-max-workers</code> config parameter.</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Authentication by login, only keeps id and secret to check the siganture
      bk_auth: { login: { primary: 1 },                   // Account login
                 id: {},                                  // Auto generated UUID
                 alias: {},                               // Account alias
                 secret: {},                              // Account password
                 token_secret: {},                        // Secret for access tokens
                 status: {},                              // Status of the account
                 type: { admin: 1 },                      // Account type: admin, ....
                 acl_deny: { admin: 1 },                  // Deny access to matched url, a regexp
                 acl_allow: { admin: 1 },                 // Only grant access if path matches this regexp
                 query_deny: { admin: 1 },                // Ignore these query params, a regexp
                 expires: { type: &quot;bigint&quot;, admin: 1 },   // Deny access to the account if this value is before current date, milliseconds
                 mtime: { type: &quot;bigint&quot;, now: 1 } },

      // Basic account information
      bk_account: { id: { primary: 1, pub: 1 },
                    login: {},
                    name: {},
                    first_name: {},
                    last_name: {},
                    alias: { pub: 1 },
                    status: { value: &quot;ok&quot; },
                    type: { admin: 1 },
                    email: {},
                    phone: {},
                    website: {},
                    birthday: {},
                    gender: {},
                    address: {},
                    city: {},
                    state: {},
                    zipcode: {},
                    country: {},
                    device_id: {},                                    // Device for notifications
                    geohash: { location: 1 },                         // To prevent regular account updates
                    latitude: { type: &quot;real&quot;, location: 1 },          // overriding location columns
                    longitude: { type: &quot;real&quot;, location: 1 },
                    location: { location: 1 },
                    ltime: { type: &quot;bigint&quot;, location: 1 },           // Last location update time
                    ctime: { type: &quot;bigint&quot;, readonly: 1, now: 1 },   // Create time
                    mtime: { type: &quot;bigint&quot;, now: 1 } },              // Last update time

     // Status/presence support
     bk_status: { id: { primary: 1 },                               // account id
                  status: {},                                       // status, online, offline, away
                  alias: {},
                  atime: { type: &quot;bigint&quot;, now: 1 },                // last access time
                  mtime: { type: &quot;bigint&quot; }},                       // last status save to db time

     // Keep track of icons uploaded
     bk_icon: { id: { primary: 1 },                         // Account id
                type: { primary: 1, pub: 1 },               // prefix:type
                prefix: {},                                 // icon prefix/namespace
                acl_allow: {},                              // Who can see it: all, auth, id:id...
                ext: {},                                    // Saved image extension
                descr: {},
                geohash: {},                                // Location associated with the icon
                latitude: { type: &quot;real&quot; },
                longitude: { type: &quot;real&quot; },
                mtime: { type: &quot;bigint&quot;, now: 1 }},         // Last time added/updated

     // Locations for all accounts to support distance searches
     bk_location: { geohash: { primary: 1 },                    // geohash, core.minDistance defines the size
                    id: { primary: 1, pub: 1 },                 // my account id, part of the primary key for pagination
                    latitude: { type: &quot;real&quot; },
                    longitude: { type: &quot;real&quot; },
                    alias: { pub: 1 },
                    mtime: { type: &quot;bigint&quot;, now: 1 }},

     // All connections between accounts: like,dislike,friend...
     bk_connection: { id: { primary: 1, pub: 1 },                    // my account_id
                      type: { primary: 1, pub: 1 },                  // type:connection
                      connection: { pub: 1 },                        // other id of the connection
                      alias: { pub: 1 },
                      status: {},
                      mtime: { type: &quot;bigint&quot;, now: 1, pub: 1 }},

     // References from other accounts, likes,dislikes...
     bk_reference: { id: { primary: 1, pub: 1 },                    // account_id
                     type: { primary: 1, pub: 1 },                  // type:connection
                     connection: { pub: 1 },                        // other id of the connection
                     alias: { pub: 1 },
                     status: {},
                     mtime: { type: &quot;bigint&quot;, now: 1, pub: 1 }},

     // New messages
     bk_message: { id: { primary: 1 },                         // my account_id
                   mtime: { primary: 1 },                      // mtime:sender
                   sender: { index: 1 },                       // Sender id
                   alias: {},                                  // Sender alias
                   acl_allow: {},                              // Who has access: all, auth, id:id...
                   msg: {},                                    // Text of the message
                   icon: { type: &quot;int&quot; }},                     // 1 - icon present, 0 - no icon

     // Archived messages
     bk_archive: { id: { primary: 1, index: 1 },               // my account_id
                   mtime: { primary: 1 },                      // mtime:sender
                   sender: { index: 1 },                       // Sender id
                   alias: {},                                  // Sender alias
                   msg: {},                                    // Text of the message
                   icon: { type: &quot;int&quot; }},                     // 1 - icon present, 0 - no icon

     // Messages sent
     bk_sent: { id: { primary: 1, index: 1 },                // my account
                mtime: { primary: 1 },                       // mtime:recipient
                recipient: { index: 1 },                     // Recipient id
                alias: {},                                   // Recipient alias
                msg: {},                                     // Text of the message
                icon: { type: &quot;int&quot; }},                      // 1 - icon present, 0 - no icon

     // All accumulated counters for accounts
     bk_counter: { id: { primary: 1, pub: 1 },                               // account id
                   ping: { type: &quot;counter&quot;, value: 0, pub: 1 },              // public column to ping the buddy with notification
                   like0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },        // who i like
                   like1: { type: &quot;counter&quot;, value: 0, autoincr: 1 },        // reversed, who likes me
                   follow0: { type: &quot;counter&quot;, value: 0, autoincr: 1 },      // who i follow
                   follow1: { type: &quot;counter&quot;, value: 0, autoincr: 1 }},     // reversed, who follows me

      // Wiki pages
      bk_pages: { id: { primary: 1, pub: 1 },
                  title: { pub: 1 },
                  subtitle: { pub: 1 },
                  icon: { pub: 1 },                            // icon class, glyphicon, fa....
                  link: { pub: 1 },                            // external link to the content
                  content: { pub: 1 },                         // the page content
                  toc: { type:&quot; bool&quot;, pub: 1 },               // produce table of content
                  pub: { type: &quot;bool&quot;, pub: 1 },               // no account to see thos page
                  userid: { pub: 1 },                          // id of the last user
                  mtime: { type: &quot;bigint&quot;, now: 1, pub: 1 }},

     // Collected metrics per worker process, basic columns are defined in the table to be collected like
     // api and db request rates(.rmean), response times(.hmean) and total number of requests(_0).
     // Counters ending with _0 are snapshots, i.e. they must be summed up for any given interval.
     // All other counters are averages.
     bk_collect: { id: { primary: 1 },
                   mtime: { type: &quot;bigint&quot;, primary: 1 },
                   app: {},
                   ip: {},
                   type: {},
                   instance: {},
                   worker: {},
                   pid: { type: &quot;int&quot; },
                   latency: { type: &quot;int&quot; },
                   cpus: { type: &quot;int&quot; },
                   mem: { type: &quot;bigint&quot; },
                   rss_hmean: { type: &quot;real&quot; },
                   heap_hmean: { type: &quot;real&quot; },
                   avg_hmean: { type: &quot;real&quot; },
                   free_hmean: { type: &quot;real&quot; },
                   util_hmean: { type: &quot;real&quot; },
                   api_req_rmean: { type: &quot;real&quot; },
                   api_req_hmean: { type: &quot;real&quot; },
                   api_req_0: { type: &quot;real&quot; },
                   api_errors_0: { type: &quot;real&quot; },
                   api_bad_0: { type: &quot;real&quot; },
                   api_que_rmean: { type: &quot;real&quot; },
                   api_que_hmean: { type: &quot;real&quot; },
                   pool_req_rmean: { type: &quot;real&quot; },
                   pool_req_hmean: { type: &quot;real&quot; },
                   pool_req_hmean: { type: &quot;real&quot; },
                   pool_req_0: { type: &quot;real&quot; },
                   pool_errors_0: { type: &quot;real&quot; },
                   pool_que_rmean: { type: &quot;real&quot; },
                   pool_que_hmean: { type: &quot;real&quot; },
                   url_account_get_rmean: { type: &quot;real&quot; },
                   url_account_get_hmean: { type: &quot;real&quot; },
                   url_account_get_0: { type: &quot;real&quot; },
                   url_account_select_rmean: { type: &quot;real&quot; },
                   url_account_select_hmean: { type: &quot;real&quot; },
                   url_account_select_0: { type: &quot;real&quot; },
                   url_account_update_rmean: { type: &quot;real&quot; },
                   url_account_update_hmean: { type: &quot;real&quot; },
                   url_account_update_0: { type: &quot;real&quot; },
                   url_message_get_rmean: { type: &quot;real&quot; },
                   url_message_get_hmean: { type: &quot;real&quot; },
                   url_message_get_0: { type: &quot;real&quot; },
                   url_message_add_rmean: { type: &quot;real&quot; },
                   url_message_add_hmean: { type: &quot;real&quot; },
                   url_message_add_0: { type: &quot;real&quot; },
                   url_counter_incr_rmean: { type: &quot;real&quot; },
                   url_counter_incr_hmean: { type: &quot;real&quot; },
                   url_counter_incr_0: { type: &quot;real&quot; },
                   url_connection_get_rmean: { type: &quot;real&quot; },
                   url_connection_get_hmean: { type: &quot;real&quot; },
                   url_connection_get_0: { type: &quot;real&quot; },
                   url_connection_select_rmean: { type: &quot;real&quot; },
                   url_connection_select_hmean: { type: &quot;real&quot; },
                   url_connection_select_0: { type: &quot;real&quot; },
                   url_connection_add_rmean: { type: &quot;real&quot; },
                   url_connection_add_hmean: { type: &quot;real&quot; },
                   url_connection_add_0: { type: &quot;real&quot; },
                   url_connection_incr_rmean: { type: &quot;real&quot; },
                   url_connection_incr_hmean: { type: &quot;real&quot; },
                   url_connection_incr_0: { type: &quot;real&quot; },
                   url_connection_del_rmean: { type: &quot;real&quot; },
                   url_connection_del_hmean: { type: &quot;real&quot; },
                   url_connection_del_0: { type: &quot;real&quot; },
                   url_location_get_rmean: { type: &quot;real&quot; },
                   url_location_get_hmean: { type: &quot;real&quot; },
                   url_location_get_0: { type: &quot;real&quot; },
                   url_location_put_rmean: { type: &quot;real&quot; },
                   url_location_put_hmean: { type: &quot;real&quot; },
                   url_location_put_0: { type: &quot;real&quot; },
                   url_icon_get_rmean: { type: &quot;real&quot; },
                   url_icon_get_hmean: { type: &quot;real&quot; },
                   url_icon_get_0: { type: &quot;real&quot; },
                   url_image_account_rmean: { type: &quot;real&quot; },
                   url_image_account_hmean: { type: &quot;real&quot; },
                   url_image_account_0: { type: &quot;real&quot; },
                   url_image_message_rmean: { type: &quot;real&quot; },
                   url_image_message_hmean: { type: &quot;real&quot; },
                   url_image_message_0: { type: &quot;real&quot; },
                   ctime: { type: &quot;bigint&quot; }},
</code></pre></li>
</ul>

<ul>
<li><p><code>api.init(options, callback)</code></p>
<p> Initialize API layer, this must be called before the <code>api</code> module can be used but it is called by the server module automatically so <code>api.init</code> is
rearely need to called directly, only for new server implementation or if using in the shell for testing.</p>
<p>During the init sequence, this function calls <code>api.initMiddleware</code> and <code>api.initApplication</code> methods which by default are empty but can be redefined in the user aplications.</p>
<p>The backendjs.js uses its own request parser that places query parameters into <code>req.query</code> or <code>req.body</code> depending on the method.</p>
<p>For GET method, <code>req.query</code> contains all url-encoded parameters, for POST method <code>req.body</code> contains url-encoded parameters or parsed JSON payload or multipart payload.</p>
<p>The reason not to do this by default is that this may not be the alwayse wanted case and distinguishing data coming in the request or in the body may be desirable,
also, this will needed only for Express handlers <code>.all</code>, when registering handler by method like <code>.get</code> or <code>.post</code> then the handler needs to deal with only either source of the request data.</p>
</li>
</ul>

<ul>
<li><p><code>api.shutdown(callback)</code></p>
<p> Gracefully close all connections, call the callback after that</p>
</li>
</ul>

<ul>
<li><p><code>api.configureWorker(options, callback)</code></p>
<p> Allow access to API table in worker processes</p>
</li>
</ul>

<ul>
<li><p><code>api.configureShell(options, callback)</code></p>
<p> Access to the API table in the shell</p>
</li>
</ul>

<ul>
<li><p><code>api.handleServerRequest(req, res)</code></p>
<p> Start Express middleware processing wrapped in the node domain</p>
</li>
</ul>

<ul>
<li><p><code>api.handleProxyRequest(req, res, callback)</code></p>
<p> Process incoming proxy request, can be overriden for custom logic with frontend proxy server. If any
response is sent or an error returned in the calback
then the request will be aborted and will not be forwarded to the web processes</p>
</li>
</ul>

<ul>
<li><p><code>api.setupSocketConnection(socket)</code></p>
<p> Called on new socket connection, supports all type of sockets</p>
</li>
</ul>

<ul>
<li><p><code>api.cleanupSocketConnection(socket)</code></p>
<p> Called when a socket connections is closed to cleanup all additional resources associated with it</p>
</li>
</ul>

<ul>
<li><p><code>api.checkWebSocketRequest(data, callback)</code></p>
<p> Called before allowing the WebSocket connection to be authorized</p>
</li>
</ul>

<ul>
<li><p><code>api.handleWebSocketConnect(socket)</code></p>
<p> Wrap external WeSocket connection into the Express routing, respond on backend command</p>
</li>
</ul>

<ul>
<li><p><code>api.createWebSocketRequest(socket, url, reply)</code></p>
<p> Wrap WebSocket into HTTP request to be proceses by the Express routes</p>
</li>
</ul>

<ul>
<li><p><code>api.closeWebSocketRequest(socket)</code></p>
<p> Close all pending requests, this is called on socket close or disconnect</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRequest(req, res, callback)</code></p>
<p> Perform authorization of the incoming request for access and permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.checkQuery(req, res, next)</code></p>
<p> Parse incoming query parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkBody(req, res, next)</code></p>
<p> Parse multipart forms for uploaded files</p>
</li>
</ul>

<ul>
<li><p><code>api.checkAccess(req, callback)</code></p>
<p> Perform URL based access checks
Check access permissions, calls the callback with the following argument:</p>
<ul>
<li>nothing if checkSignature needs to be called</li>
<li>an object with status: 200 to skip authorization and proceed with routes processing</li>
<li>an object with status: 0 means response has been sent, just stop</li>
<li>an object with status other than 0 or 200 to return the status and stop request processing</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkAuthorization(req, status, callback)</code></p>
<p> Perform authorization checks after the account been checked for valid signature, this is called even if the signature verification failed</p>
<ul>
<li>req is Express request object</li>
<li>status contains the signature verification status, an object with status: and message: properties, can be null</li>
<li>callback is a function(status) to be called with the resulted status where status must be an object with status and message properties as well</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkSignature(req, callback)</code></p>
<p> Verify request signature from the request object, uses properties: .host, .method, .url or .originalUrl, .headers</p>
</li>
</ul>

<ul>
<li><p><code>api.parseSignature(req)</code></p>
<p> Parse incoming request for signature and return all pieces wrapped in an object, this object
will be used by verifySignature function for verification against an account
signature version:</p>
<ul>
<li>1 regular signature signed with secret for specific requests</li>
<li>2 to be sent in cookies and uses wild support for host and path
If the signature successfully recognized it is saved in the request for subsequent use as req.signature</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.setSessionSignature(req, options)</code></p>
<p> Setup session cookies for automatic authentication without signing, req must be complete with all required
properties after successful authorization.</p>
</li>
</ul>

<ul>
<li><p><code>api.initAccountAPI()</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>api.initStatusAPI()</code></p>
<p> Status/presence</p>
</li>
</ul>

<ul>
<li><p><code>api.initIconAPI()</code></p>
<p> Generic icon management</p>
</li>
</ul>

<ul>
<li><p><code>api.initFileAPI()</code></p>
<p> Generic file management</p>
</li>
</ul>

<ul>
<li><p><code>api.initMessageAPI()</code></p>
<p> Messaging management</p>
</li>
</ul>

<ul>
<li><p><code>api.initCounterAPI()</code></p>
<p> Counters management</p>
</li>
</ul>

<ul>
<li><p><code>api.initConnectionAPI()</code></p>
<p> Connections management</p>
</li>
</ul>

<ul>
<li><p><code>api.initLocationAPI()</code></p>
<p> Geo locations management</p>
</li>
</ul>

<ul>
<li><p><code>api.initPagesAPI()</code></p>
<p> API for wiki pages support</p>
</li>
</ul>

<ul>
<li><p><code>api.initSystemAPI()</code></p>
<p> API for internal provisioning and configuration</p>
</li>
</ul>

<ul>
<li><p><code>api.initDataAPI()</code></p>
<p> API for full access to all tables</p>
</li>
</ul>

<ul>
<li><p><code>api.initTables(options, callback)</code></p>
<p> Called in the master process to create/upgrade API related tables</p>
</li>
</ul>

<ul>
<li><p><code>api.getOptions(req)</code></p>
<p> Convert query options into database options, most options are the same as for <code>db.select</code> but prepended with underscore to
distinguish control parameters from query parameters.</p>
</li>
</ul>

<ul>
<li><p><code>api.getTokenSecret(req)</code></p>
<p> Return a secret to be used for enrypting tokens</p>
</li>
</ul>

<ul>
<li><p><code>api.getResultPage(req, options, rows, info)</code></p>
<p> Return an object to be returned to the client as a page of result data with possibly next token
if present in the info. This result object can be used for pagination responses.</p>
</li>
</ul>

<ul>
<li><p><code>api.getPublicColumns(table, options)</code></p>
<p> Columns that are allowed to be visible, used in select to limit number of columns to be returned by a query</p>
<ul>
<li>pub property means public column</li>
</ul>
<p>options may be used to define the following properties:</p>
<ul>
<li>columns - list of public columns to be returned, overrides the public columns in the definition list</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkPublicColumns(table, rows, options)</code></p>
<p> Process records and keep only public properties as defined in the table columns. This method is supposed to be used in the post process
callbacks after all records have been procersses and are ready to be returned to the client, the last step would be to cleanup all non public columns if necessary.</p>
<p><code>table</code> can be a single table name or a list of table names which combined public columns need to be kept in the rows. List of request tables
is kept in the <code>req.options.cleanup</code> which is by default is table name of the API endpoint, for example for /account/get it will contain bk_account, for
/connection/get - bk_connection.</p>
<p>In the <code>options</code> account object can be present to detect account own records which will not be cleaned and all properties will be returned, by default <code>id</code>
property is used to detect current account but can be specified by the <code>options.key</code> property.</p>
<p>By default primary keys are not kept and must be marked with <code>pub</code> property in the table definition to be returned.</p>
</li>
</ul>

<ul>
<li><p><code>api.describeTables(tables)</code></p>
<p> Define new tables or extend/customize existing tables. Table definitions are used with every database operation,
on startup, the backend read all existing table columns from the database and cache them in the memory but some properties
like public columns are only specific to the backend so to mark such columns the table with such properties must be described
using this method. Only columns with changed properties need to be specified, other columns will be left as it is.</p>
<p>Example</p>
<pre><code>    api.describeTables({ bk_account: { name: { pub: 1 } },

                         test: { id: { primary: 1, type: &quot;int&quot; },
                                 name: { pub: 1, index: 1 } });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.clearQuery(query, options, table, name)</code></p>
<p> Clear request query properties specified in the table definition, if any columns for the table contains the property <code>name</code> nonempty, then
all request properties with the same name as this column name will be removed from the query. This for example is used for the <code>bk_account</code>
table to disable updating location related columns because speial location API maintains location data and updates the accounts table.</p>
<p>The options can have a property in the form <code>keep_{name}</code> which will prevent from clearing the query for the name, this is for dynamic enabling/disabling
this functionality without clearing table column definitions.</p>
</li>
</ul>

<ul>
<li><p><code>api.findHook(type, method, path)</code></p>
<p> Find registered hooks for given type and path</p>
</li>
</ul>

<ul>
<li><p><code>api.addHook(type, method, path, callback)</code></p>
<p> Register a hook callback for the type and method and request url, if already exists does nothing.</p>
</li>
</ul>

<ul>
<li><p><code>api.registerAccessCheck(method, path, callback)</code></p>
<p> Register a handler to check access for any given endpoint, it works the same way as the global accessCheck function and is called before
validating the signature or session cookies.</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, cb) {}, to indicate an error condition pass an object
with the callback with status: and message: properties, status != 200 means error</li>
</ul>
<p>Example:</p>
<pre><code>    api.registerAccessCheck(&#39;&#39;, &#39;account&#39;, function(req, cb) { cb({status:500,message:&quot;access disabled&quot;}) }))

    api.registerAccessCheck(&#39;POST&#39;, &#39;/account/add&#39;, function(req, cb) {
       if (!req.query.invitecode) return cb({ status: 400, message: &quot;invitation code is required&quot; });
       cb();
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerPreProcess(method, path, callback)</code></p>
<p> Similar to <code>registerAccessCheck</code> but this callback will be called after the signature or session is verified but before
the API route method is called.</p>
<p>The purpose of this hook is to perform some preparations or check permissions of a valid user to resources or in case of error perform any other action
like redirection or returning something explaining what to do in case of failure. The callback for this call is different then in <code>checkAccess</code> hooks.</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similr to registering Express routes</li>
<li>callback is a function(req, status, cb) where status is an object { status:..., message: ..} passed from the checkSignature call, if status != 200 it means
an error condition, the callback must pass the same or modified status object in its own <code>cb</code> callback</li>
</ul>
<p>Example:</p>
<pre><code>     api.registerPreProcess(&#39;GET&#39;, &#39;/account/get&#39;, function(req, status, cb) {
          if (status.status != 200) status = { status: 302, url: &#39;/error.html&#39; };
          cb(status)
     });
</code></pre><p>Example with admin access only:</p>
<pre><code>    api.registerPreProcess(&#39;POST&#39;, &#39;/data/&#39;, function(req, status, cb) {
        if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &quot;access denied, admins only&quot; });
        cb();
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerPostProcess(method, path, callback)</code></p>
<p> Register a callback to be called after successfull API action, status 200 only.
The purpose is to perform some additional actions after the standard API completed or to customize the result</p>
<ul>
<li>method can be &#39;&#39; in such case all mathods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, res, rows) where rows is the result returned by the API handler,
the callback may not return data back to the client, in this next post process hook will be called and eventually the result will be sent back to the client.
<strong>To indicate that this hook will send the result eventually it must return true, otherwise the rows will be sent afer all hooks are called</strong></li>
</ul>
<p>Example, just update the rows, it will be sent</p>
<pre><code>    api.registerPostProcess(&#39;&#39;, &#39;/data/&#39;, function(req, res, rows) {
        rows.forEach(function(row) { ...});
    });
</code></pre><p>Example, add data to the rows</p>
<pre><code>    api.registerPostProcess(&#39;&#39;, &#39;/data/&#39;, function(req, res, row) {
        db.get(&quot;bk_account&quot;, { id: row.id }, function(err, rec) {
            row.name = rec.name;
            res.json(row);
        });
        return true;
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.sendJSON(req, err, rows)</code></p>
<p> Send result back with possibly executing post-process callback, this is used by all API handlers to allow custom post processing in the apps.
If err is not null the error message is returned immediately.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendReply(res, status, text)</code></p>
<p> Send formatted JSON reply to API client, if status is an instance of Error then error message with status 500 is sent back</p>
</li>
</ul>

<ul>
<li><p><code>api.sendStatus(res, options)</code></p>
<p> Return reply to the client using the options object, it cantains the following properties:</p>
<ul>
<li>status - defines the respone status code</li>
<li>message  - property to be sent as status line and in the body</li>
<li>type - defines Content-Type header, the message will be sent in the body</li>
<li>url - for redirects when status is 301 or 302</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendFile(req, res, file, redirect)</code></p>
<p> Send file back to the client, res is Express response object</p>
</li>
</ul>

<ul>
<li><p><code>api.subscribe(req, options)</code></p>
<p> Subscribe for events, this is used by <code>/acount/subscribe</code> API call but can be used in generic way, if no options
provided by default it will listen on req.account.id, the default API implementation for Connection, Counter, Messages publish
events using account id as a key.</p>
<ul>
<li>req is always an Express request object</li>
<li>optons may contain the following propertis:<ul>
<li>key - alternative key to subscribe for</li>
<li>timeout - how long to wait before dropping the connection, default 15 mins</li>
<li>interval - how often send notifications to the client, this allows buffering several events and notify about them at once instead triggering
 event condition every time, useful in case of very frequent events</li>
<li>match - a regexp that matched the message text, if not matched these events will be dropped</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.unsubscribe(req, options)</code></p>
<p> Disconnect from subscription service. This forces disconnect even for persistent connections like websockets.</p>
</li>
</ul>

<ul>
<li><p><code>api.publish(key, event, options)</code></p>
<p> Publish an event for an account, key is account id or other key used for subscription, event is a string or an object</p>
</li>
</ul>

<ul>
<li><p><code>api.sendEvent(req, key, data)</code></p>
<p> Process a message received from subscription server or other even notifier, it is used by <code>api.subscribe</code> method for delivery events to the clients</p>
</li>
</ul>

<ul>
<li><p><code>api.iconPath(id, options)</code></p>
<p> Full path to the icon, perform necessary hashing and sharding, id can be a number or any string</p>
</li>
</ul>

<ul>
<li><p><code>api.downloadIcon(uri, options, callback)</code></p>
<p> Download image and convert into JPG, store under core.path.images
Options may be controlled using the properties:</p>
<ul>
<li>force - force rescaling for all types even if already exists</li>
<li>id - id for the icon</li>
<li>type - type for the icon, prepended to the icon id</li>
<li>prefix - where to store all scaled icons</li>
<li>verify - check if the original icon is the same as at the source</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.saveIcon(file, id, options, callback)</code></p>
<p> Save original or just downloaded file in the proper location according to the types for given id,
this function is used after downloading new image or when moving images from other places. On success
the callback will be called with the second argument set to the output file name where the image has been saved.
Valid properties in the options:</p>
<ul>
<li>type - icon type, this will be prepended to the name of the icon</li>
<li>prefix - top level subdirectory under images/</li>
<li>force - to rescale even if it already exists</li>
<li>width, height, filter, ext, quality for backend.resizeImage function</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.scaleIcon(infile, options, callback)</code></p>
<p> Scale image using ImageMagick, return err if failed</p>
<ul>
<li>infile can be a string with file name or a Buffer with actual image data</li>
<li>options can specify image properties:<ul>
<li>outfile - if not empty is a file name where to store scaled image or if empty the new image contents will be returned in the callback as a buffer</li>
<li>width, height - new image dimensions<ul>
<li>if width or height is negative this means do not perform upscale, keep the original size if smaller than given positive value,</li>
<li>if any is 0 that means keep the original size</li>
</ul>
</li>
<li>filter - ImageMagick image filters, default is lanczos</li>
<li>quality - 0-99 percent, image scaling quality</li>
<li>ext - image format: png, gif, jpg, jp2</li>
<li>flip - flip horizontally</li>
<li>flop - flip vertically</li>
<li>blue_radius, blur_sigma - perform adaptive blur on the image</li>
<li>crop_x, crop_y, crop_width, crop_height - perform crop using given dimensions</li>
<li>sharpen_radius, sharpen_sigma - perform sharpening of the image</li>
<li>brightness - use thing to change brightness of the image</li>
<li>contrast - set new contrast of the image</li>
<li>rotate - rotation angle</li>
<li>bgcolor - color for the background, used in rotation</li>
<li>quantized - set number of colors for quantize</li>
<li>treedepth - set tree depth for quantixe process</li>
<li>dither - set 0 or 1 for quantixe and posterize processes</li>
<li>posterize - set number of color levels</li>
<li>normalize - normalize image</li>
<li>opacity - set image opacity</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.handleIconRequest(req, res, options, callback)</code></p>
<p> Process icon request, put or del, update table and deal with the actual image data, always overwrite the icon file
Verify icon limits before adding new icons</p>
</li>
</ul>

<ul>
<li><p><code>api.formatIcon(row, options)</code></p>
<p> Return formatted icon URL for the given account, verify permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.checkIcon(row, options, cols)</code></p>
<p> Verify icon permissions and format for the result, used in setProcessRow for the bk_icon table</p>
</li>
</ul>

<ul>
<li><p><code>api.selectIcon(req, options, callback)</code></p>
<p> Return list of icons for the account, used in /icon/get API call</p>
</li>
</ul>

<ul>
<li><p><code>api.getIcon(req, res, id, options)</code></p>
<p> Return icon to the client, checks the bk_icon table for existence and permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.sendIcon(req, res, id, options)</code></p>
<p> Send an icon to the client, only handles files</p>
</li>
</ul>

<ul>
<li><p><code>api.putIcon(req, id, options, callback)</code></p>
<p> Store an icon for account, .type defines icon prefix</p>
</li>
</ul>

<ul>
<li><p><code>api.storeIcon(file, id, options, callback)</code></p>
<p> Place the icon data to the destination, if api.imagesS3 or options.imagesS3 specified then plave the image on the S3 drive</p>
</li>
</ul>

<ul>
<li><p><code>api.delIcon(id, options, callback)</code></p>
<p> Delete an icon for account, .type defines icon prefix</p>
</li>
</ul>

<ul>
<li><p><code>api.getFile(req, res, file, options)</code></p>
<p> Send a file to the client</p>
</li>
</ul>

<ul>
<li><p><code>api.putFile(req, name, options, callback)</code></p>
<p> Upload file and store in the filesystem or S3, try to find the file in multipart form, in the body or query by the given name</p>
<ul>
<li>name is the name property to look for in the multipart body or in the request body or query</li>
<li>callback will be called with err and actual filename saved
Output file name is built according to the following options properties:</li>
<li>name - defines the basename for the file, no extention, if not given same name as property will be used</li>
<li>prefix - the folder prefix where the file will be uploaded, all leading folders will be created automatically</li>
<li>ext - what file extention to use, appended to name, if no ext is given the extension from the uploaded file will be used or no extention if could not determine one.</li>
<li>extkeep - tells always to keep actual extention from the uploaded file</li>
<li>encoding - encoding of the body, default is base64</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.storeFile(tmpfile, outfile, options, callback)</code></p>
<p> Place the uploaded tmpfile to the destination pointed by outfile</p>
</li>
</ul>

<ul>
<li><p><code>api.delFile(file, options, callback)</code></p>
<p> Delete file by name from the local filesystem or S3 drive if filesS3 is defined in api or options objects</p>
</li>
</ul>

<ul>
<li><p><code>api.getStatus(id, options, callback)</code></p>
<p> Returns status record for given account, used in /status/get API call.
It always returns status object even if it was never set before, on return the record contains
a property <code>online</code> set to true of false according to the idle period and actual status.</p>
<p>If id is an array, then return all status records for specified list of account ids.</p>
<p>If status was explicitely set to <code>offline</code> then it is considered offline until changed to to other value,
for other cases <code>status</code> property is not used, it is supposed for the application extention.</p>
</li>
</ul>

<ul>
<li><p><code>api.putStatus(obj, options, callback)</code></p>
<p> Maintain online status, update to db every status-interval seconds, if options.check is given only update db if last update happened
longer than status-interval seconds ago, keep atime up-to-date in the cache on every status update.
On return the row will have a property <code>saved</code> if it was flushed to db.</p>
</li>
</ul>

<ul>
<li><p><code>api.preparePages(options)</code></p>
<p> Prepare a markdown page, the following properties can be used in the options:</p>
<ul>
<li>content - the markdown contents</li>
<li>title - tile to be rendered, this will be discovered by taking first # heading if not set</li>
<li>subtitle - subtitle or short description</li>
<li>toc - if not empty, then it is replaced with the table of contents by collecting all heading tags, i.e #</li>
<li>id - id or file name, if no title is specified or discovered it will be used as a title</li>
<li>render - if true render into html, otherwise return just markdown</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendPages(req, options)</code></p>
<p> Send rendered markdown to the client response</p>
</li>
</ul>

<ul>
<li><p><code>api.incrCounter(req, options, callback)</code></p>
<p> Increase a counter, used in /counter/incr API call, options.op can be set to &#39;put&#39;</p>
</li>
</ul>

<ul>
<li><p><code>api.incrAutoCounter(id, type, num, options, callback)</code></p>
<p> Update auto counter for account and type</p>
</li>
</ul>

<ul>
<li><p><code>api.getConnection(req, options, callback)</code></p>
<p> Return all connections for the current account, this function is called by the <code>/connection/get</code> API call.</p>
</li>
</ul>

<ul>
<li><p><code>api.selectConnection(req, options, callback)</code></p>
<p> Return all connections for the current account, this function is called by the <code>/connection/select</code> API call.</p>
</li>
</ul>

<ul>
<li><p><code>api.putConnection(req, options, callback)</code></p>
<p> Create a connection between 2 accounts, this function is called by the <code>/connection/add</code> API call with query parameters coming from the Express request.</p>
</li>
</ul>

<ul>
<li><p><code>api.delConnection(req, options, callback)</code></p>
<p> Delete a connection, this function is called by the <code>/connection/del</code> API call</p>
</li>
</ul>

<ul>
<li><p><code>api.queryConnection(id, obj, options, callback)</code></p>
<p> Return all connections for the account id with optional query properties, obj.type should not include :</p>
</li>
</ul>

<ul>
<li><p><code>api.readConnection(id, obj, options, callback)</code></p>
<p> Return one connection for given id, obj must have .id and .type properties defined,
if options.details is 1 then combine with account record.</p>
</li>
</ul>

<ul>
<li><p><code>api.makeConnection(id, obj, options, callback)</code></p>
<p> Lower level connection creation with all counters support, can be used outside of the current account scope for
any two accounts and arbitrary properties, <code>id</code> is the primary account id, <code>obj</code> contains id and type for other account
with other properties to be added. <code>obj</code> is left untouched.</p>
<p>To maintain aliases for both sides of the connection, set alias in the obj for the bk_connection and options.alias for bk_reference.</p>
<p>The following properties can alter the actions:</p>
<ul>
<li>publish - send notification via pub/sub system if present</li>
<li>nocounter - do not update auto increment counters</li>
<li>noreference - do not create reference part of the connection</li>
<li>connected - return existing connection record for the same type from the other account</li>
<li>alias - an alias for the reference record for cases wen connecting 2 different accounts, it has preference over options.account.</li>
<li>account - an object with account properties like id, alias to be used in the connection/reference records, specifically options.account.alias will
be used for the reference record to show the alias of the other account, for the primary connection obj.alias is used if defined.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.deleteConnection(id, obj, options, callback)</code></p>
<p> Lower level connection deletion, for given account <code>id</code>, the other id and type is in the <code>obj</code>, performs deletion of all
connections. If any of obj.id or obj.type are not specified then perform a query for matching connections and delete only matched connection.</p>
</li>
</ul>

<ul>
<li><p><code>api.getLocation(req, options, callback)</code></p>
<p> Perform locations search, request comes from the Express server, callback will takes err and data to be returned back to the client, this function
is used in <code>/location/get</code> request. It can be used in the applications with customized input and output if neccesary for the application specific logic.</p>
<p>Example</p>
<pre><code>    # Request will look like: /recent/locations?latitude=34.1&amp;longitude=-118.1&amp;mtime=123456789
    this.app.all(/^\/recent\/locations$/, function(req, res) {
        var options = self.getOptions(req);
        options.keys = [&quot;geohash&quot;,&quot;mtime&quot;];
        options.ops = { mtime: &#39;gt&#39; };
        options.details = true;
        self.getLocations(req, options, function(err, data) {
            self.sendJSON(req, err, data);
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.putLocation(req, options, callback)</code></p>
<p> Save location coordinates for current account, this function is called by the <code>/location/put</code> API call</p>
</li>
</ul>

<ul>
<li><p><code>api.getArchiveMessage(req, options, callback)</code></p>
<p> Return archived messages, used in /message/get API call</p>
</li>
</ul>

<ul>
<li><p><code>api.getSentMessage(req, options, callback)</code></p>
<p> Return sent messages to the specified account, used in /message/get/sent API call</p>
</li>
</ul>

<ul>
<li><p><code>api.getMessage(req, options, callback)</code></p>
<p> Return new/unread messages, used in /message/get API call</p>
</li>
</ul>

<ul>
<li><p><code>api.archiveMessage(req, options, callback)</code></p>
<p> Mark a message as archived, used in /message/archive API call</p>
</li>
</ul>

<ul>
<li><p><code>api.addMessage(req, options, callback)</code></p>
<p> Add new message, used in /message/add API call</p>
</li>
</ul>

<ul>
<li><p><code>api.delMessage(req, options, callback)</code></p>
<p> Delete a message or all messages for the given account from the given sender, used in /message/del` API call</p>
</li>
</ul>

<ul>
<li><p><code>api.delArchiveMessage(req, options, callback)</code></p>
<p> Delete the messages in the archive, used in /message/del/archive` API call</p>
</li>
</ul>

<ul>
<li><p><code>api.delSentMessage(req, options, callback)</code></p>
<p> Delete the messages i sent, used in /message/del/sent` API call</p>
</li>
</ul>

<ul>
<li><p><code>api.registerOAuthStrategy(strategy, options, callback)</code></p>
<p> Given passport strategy setup OAuth callbacks and handle the login process by creating a mapping account for each
OAUTH authenticated account. The callback will be called as function(req,res) with <code>req.user</code> signifies the successful
login and hold the account properties.</p>
<p>The following options properties are accepted:</p>
<ul>
<li>cliendID,</li>
<li>clientSecret,</li>
<li>callbackURL - passport OAUTH properties</li>
<li>session - setup cookie session on success</li>
<li>successUrl - redirect url on success</li>
<li>failureUrl - redirect url on failure</li>
<li>fetchAccount - a new function to be used instead of api.fetchAccount for new account creation or mapping
 for the given authenticated profile. This is for processing or customizing new account properties and doing
 some post processing work after the account has been created.
 For any function, <code>req.profile</code>, <code>req.accessToken</code>,<code>req.refreshToken</code> will be set for the authenticated profile object from the provider.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.getAccount(req, options, callback)</code></p>
<p> Return an account, used in /account/get API call</p>
</li>
</ul>

<ul>
<li><p><code>api.notifyAccount(id, options, callback)</code></p>
<p> Send Push notification to the account, the actual transport delivery must be setup before calling this and passed in the options
as handler: property which accepts the same arguments as this function. The delivery is not guaranteed, only will be sent if the account is considered
&quot;offline&quot; according to the status and/or idle time. If the messages was queued for delivery, the row returned will contain the property sent:.
The options may contain the following:</p>
<ul>
<li>msg - message text to send</li>
<li>badge - a badge number to be sent</li>
<li>prefix - prepend the message with this prefix</li>
<li>check - check the account status, if not specified the message will be sent unconditionally otherwise only if idle</li>
<li>allow - the account property to check if notifications are enabled, must be a boolean true or number &gt; 0 to flag it is enabled, if it is an Array then
  all properties in the array are checked against the account properties and all must allow notifications. If it is an object then only the object properties and values are checked.</li>
<li>skip - Array or an object with account ids which should be skipped, this is for mass sending in order to reuse the same options</li>
<li>logging - logging level about the notification send status, default is debug, can be any valid logger level, must be a string, not a number</li>
<li>service - name of the standard delivery service supported by the backend, it is be used instead of custom handler, one of the following: apple, google</li>
<li>device_id - the device to send the message to instesd of the device_id property fro the account record</li>
</ul>
<p>In addition the device_id can be saved in the format service://id where the service is one of the supported delivery services, this way the notification
system will pick the right delivery service depending on the device id, the default service is apple.</p>
</li>
</ul>

<ul>
<li><p><code>api.listAccount(rows, options, callback)</code></p>
<p> Return account details for the list of rows, options.key specified the column to use for the account id in the <code>rows</code>, or <code>id</code> will be used.
The result accounts are cleaned for public columns, all original properties from the <code>rows</code> are kept as is.
If options.existing is 1 then return only record with found accounts, all other records in the rows will be deleted</p>
</li>
</ul>

<ul>
<li><p><code>api.selectAccount(req, options, callback)</code></p>
<p> Query accounts, used in /accout/select API call, simple wrapper around db.select but can be replaced in the apps while using the same API endpoint</p>
</li>
</ul>

<ul>
<li><p><code>api.addAccount(req, options, callback)</code></p>
<p> Register new account, used in /account/add API call</p>
</li>
</ul>

<ul>
<li><p><code>api.fetchAccount(req, options, callback)</code></p>
<p> Given a profile data from some other system, check if there is an account or create a new account for the given
profile, return bk_account record in the callback. req.query contains profile fields converted to bk_auth/bk_account names
so the whole req.query can be saved as it is. <code>req.query.login</code> must exist.</p>
<p>This method is supposed to be called after the user is authenticated and verified, it does not
check secrets but only existence of a user by login. If  user with login exists, this works as <code>api.getAccount</code>
with an extra call to bk_auth. On success the current account is active and set as <code>req.account</code>.</p>
<p>If new account ws created, the generated secret will be returned and must be saved by the client for subsequent
API calls unless cookie session is established.</p>
<p>if <code>req.query.icon&#39; is set with the url of the profile image, it will be downloaded and saved as account icon type</code>0<code>.</code>options.width`
if specified will be used to resize the image.</p>
</li>
</ul>

<ul>
<li><p><code>api.updateAccount(req, options, callback)</code></p>
<p> Update existing account, used in /account/update API call</p>
</li>
</ul>

<ul>
<li><p><code>api.setAccountSecret(req, options, callback)</code></p>
<p> Change account secret, used in /account/put/secret API call</p>
</li>
</ul>

<ul>
<li><p><code>api.deleteAccount(id, options, callback)</code></p>
<p> Delete account specified by the obj. Used in <code>/account/del</code> API call.
The options may contain keep: {} object with table names to be kept without the bk_ prefix, for example
delete an account but keep all messages and location: keep: { message: 1, location: 1 }</p>
</li>
</ul>

<ul>
<li><p><code>api.initStatistics()</code></p>
<p> Setup statistics collections</p>
</li>
</ul>

<ul>
<li><p><code>api.getStatistics(options)</code></p>
<p> Updates metrics with the current values and returns an object ready to be saved in the database, i.e. flattened ito one object
where all property names of the complex objects are combined into one name separated by comma.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendStatistics()</code></p>
<p> Send collected statistics to the collection server, <code>backend-host</code> must be configured and possibly <code>backend-login</code> and <code>backend-secret</code> in case
the system API is secured, the user can be any valid user registered in the bk_auth table.</p>
</li>
</ul>

<ul>
<li><p><code>api.saveStatistics(obj, callback)</code></p>
<p> Save collected statistics in the bk_collect table, this can be called via API or directly by the backend, this wrapper
is supposed to be overrriden by the application with additional logic how the statistics is saved. Columns in the bk_collect table
must be defined for any metrics to be saved, use api.describeTable with additional columns from the api.metrics object in additional to the default ones.</p>
<p>Example, add pool cache stats to the table</p>
<pre><code>    api.describeTable({ bk_collect: { pool_cache_rmean: { type: &quot;real&quot; },
                                      pool_cache_hmean: { type: &quot;real&quot; } });
</code></pre></li>
</ul>

<ul>
<li><p><code>api.calcStatistics(query, options, callback)</code></p>
<p> Calculate statistics for a period of time, query and options must confirm to the db.select conventions.</p>
</li>
</ul>

<h2 id="module-app">Module: APP</h2>
<ul>
<li><p><code>app</code></p>
<p> This is a skeleton module to be extended by the specific application logic. It provides all callback and hooks that are called by the core backend modules
during different phases, like initialization, shutting down, etc...</p>
<p>It should be used for custom functions and methods to be defined, the <code>app</code> module is always available.</p>
</li>
</ul>

<ul>
<li><p><code>app.configure(options, callback)</code></p>
<p> Called after all config files are loaded and command line args are parsed, home directory is set but before the db is initialized,
the primary purpose of this early call is to setup environment before connecting to the database. This is called regardless of the server
to be started and intended to initialize the common environment before the database and other subsystems are initialized.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureModule(options, callback)</code></p>
<p> Called after the core.init has been initialized successfully, this can be redefined in the applications to add additional
init steps that all processes require to have. All database pools and other confugration is ready at this point. This hook is
called regardless of what kind of server is about to start, it is always called before starting a server or shell.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMiddleware(options, callback)</code></p>
<p> This handler is called during the Express server initialization just after the security middleware.</p>
<p>NOTE: <code>options.app</code> refers to the Express instance.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureWeb(options, callback)</code></p>
<p> This handler is called after the Express server has been setup and all default API endpoints initialized but the Web server
is not ready for incoming requests yet. This handler can setup additional API endpoints, add/modify table descriptions.</p>
<p>NOTE: <code>options.app</code> refers to the Express instance</p>
</li>
</ul>

<ul>
<li><p><code>app.shutdownWeb(options, callback)</code></p>
<p> Perform shutdown sequence when a Web process is about to exit</p>
<p>NOTE: <code>options.app</code> refers to the Express instance</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMaster(options, callback)</code></p>
<p> This handler is called during the master server startup, this is the process that monitors the worker jobs and performs jobs scheduling</p>
</li>
</ul>

<ul>
<li><p><code>app.configureServer(options, callback)</code></p>
<p> This handler is called during the Web server startup, this is the master process that creates Web workers for handling Web requests, this process
interacts with the Web workers via IPC sockets between processes and relaunches them if any Web worker dies.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureWorker(options, callback)</code></p>
<p> This handler is called on job worker instance startup after the tables are intialized and it is ready to process the job</p>
</li>
</ul>

<ul>
<li><p><code>app.shutdownWorker(options, callback)</code></p>
<p> Perform last minute operations inside a worker process before exit, the callback must be called eventually which will exit the process.
This method can be overrided to implement custom worker shutdown procedure in order to finish pending tasks like network calls.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMonitor(options, callback)</code></p>
<p> This callback is called when the monitor process is ready, there is no any other code is supposed to run inside the monitor, but
in case it is needed, this is the hook to be used.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureShell(options, callback)</code></p>
<p> This callback is called by the shell process to setup additional command or to execute a command which is not
supported by the standard shell. Setting options.done to 1 will stop the shell, this is a signal that command has already
been processed.</p>
</li>
</ul>

<h2 id="module-aws">Module: AWS</h2>
<ul>
<li><p><code>aws.configure(options, callback)</code></p>
<p> Initialization of metadata</p>
</li>
</ul>

<ul>
<li><p><code>aws.configureServer(options, callback)</code></p>
<p> Execute on Web server startup</p>
</li>
</ul>

<ul>
<li><p><code>aws.readCredentials(profile, callback)</code></p>
<p> Read key and secret from the AWS SDK credentials file</p>
</li>
</ul>

<ul>
<li><p><code>aws.parseXMLResponse(err, params, callback)</code></p>
<p> Parse AWS response and try to extract error code and message, convert XML into an object.</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySign(region, service, host, method, path, body, headers)</code></p>
<p> Build version 4 signature headers</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryPrepare(action, version, obj, options)</code></p>
<p> Return a request object ready to be sent to AWS, properly formatted</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryAWS(region, endpoint, proto, path, obj, callback)</code></p>
<p> Make AWS request, return parsed response as Javascript object or null in case of error</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryEndpoint(endpoint, version, action, obj, options, callback)</code></p>
<p> AWS generic query interface</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryEC2(action, obj, options, callback)</code></p>
<p> AWS EC2 API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryELB(action, obj, options, callback)</code></p>
<p> AWS ELB API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySQS(action, queue, obj, options, callback)</code></p>
<p> AWS SQS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySNS(action, obj, options, callback)</code></p>
<p> AWS SNS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySES(action, obj, options, callback)</code></p>
<p> AWS SES API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryCFN(action, obj, options, callback)</code></p>
<p> AWS CFN API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryElastiCache(action, obj, options, callback)</code></p>
<p> AWS Elastic Cache API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryRoute53(method, path, data, options, callback)</code></p>
<p> Make a request to Route53 service</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryDDB (action, obj, options, callback)</code></p>
<p> DynamoDB requests</p>
</li>
</ul>

<ul>
<li><p><code>aws.signS3(method, bucket, path, options)</code></p>
<p> Sign S3 AWS request, returns url to be send to S3 server, options will have all updated headers to be sent as well</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryS3(bucket, path, options, callback)</code></p>
<p> S3 requests
Options may contain the following properties:</p>
<ul>
<li>method - HTTP method</li>
<li>query - query parameters for the url as an object</li>
<li>postdata - any data to be sent with POST</li>
<li>postfile - file to be uploaded to S3 bucket</li>
<li>expires - absolute time when this request is expires</li>
<li>headers - HTTP headers to be sent with request</li>
<li>file - file name where to save downloaded contents</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.s3GetFile(path, options, callback)</code></p>
<p> Retrieve a file from S3 bucket, root of the path is a bucket, path can have a protocol prepended like s3://, it will be ignored</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3PutFile(path, file, options, callback)</code></p>
<p> Upload a file to S3 bucket, <code>file</code> can be a Buffer or a file name</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3ParseUrl(url)</code></p>
<p> Parse an S3 URL and return an object with bucket and path</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2RunInstances(options, callback)</code></p>
<p> Run AWS instances, supports all native EC2 parameters with first capital letter but also accepts simple parameters in the options:</p>
<ul>
<li>min - min number of instances to run, default 1</li>
<li>max - max number of instances to run, default 1</li>
<li>imageId - AMI id, use aws.imageId if not given or options.ImageId attribute</li>
<li>instanceType - instance type, use aws.instanceType if not given or options.InstanceType attribute</li>
<li>keyName - Keypair, use aws.keyName if not given or options.KeyName attribute</li>
<li>data - user data, in clear text</li>
<li>terminate - set instance initiated shutdown behaviour to terminate</li>
<li>stop - set instance initiated shutdown behaviour to stop</li>
<li>groupId - one group id or an array with security group ids</li>
<li>ip - a static private IP adress to assign</li>
<li>publicIp - associate with a public IP address</li>
<li>file - pass contents of a file as user data, contents are read using sync method</li>
<li>waitTimeout - how long to wait in ms for instance to be runnable</li>
<li>waitDelay  - now often in ms to poll for status while waiting</li>
<li>waitRunning - if 1 then wait for instance to be in running state, this is implied also by elbName, name, elasticIp properties in the options</li>
<li>name - assign a tag to the instance as <code>Name:</code>, any occurences of %i will be replaced with the instance index</li>
<li>elbName - join elastic balancer after the startup</li>
<li>elasticIp - asociate with the given Elastic IP address after the start</li>
<li>iamProfile - IAM profile to assign for instance credentials, if not given use aws.iamProfile or options[&#39;IamInstanceProfile.Name&#39;] attribute</li>
<li>availZone - availability zone, if not given use aws.availZone or options[&#39;Placement.AvailabilityZone&#39;] attribute</li>
<li>subnetId - subnet id, if not given use aws.subnetId or options.SubnetId attribute</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2WaitForInstance(instanceId, status, options, callback)</code></p>
<p> Check an instance status and keep waiting until it is equal what we expect or timeout occured.
The <code>status</code> can be one of: pending | running | shutting-down | terminated | stopping | stopped
The options can specify the following:</p>
<ul>
<li>waitTimeout - how long to wait in ms until give up, default is 30 secs</li>
<li>waitDelay - how long in ms between polls</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2CreateTags(id, name, options, callback)</code></p>
<p> Create tags for a resource. Options may contain tags property which is an object with tag key and value</p>
<p>Example</p>
<pre><code>aws.ec2CreateTags(&quot;i-1234&quot;,&quot;My Instance&quot;, { tags: { tag2 : &quot;val2&quot;, tag3: &quot;val3&quot; } } )
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ec2AssociateAddress(instanceId, elasticIp, options, callback)</code></p>
<p> Associate an Elastic IP with an instance. Default behaviour is to reassociate if the EIP is taken.
The options can specify the following:</p>
<ul>
<li>subnetId - required for instances in VPC, allocation id will be retrieved for the given ip address automatically</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2DeregisterImage(ami_id, options, callback)</code></p>
<p> Deregister an AMI by id. If <code>options.snapshots</code> is set, then delete all snapshots for this image as well</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceMeta(path, callback)</code></p>
<p> Retrieve instance meta data</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceCredentials(callback)</code></p>
<p> Retrieve instance credentials using EC2 instance profile and setup for AWS access</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceInfo(callback)</code></p>
<p> Retrieve instance launch index from the meta data if running on AWS instance</p>
</li>
</ul>

<ul>
<li><p><code>aws.elbRegisterInstances(name, instance, options, callback)</code></p>
<p> Register an instance(s) with ELB, instance can be one id or a list of ids</p>
</li>
</ul>

<ul>
<li><p><code>aws.elbDeregisterInstances(name, instance, options, callback)</code></p>
<p> Deregister an instance(s) from ELB, instance can be one id or a list of ids</p>
</li>
</ul>

<ul>
<li><p><code>aws.sqsReceiveMessage(url, options, callback)</code></p>
<p> Receive message(s) from the SQS queue, the callback will receive a list with messages if no error.
The following options can be specified:</p>
<ul>
<li>count - how many messages to receive</li>
<li>timeout - how long to wait, this is for Long Poll</li>
<li>visibilityTimeout - the duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sqsSendMessage(url, body, options, callback)</code></p>
<p> Send a message to the SQS queue.
The options can specify the following:</p>
<ul>
<li>delay - pass as DelaySeconds parameter</li>
<li>attrs - an object with additional message attributes to send, use only string, numbers or binary values, all other types will be converted into strings</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsCreatePlatformEndpoint(token, options, callback)</code></p>
<p> Creates an endpoint for a device and mobile app on one of the supported push notification services, such as GCM and APNS.</p>
<p>The following properties can be specified in the options:</p>
<ul>
<li>appArn - an application ARN to be used for push notifications, if not passed, global <code>-sns-app-arn</code> will be used.</li>
<li>data - a user data to be associated with the endpoint arn</li>
</ul>
<p>All capitalized properties in the options will be pased as is. The callback will be called with an error if any and the endpoint ARN</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetEndpointAttributes(arn, options, callback)</code></p>
<p> Sets the attributes for an endpoint for a device on one of the supported push notification services, such as GCM and APNS.</p>
<p>The following properties can be specified in the options:</p>
<ul>
<li>token - a device token for the notification service</li>
<li>data - a user data to be associated with the endpoint arn</li>
<li>enabled - true or false to enable/disable the deliver of notifications to this endpoint</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsDeleteEndpoint(arn, options, callback)</code></p>
<p> Deletes the endpoint from Amazon SNS.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsPublish(arn, msg, options, callback)</code></p>
<p> Sends a message to all of a topic&#39;s subscribed endpoints or to a mobile endpoint.
If msg is an object, then it will be pushed as JSON.
The options may take the following properties:</p>
<ul>
<li>subject - optional subject to be included in the message if the target supports it</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsCreateTopic(name, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetTopicAttributes(arn, options, callback)</code></p>
<p> Updates the topic attributes.
The following options can be used:</p>
<ul>
<li>name - new topic name</li>
<li>policy - an object with access policy</li>
<li>deliveryPolicy - an object with delivery attributes, can specify all or only the ones that needed to be updated</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsDeleteTopic(arn, options, callback)</code></p>
<p> Deletes the topic from Amazon SNS.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSubscribe(arn, endpoint, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success, if the topic requires
confirmation the arn returned will bt null and a token will be sent to the endpoint for confirmation.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsConfirmSubscription(arn, token, options, callback)</code></p>
<p> Verifies an endpoint owner&#39;s intent to receive messages by validating the token sent to the
endpoint by an earlier Subscribe action. If the token is valid, the action creates a new subscription
and returns its Amazon Resource Name (ARN) in the callback.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetSubscriptionAttributes(arn, options, callback)</code></p>
<p> Updates the subscription attributes.
The following options can be used:</p>
<ul>
<li>name - new topic name</li>
<li>deliveryPolicy - an object with delivery attributes, can specify all or only the ones that needed to be updated</li>
<li>minDelayTarget - update delivery policy by attribute name</li>
<li>maxDelayTarget</li>
<li>numRetries</li>
<li>numMaxDelayRetries</li>
<li>backoffFunction - one of linear|arithmetic|geometric|exponential</li>
<li>maxReceivesPerSecond</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsUnsubscribe(arn, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success.</p>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendEmail(to, subject, body, options, callback)</code></p>
<p> Send an email via SES
The following options supported:</p>
<ul>
<li>from - an email to use in the From: header</li>
<li>cc - list of email to use in CC: header</li>
<li>bcc - list of emails to use in Bcc: header</li>
<li>replyTo - list of emails to ue in ReplyTo: header</li>
<li>returnPath - email where to send bounces</li>
<li>charset - charset to use, default is UTF-8</li>
<li>html - if set the body is sent as MIME HTML</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendRawEmail(body, options, callback)</code></p>
<p> Send raw email
The following options accepted:</p>
<ul>
<li>to - list of email addresses to use in RCPT TO</li>
<li>from - an email to use in from header</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.toDynamoDB(value, level)</code></p>
<p> Convert a Javascript object into DynamoDB object</p>
</li>
</ul>

<ul>
<li><p><code>aws.fromDynamoDB(value)</code></p>
<p> Convert a DynamoDB object into Javascript object</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryFilter(obj, options)</code></p>
<p> Build query or scan filter objects for the given object, all properties in the obj are used</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbListTables(options, callback)</code></p>
<p> Return list of tables in .TableNames property of the result</p>
<p>Example:</p>
<pre><code>    { TableNames: [ name, ...] }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbDescribeTable(name, options, callback)</code></p>
<p> Return table definition and parameters in the result structure with property of the given table name</p>
<p>Example:</p>
<pre><code>    { name: { AttributeDefinitions: [], KeySchema: [] ...} }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbCreateTable(name, attrs, keys, options, callback)</code></p>
<p> Create a table</p>
<ul>
<li>attrs can be an array in native DDB JSON format or an object with name:type properties, type is one of S, N, NN, NS, BS</li>
<li>keys can be an array in native DDB JSON format or an object with name:keytype properties, keytype is one of HASH or RANGE value in the same format as for primary keys</li>
<li>options may contain any valid native property if it starts with capital letter and the following:<ul>
<li>waitTimeout - number of milliseconds to wait for ACTIVE status</li>
<li>waitDelay - how often to pool for table status, default is 250ms</li>
<li>local - an object with each property for a local secondary index name defining key format the same way as for primary keys, all Uppercase properties are added to the top index object</li>
<li>global - an object for global secondary indexes, same format as for local indexes</li>
<li>projection - an object with index name and list of projected properties to be included in the index or &quot;ALL&quot; for all properties, if omitted then default KEYS_ONLY is assumed</li>
<li>readCapacity - read capacity units for provisioned throughput</li>
<li>writeCapacity - write capacity units</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> Example:</p>
<pre><code>      ddbCreateTable(&#39;users&#39;, { id:&#39;S&#39;,mtime:&#39;N&#39;,name:&#39;S&#39;},
                              { id:&#39;HASH&#39;,name:&#39;RANGE&#39;},
                              { local: { mtime: { mtime: &quot;HASH&quot; } },
                                global: { name: { name: &#39;HASH&#39;, ProvisionedThroughput: { ReadCapacityUnits: 50 } } },
                                projection: { mtime: [&#39;gender&#39;,&#39;age&#39;],
                                              name: [&#39;name&#39;,&#39;gender&#39;] },
                                readCapacity: 10,
                                writeCapacity: 10 });
</code></pre>
<ul>
<li><p><code>aws.ddbDeleteTable(name, options, callback)</code></p>
<p> Remove a table from the database</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbWaitForTable(name, item, options, callback)</code></p>
<p> Call the callback after specified period of time or when table status become different from the given waiting status.
if options.waitTimeout is not specified calls the callback immediately. options.waitStatus is checked if given and keeps waiting
while the status is equal to it. options.waitDelay can be specified how often to request new status, default is 250ms.</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbUpdateTable(options, callback)</code></p>
<p> Update tables provisioned throughput settings, options is used instead of table name so this call can be used directly in the cron jobs to adjust
provisionined throughput on demand.
Options must provide the following properties:</p>
<ul>
<li>name - table name</li>
<li>readCapacity -</li>
<li>writeCapacity - new povisioned throughtput settings</li>
</ul>
<p>Example of crontab job in etc/crontab:</p>
<pre><code>        [
        { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 0 1 * * *&quot;, &quot;job&quot;: { &quot;aws.ddbUpdateTable&quot;: { &quot;name&quot;: &quot;bk_account&quot;, &quot;readCapacity&quot;: 1000, &quot;writeCapacity&quot;: 1000 } } },
        { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 0 6 * * *&quot;, &quot;job&quot;: { &quot;aws.ddbUpdateTable&quot;: { &quot;name&quot;: &quot;bk_account&quot;, &quot;readCapacity&quot;: 2000, &quot;writeCapacity&quot;: 2000 } } }
        ]
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbPutItem(name, item, options, callback)</code></p>
<p> Put or add an item</p>
<ul>
<li>item is an object, type will be inferred from the native js type.</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>expected - an object with column names to be used in Expected clause and value as null to set condition to { Exists: false } or<pre><code>any other exact value to be checked against which corresponds to { Exists: true, Value: value }
</code></pre></li>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used<pre><code>for ExpressionAttributeValues parameters
</code></pre></li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used<pre><code>for ExpressionAttributeNames parameter
</code></pre></li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbPutItem(&quot;users&quot;, { id: 1, name: &quot;john&quot;, mtime: 11233434 }, { expected: { name: null } })
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbUpdateItem(name, keys, item, options, callback)</code></p>
<p> Update an item</p>
<ul>
<li>keys is an object with primary key attributes name and value.</li>
<li>item is an object with properties where value can be:<ul>
<li>number/string/array - action PUT, replace or add new value</li>
<li>null/empty string - action DELETE</li>
</ul>
</li>
<li>item can be a string with Update expression</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
<li>ops - an object with operators to be used for properties if other than PUT</li>
<li>expected - an object with column names to be used in Expected clause and value as null to set condition to { Exists: false } or
  any other exact value to be checked against which corresponds to { Exists: true, Value: value }. If it is an object then it is treated as
  { op: value } and options.ops is ignored otherwise the conditional comparison operator is taken from options.ops the same way as for queries.</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39; }, { op: { icons: &#39;ADD&#39; }, expected: { id: 1 }, ReturnValues: &quot;ALL_NEW&quot; })
    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39; }, { op: { icons: &#39;ADD&#39; }, expected: { id: null } })
    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39;, num: 1 }, { op: { num: &#39;ADD&#39;, icons: &#39;ADD&#39; }, expected: { id: null, num: { gt: 0 } } })
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbDeleteItem(name, keys, options, callback)</code></p>
<p> Delete an item from a table</p>
<ul>
<li>keys is an object with name: value for hash/range attributes</li>
<li>options may contain any valid native property if it starts with capital letter and the following special options:<ul>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbDeleteItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, {})
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbBatchWriteItem(items, options, callback)</code></p>
<p> Update items from the list at the same time</p>
<ul>
<li>items is a list of objects with table name as property and list of operations, an operation can be PutRequest or DeleteRequest</li>
<li>options may contain any valid native property if it starts with capital letter.</li>
</ul>
<p>Example:</p>
<pre><code>    { table: [ { PutRequest: { id: 1, name: &quot;tt&quot; } }, ] }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbBatchGetItem(items, options, callback)</code></p>
<p> Retrieve all items for given list of keys</p>
<ul>
<li>items is an object with table name as property name and list of options for GetItem request</li>
<li>options may contain any valid native property if it starts with capital letter.</li>
</ul>
<p>Example:</p>
<pre><code>    { users: { keys: [{ id: 1, name: &quot;john&quot; },{ id: .., name: .. }], select: [&#39;name&#39;,&#39;id&#39;], consistent: true }, ... }
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbGetItem(name, keys, options, callback)</code></p>
<p> Retrieve one item by primary key</p>
<ul>
<li>keys - an object with primary key attributes name and value.</li>
<li>select - list of columns to return, otherwise all columns will be returned</li>
<li><p>options may contain any native property allowed in the request or special properties:</p>
<ul>
<li>consistent - set consistency level for the request</li>
<li>projection - projection expression</li>
<li><p>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter
Example:</p>
<p> ddbGetItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39; })</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbQueryTable(name, condition, options, callback)</code></p>
<p> Query on a table, return all matching items</p>
<ul>
<li>condition is an object with name: value pairs, by default EQ opeartor is used for comparison</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key when paginating, can be a string/number for hash or an object with hash/range properties</li>
<li>consistent - set consistency level for the request</li>
<li>select - list of attributes to get only</li>
<li>total - return number of matching records</li>
<li>count - limit number of record in result</li>
<li>desc - descending order</li>
<li>sort - index name to use, indexes are named the same as the corresponding column, with index primary keys for Keycondition will be used</li>
<li>ops - an object with operators to be used for properties if other than EQ.</li>
<li>keys - list of primary key columns, if there are other properties in the condition then they will be<pre><code>   put into QueryFilter instead of KeyConditions. If keys is absent, all properties in the condition are treated as primary keys.
</code></pre></li>
<li>projection - projection expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
<li>expr - filtering expression</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    aws.ddbQueryTable(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39;, ops: { name: &#39;gt&#39; } })
    aws.ddbQueryTable(&quot;users&quot;, { id: 1, name: &quot;john&quot;, status: &quot;ok&quot; }, { keys: [&quot;id&quot;], select: &#39;id,name&#39;, ops: { name: &#39;gt&#39; } })
    aws.ddbQueryTable(&quot;users&quot;, { id: 1 }, { expr: &quot;status=:s&quot;, values: { s: &quot;status&quot; } })
</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbScanTable(name, condition, options, callback)</code></p>
<p> Scan a table for all matching items</p>
<ul>
<li>condition is an object with name: value pairs or a string with FilterExpression</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key</li>
<li>ops - an object with operators to be used for properties if other than EQ.</li>
<li>projection - projection expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    aws.ddbScanTable(&quot;users&quot;, { id: 1, name: &#39;a&#39; }, { ops: { name: &#39;gt&#39; }})
    aws.ddbScanTable(&quot;users&quot;, &quot;id=:id AND name=:name&quot;, { values: { id: 1, name: &#39;a&#39; } });
</code></pre></li>
</ul>

<h2 id="module-core">Module: CORE</h2>
<ul>
<li><p><code>core</code></p>
<p> The primary object containing all config options and common functions</p>
</li>
</ul>

<ul>
<li><p><code>core.init(options, callback)</code></p>
<p> Main initialization, must be called prior to perform any actions.
If options are given they may contain the following properties:</p>
<ul>
<li>noPools - if true do not initialize database pools except default sqlite</li>
<li>noDns - do not retrieve config from DNS</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.noop()</code></p>
<p> Empty function to be used when callback was no provided</p>
</li>
</ul>

<ul>
<li><p><code>core.run(options, callback)</code></p>
<p> Run any backend function after environment has been initialized, this is to be used in shell scripts,
core.init will parse all command line arguments, the simplest case to run from /data directory and it will use
default environment or pass -home dir so the script will reuse same config and paths as the server
context can be specified for the callback, if no then it run in the core context</p>
<ul>
<li>require(&#39;backendjs&#39;).run(function() {}) is one example where this call is used as a shortcut for ad-hoc scripting</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.exit(code, msg)</code></p>
<p> Exit the process with possible message to be displayed and status code</p>
</li>
</ul>

<ul>
<li><p><code>core.setHome(home)</code></p>
<p> Switch to new home directory, exit if we cannot, this is important for relative paths to work if used,
no need to do this in worker because we already switched to home directory in the master and all child processes
inherit current directory
Important note: If run with combined server or as a daemon then this MUST be an absolute path, otherwise calling
it in the spawned web master will fail due to the fact that we already set the home and relative path will not work after that.</p>
</li>
</ul>

<ul>
<li><p><code>core.parseConfig(data)</code></p>
<p> Parse config lines for the file or other place</p>
</li>
</ul>

<ul>
<li><p><code>core.parseArgs(argv)</code></p>
<p> Parse command line arguments</p>
</li>
</ul>

<ul>
<li><p><code>core.processArgs(name, ctx, argv, pass)</code></p>
<p> Config parameters defined in a module as a list of parameter names prefixed with module name, a parameters can be
a string which defines text parameter or an object with the properties: name, type, value, decimals, min, max, separator
type can be bool, number, list, json</p>
</li>
</ul>

<ul>
<li><p><code>core.describeArgs(module, args)</code></p>
<p> Add custom config parameters to be understood and processed by the config parser</p>
<ul>
<li>module - name of the module to add these params to, if it is an empty string or skipped then the module where any
parameter goes is determined by the prefix, for example if name is &#39;aws-elastic-ip&#39; then it will be added to the aws module,
all not matched parameters will be added to the core module.</li>
<li>args - a list of objects in the format: { name: N, type: T, descr: D, min: M, max: M, array: B }, all except name are optional.</li>
</ul>
<p>Example:</p>
<pre><code>core.describeArgs(&quot;api&quot;, [ { name: &quot;num&quot;, type: &quot;int&quot;, descr: &quot;int param&quot; }, { name: &quot;list&quot;, array: 1, descr: &quot;list of words&quot; } ]);
core.describeArgs([ { name: &quot;api-list&quot;, array: 1, descr: &quot;list of words&quot; } ]);
</code></pre></li>
</ul>

<ul>
<li><p><code>core.showHelp(options)</code></p>
<p> Print help about command line arguments and exit</p>
</li>
</ul>

<ul>
<li><p><code>core.loadConfig(file, callback)</code></p>
<p> Parse the config file, configFile can point to a file or can be skipped and the default file will be loaded</p>
</li>
</ul>

<ul>
<li><p><code>core.loadDnsConfig(options, callback)</code></p>
<p> Load configuration from the DNS TXT records</p>
</li>
</ul>

<ul>
<li><p><code>core.encodeURIComponent(str)</code></p>
<p> Encode with additional symbols, convert these into percent encoded:</p>
<pre><code>    ! -&gt; %21, * -&gt; %2A, &#39; -&gt; %27, ( -&gt; %28, ) -&gt; %29
</code></pre></li>
</ul>

<ul>
<li><p><code>core.processName()</code></p>
<p> Return unique process name based on the cluster status, worker or master and the role. This is can be reused by other workers within the role thus
making it usable for repeating environments or storage solutions.</p>
</li>
</ul>

<ul>
<li><p><code>core.toTitle(name)</code></p>
<p> Convert text into capitalized words</p>
</li>
</ul>

<ul>
<li><p><code>core.toCamel(name)</code></p>
<p> Convert into camelized form</p>
</li>
</ul>

<ul>
<li><p><code>core.toUncamel(str)</code></p>
<p> Convert Camel names into names with dashes</p>
</li>
</ul>

<ul>
<li><p><code>core.toNumber(str, decimals, dflt, min, max)</code></p>
<p> Safe version, use 0 instead of NaN, handle booleans, if decimals specified, returns float</p>
</li>
</ul>

<ul>
<li><p><code>core.toBool(val, dflt)</code></p>
<p> Return true if value represents true condition</p>
</li>
</ul>

<ul>
<li><p><code>core.toDate(val, dflt)</code></p>
<p> Return Date object for given text or numeric date representation, for invalid date returns 1969</p>
</li>
</ul>

<ul>
<li><p><code>core.toValue(val, type)</code></p>
<p> Convert value to the proper type</p>
</li>
</ul>

<ul>
<li><p><code>core.toRegexpMap(obj, val, del)</code></p>
<p> Add a regexp to the object that consist of list of patterns and compiled regexp, this is used in config type <code>regexpmap</code></p>
</li>
</ul>

<ul>
<li><p><code>core.isNumeric(type)</code></p>
<p> Returns true if the given type belongs to the numeric family</p>
</li>
</ul>

<ul>
<li><p><code>core.isTrue(val1, val2, op, type)</code></p>
<p> Evaluate expr, compare 2 values with optional type and operation</p>
</li>
</ul>

<ul>
<li><p><code>core.createServer(options, callback)</code></p>
<p> Create a Web server with options and request handler, returns a server object.
Options can have the following properties:</p>
<ul>
<li>port - port number is required</li>
<li>bind - address to bind</li>
<li>restart - name of the processes to restart on address in use error, usually &quot;web&quot;</li>
<li>ssl - an object with SSL options for TLS createServer call</li>
<li>timeout - number of milliseconds for the request timeout</li>
<li>name - server name to be assigned</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.httpGet(uri, params, callback)</code></p>
<p> Downloads file using HTTP and pass it to the callback if provided</p>
<ul>
<li>uri can be full URL or an object with parts of the url, same format as in url.format</li>
<li>params can contain the following options:<ul>
<li>method - GET, POST</li>
<li>headers - object with headers to pass to HTTP request, properties must be all lower case</li>
<li>cookies - a list with cookies or a boolean to load cookies from the db</li>
<li>file - file name where to save response, in case of error response the error body will be saved as well</li>
<li>postdata - data to be sent with the request in the body</li>
<li>postfile - file to be uploaded in the POST body, not as multipart</li>
<li>query - additional query parameters to be added to the url as an object or as encoded string</li>
<li>sign - sign request with provided email/secret properties</li>
</ul>
</li>
<li>callback will be called with the arguments:
 first argument is error object if any
 second is params object itself with updated fields
 third is HTTP response object
On end, the object params will contain the following updated properties:</li>
<li>data if file was not specified, data will contain collected response body as string</li>
<li>status - HTTP response status code</li>
<li>mtime - Date object with the last modified time of the requested file</li>
<li>size - size of the response body or file
Note: SIDE EFFECT: params object is modified in place so many options will be changed/removed or added</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.signUrl(login, secret, host, uri, options)</code></p>
<p> Produce signed URL to be used in embedded cases or with expiration so the url can be passed and be valid for longer time.
Host passed here must be the actual host where the request will be sent</p>
</li>
</ul>

<ul>
<li><p><code>core.signRequest(login, secret, method, host, uri, options)</code></p>
<p> Sign HTTP request for the API server:
url must include all query parameters already encoded and ready to be sent
options may contains the following:</p>
<ul>
<li>expires is absolute time in milliseconds when this request will expire, default is 30 seconds from now</li>
<li>version a version number defining how the signature will be signed</li>
<li>type - content-type header, may be omitted</li>
<li>tag - a custom tag, vendor specific, opaque to the bkjs, can be used for passing additional account or session inforamtion</li>
<li>checksum - SHA1 digest of the whole content body, may be omitted</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.sendRequest(options, callback)</code></p>
<p> Make a request to the backend endpoint, save data in the queue in case of error, if data specified,
POST request is made, if data is an object, it is converted into string.
Returns params as in httpGet with .json property assigned with an object from parsed JSON response.
<em>When used with API endpoints, the <code>backend-host</code> parameter must be set in the config or command line to the base URL of the backend,
like <a href="http://localhost:8000">http://localhost:8000</a>, this is when <code>uri</code> is relative URL. Absolute URLs do not need this parameter.</em>
Special parameters for options:</p>
<ul>
<li>url - url if options is first argument</li>
<li>login - login to use for access credentials instead of global credentials</li>
<li>secret - secret to use for access instead of global credentials</li>
<li>proxy - used as a proxy to backend, handles all errors and returns .status and .json to be passed back to API client</li>
<li>checksum - calculate checksum from the data</li>
<li>anystatus - keep any HTTP status, dont treat as error if not 200</li>
<li>obj - return just result object, not the whole params</li>
<li>queue - perform queue management, save in the bk_queue if cannot send right now, delete from bk_queue if sent</li>
<li>etime - when this request expires, for queue management</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.deferCallback(obj, msg, callback, timeout)</code></p>
<p> Register the callback to be run later for the given message, the message must have id property which will be used for keeping track of the replies.
A timeout is created for this message, if runCallback for this message will not be called in time the timeout handler will call the callback
anyways with the original message.
The callback passed will be called with only one argument which is the message, what is inside the message this function does not care. If
any errors must be passed, use the message object for it, no other arguments are expected.</p>
</li>
</ul>

<ul>
<li><p><code>core.runMethods(name, options, callback)</code></p>
<p> Run a method for every module, a method must conform to the following signature: <code>function(options, callback)</code> and
call the callback when finished. The callback second argument will be the options, so it is possible to pass anything
in the options back to the caller. Errors from a module is never propagated and simply ignored.</p>
</li>
</ul>

<ul>
<li><p><code>core.runCallback(obj, msg)</code></p>
<p> Run delayed callback for the message previously registered with the <code>deferCallback</code> method.
The message must have id property which is used to find the corresponding callback, if msg is a JSON string it will be converted into the object.</p>
</li>
</ul>

<ul>
<li><p><code>core.forEach(list, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array in parallel. Execute a callback when all items
have been completed or immediately if there is an error provided.</p>
<pre><code>    core.forEach([ 1, 2, 3 ], function (i, next) {
        console.log(i);
        next();
    }, function (err) {
        console.log(&#39;done&#39;);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>core.forEachSeries(list, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array serially. Execute a callback when all items
have been completed or immediately if there is is an error provided.</p>
<pre><code>    core.forEachSeries([ 1, 2, 3 ], function (i, next) {
      console.log(i);
      next();
    }, function (err) {
      console.log(&#39;done&#39;);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>core.forEachLimit(list, limit, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array in parallel as many as specified in <code>limit</code> at a time. Execute a callback when all items
have been completed or immediately if there is is an error provided.</p>
</li>
</ul>

<ul>
<li><p><code>core.parallel(tasks, callback)</code></p>
<p> Execute a list of functions in parellel and execute a callback upon completion or occurance of an error. Each function will be passed
a callback to signal completion. The callback accepts an error for the first argument. The iterator and callback will be
called via setImmediate function to allow the main loop to process I/O.</p>
</li>
</ul>

<ul>
<li><p><code>core.series(tasks, callback)</code></p>
<p> Execute a list of functions serially and execute a callback upon completion or occurance of an error. Each function will be passed
a callback to signal completion. The callback accepts either an error for the first argument. The iterator and callback will be
called via setImmediate function to allow the main loop to process I/O.</p>
<pre><code>    core.series([
       function(next) {
          setTimeout(function () { next(); }, 100);
       },
       function(next) {
          setTimeout(function () { next(); }, 100);
       },
    ], function(err) {
        console.log(err);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>core.whilst(test, iterator, callback)</code></p>
<p> While the test function returns true keep running the iterator, call the callback at the end if specified. All functions are called via setImmediate.</p>
<pre><code>    var count = 0;
    core.whilst(function() { return count &lt; 5; },
                function (callback) {
                    count++;
                    setTimeout(callback, 1000);
                }, function (err) {
                    console.log(count);
                });
</code></pre></li>
</ul>

<ul>
<li><p><code>core.doWhilst(iterator, test, callback)</code></p>
<p> Keep running iterator while the test function returns true, call the callback at the end if specified. All functions are called via setImmediate.</p>
</li>
</ul>

<ul>
<li><p><code>core.addModule()</code></p>
<p> Adds reference to the objects in the core for further access, specify module name, module reference pairs.
This is used the the core itself to register all internal modules and makes it available in the shell and in the <code>core.modules</code> object.</p>
<p>Also this is used when cresting modular backend application by separating the logic into different modules, by registering such
modules with the core it makes the module a first class citizen in the backendjs core and exposes all the callbacks and methods.</p>
<p>For example, the module below will register API routes and some methods</p>
<pre><code> var bkjs = require(&quot;backendjs&quot;);
 var mymod = {}
 exports.module = mymod;
 core.addModule(&quot;mymod&quot;, mymod);
 mymod.configureWeb = function(options, callback) {
    bkjs.api.app.all(&quot;/mymod&quot;, function(req, res) {
         res.json({});
    });
 }
</code></pre></li>
</ul>
<p> In the main app.js just load it and the rest will be done automatically, i.e. routes will be created ...</p>
<pre><code>   var mymod = require(&quot;./mymod.js&quot;);
</code></pre><p> Running the shell will make the object <code>mymod</code> available</p>
<pre><code>   ./app.sh -shell
   &gt; mymod
     {}
</code></pre>
<ul>
<li><p><code>core.loadModules(name, options, callback)</code></p>
<p> Dynamically load services from the specified directory. The modules are loaded using <code>require</code> as normal node module but in addition if the module exports
<code>init</code> method it is called immediately with options passed as an argument. This is a synchronous function so it is supposed to be
called on startup, not dynamically during a request processing. Only top level .js files are loaded, not subdirectories. <code>core.addModule</code> is called
automatically.</p>
<p>Example, to load all modules from the local relative directory</p>
<pre><code> core.loadModules(&quot;modules&quot;)
</code></pre></li>
</ul>

<ul>
<li><p><code>core.createPool(options)</code></p>
<p> Create a resource pool, create and close callbacks must be given which perform allocation and deallocation of the resources like db connections.
Options defines the following properties:</p>
<ul>
<li>create - method to be called to return a new resource item, takes 1 argument, a callback as function(err, item)</li>
<li>destroy - method to be called to destroy a resource item</li>
<li>validate - method to verify actibe resource item, return false if it needs to be destroyed</li>
<li>min - min number of active resource items</li>
<li>max - max number of active resource items</li>
<li>max_queue - how big the waiting queue can be, above this all requests will be rejected immediately</li>
<li>timeout - number of milliseconds to wait for the next available resource item, cannot be 0</li>
<li>idle - number of milliseconds before starting to destroy all active resources above the minimum, 0 to disable.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.getArg(name, dflt)</code></p>
<p> Return commandline argument value by name</p>
</li>
</ul>

<ul>
<li><p><code>core.getArgInt(name, dflt)</code></p>
<p> Return commandline argument value as a number</p>
</li>
</ul>

<ul>
<li><p><code>core.isArg(name)</code></p>
<p> Returns true of given arg(s) are present in the command line, name can be a string or an array of strings.</p>
</li>
</ul>

<ul>
<li><p><code>core.sendmail(options, callback)</code></p>
<p> Send email</p>
</li>
</ul>

<ul>
<li><p><code>core.forEachLine(file, options, lineCallback, endCallback)</code></p>
<p> Call callback for each line in the file
options may specify the following parameters:</p>
<ul>
<li>sync - read file synchronously and call callback for every line</li>
<li>abort - signal to stop processing</li>
<li>limit - number of lines to process and exit</li>
<li>progress - if &gt; 0 report how many lines processed so far every specified lines</li>
<li>until - skip lines until this regexp matches</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.geoHash(latitude, longitude, options)</code></p>
<p> Return object with geohash for given coordinates to be used for location search
options may contain the following properties:</p>
<ul>
<li>distance - limit the range key with the closest range smaller than then distance, required for search but for updates may be omitted</li>
<li>minDistance - radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
 this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table
 if not specified default <code>min-distance</code> value will be used.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.geoDistance(latitude1, longitude1, latitude2, longitude2, options)</code></p>
<p> Return distance between two locations, options can specify the following properties:</p>
<ul>
<li>round - a number how to round the distance</li>
</ul>
<p>Example: round to the nearest full 5 km and use only 1 decimal point, if the distance is 13, it will be 15.0</p>
<pre><code>core.geoDistance(34, -188, 34.4, -119, { round: 5.1 })
</code></pre></li>
</ul>

<ul>
<li><p><code>core.geoHashDistance(geohash1, geohash2, options)</code></p>
<p> Same as geoDistance but operates on 2 geohashes instead of coordinates.</p>
</li>
</ul>

<ul>
<li><p><code>core.encrypt(key, data, algorithm, encoding)</code></p>
<p> Encrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>core.decrypt(key, data, algorithm, encoding)</code></p>
<p> Decrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>core.sign (key, data, algorithm, encode)</code></p>
<p> HMAC signing and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>core.hash (data, algorithm, encode)</code></p>
<p> Hash and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>core.uuid()</code></p>
<p> Return unique Id without any special characters and in lower case</p>
</li>
</ul>

<ul>
<li><p><code>core.random(size)</code></p>
<p> Generate random key, size if specified defines how many random bits to generate</p>
</li>
</ul>

<ul>
<li><p><code>core.randomUShort()</code></p>
<p> Return random number between 0 and USHORT_MAX</p>
</li>
</ul>

<ul>
<li><p><code>core.randomShort()</code></p>
<p> Return random number between 0 and SHORT_MAX</p>
</li>
</ul>

<ul>
<li><p><code>core.randomUInt()</code></p>
<p> Return rando number between 0 and UINT_MAX</p>
</li>
</ul>

<ul>
<li><p><code>core.randomInt(min, max)</code></p>
<p> Return random integer between min and max inclusive</p>
</li>
</ul>

<ul>
<li><p><code>core.randomNum(min, max, decs)</code></p>
<p> Generates a random number between given min and max (required)
Optional third parameter indicates the number of decimal points to return:</p>
<ul>
<li>If it is not given or is NaN, random number is unmodified</li>
<li>If &gt;0, then that many decimal points are returned (e.g., &quot;2&quot; -&gt; 12.52</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.now()</code></p>
<p> Return number of seconds for current time</p>
</li>
</ul>

<ul>
<li><p><code>core.strftime(date, fmt, utc)</code></p>
<p> Format date object</p>
</li>
</ul>

<ul>
<li><p><code>core.strSplit(str, sep, num)</code></p>
<p> Split string into array, ignore empty items, <code>sep</code> is an RegExp to use as a separator instead of default  pattern <code>[,\|]</code>, if num is 1, then convert all items into numbers</p>
</li>
</ul>

<ul>
<li><p><code>core.strSplitUnique(str, sep, num)</code></p>
<p> Split as above but keep only unique items</p>
</li>
</ul>

<ul>
<li><p><code>core.arrayUnique(list, key)</code></p>
<p> Returns only unique items in the array, optional <code>key</code> specified the name of the column to use when determining uniqueness if items are objects.</p>
</li>
</ul>

<ul>
<li><p><code>core.jsonToBase64(data, secret)</code></p>
<p> Stringify JSON into base64 string, if secret is given, sign the data with it</p>
</li>
</ul>

<ul>
<li><p><code>core.base64ToJson(data, secret)</code></p>
<p> Parse base64 JSON into JavaScript object, in some cases this can be just a number then it is passed as it is, if secret is given verify
that data is not chnaged and was signed with the same secret</p>
</li>
</ul>

<ul>
<li><p><code>core.parseLocalAddress(str)</code></p>
<p> Given a string with list of urls try to find if any points to our local server using IP address or host name, returns the url
in format: protocol://*:port, mostly to be used with nanomsg sockets</p>
</li>
</ul>

<ul>
<li><p><code>core.moveFile(src, dst, overwrite, callback)</code></p>
<p> Copy file and then remove the source, do not overwrite existing file</p>
</li>
</ul>

<ul>
<li><p><code>core.copyFile(src, dst, overwrite, callback)</code></p>
<p> Copy file, overwrite is optional flag, by default do not overwrite</p>
</li>
</ul>

<ul>
<li><p><code>core.execProcess(cmd, callback)</code></p>
<p> Run the process and return all output to the callback, this a simply wrapper around child_processes.exec so the core.runProcess
can be used without importing the child_processes module. All fatal errors are logged.</p>
</li>
</ul>

<ul>
<li><p><code>core.spawnProcess(cmd, args, options, callback)</code></p>
<p> Run specified command with the optional arguments, this is similar to child_process.spawn with callback being called after the process exited</p>
<p>Example</p>
<pre><code>    core.spawProcess(&quot;ls&quot;, &quot;-ls&quot;, { cwd: &quot;/tmp&quot; }, db.showResult)
</code></pre></li>
</ul>

<ul>
<li><p><code>core.spawnSeries(cmds, options, callback)</code></p>
<p> Run a series of commands, <code>cmds</code> is an object where a property name is a command to execute and the value is an array of arguments or null.
if <code>options.error</code> is 1, then stop on first error or if non-zero status on a process exit.</p>
<p>Example:</p>
<pre><code>    core.spawnSeries({&quot;ls&quot;: &quot;-la&quot;,
                      &quot;ps&quot;: &quot;augx&quot;,
                      &quot;du&quot;: { argv: &quot;-sh&quot;, stdio: &quot;inherit&quot;, cwd: &quot;/tmp&quot; },
                      &quot;uname&quot;: [&quot;-a&quot;] },
                     db.showResult)
</code></pre></li>
</ul>

<ul>
<li><p><code>core.killBackend(name, signal, callback)</code></p>
<p> Kill all backend processes that match name and not the current process</p>
</li>
</ul>

<ul>
<li><p><code>core.shutdown()</code></p>
<p> Shutdown the machine now</p>
</li>
</ul>

<ul>
<li><p><code>core.statSync(file)</code></p>
<p> Non-exception version, returns empty object,
mtime is 0 in case file does not exist or number of seconds of last modified time
mdate is a Date object with last modified time</p>
</li>
</ul>

<ul>
<li><p><code>core.readFileSync(file, options)</code></p>
<p> Return contents of a file, empty if not exist or on error.
Options can specify the format:</p>
<ul>
<li>json - parse file as JSON, return an object, in case of error an empty object</li>
<li>list - split contents with the given separator</li>
<li>encoding - file encoding when converting to string</li>
<li>logger - if 1 log all errors</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.findFilter(file, stat, options)</code></p>
<p> Filter function to be used in findFile methods</p>
</li>
</ul>

<ul>
<li><p><code>core.findFileSync(file, options)</code></p>
<p> Return list of files than match filter recursively starting with given path, file is the starting path.
The options may contain the following:</p>
<ul>
<li>include - a regexp with file pattern to include</li>
<li>exclude - a regexp with file pattern to exclude</li>
<li>filter - a function(file, stat) that return 1 if the given file matches, stat is a object returned by fs.statSync</li>
<li>depth - if a number it specifies max depth to go into the subfolders, starts with 1</li>
<li>types - a string with types of files to include: d - a dir, f - a file, l - a symlink, c - char dev, b - block dev, s - socket, p - a FIFO</li>
<li>base - if set only keep base file name in the result, not full path</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.findFile(dir, options, callback)</code></p>
<p> Async version of find file, same options as in the sync version</p>
</li>
</ul>

<ul>
<li><p><code>core.makePathSync(dir)</code></p>
<p> Recursively create all directories, return 1 if created or 0 on error or if exists, no exceptions are raised, error is logged only</p>
</li>
</ul>

<ul>
<li><p><code>core.makePath(dir, callback)</code></p>
<p> Async version of makePath, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>core.unlinkPath(dir, callback)</code></p>
<p> Recursively remove all files and folders in the given path, returns an error to the callback if any</p>
</li>
</ul>

<ul>
<li><p><code>core.unlinkPathSync(dir)</code></p>
<p> Recursively remove all files and folders in the given path, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>core.mkdirSync()</code></p>
<p> Create a directories if do not exist, multiple dirs can be specified</p>
</li>
</ul>

<ul>
<li><p><code>core.chownSync()</code></p>
<p> Change file owner, multiples files can be specified, do not report errors about non existent files, the uid/gid must be set to non-root user
for this function to work and it is called by the root only</p>
</li>
</ul>

<ul>
<li><p><code>core.dropPrivileges()</code></p>
<p> Drop root privileges and switch to regular user</p>
</li>
</ul>

<ul>
<li><p><code>core.setTimeout(name, callback, timeout)</code></p>
<p> Set or reset a timer</p>
</li>
</ul>

<ul>
<li><p><code>core.domainName(host)</code></p>
<p> Extract domain from local host name</p>
</li>
</ul>

<ul>
<li><p><code>core.typeName(v)</code></p>
<p> Return object type, try to detect any distinguished type</p>
</li>
</ul>

<ul>
<li><p><code>core.isEmpty(val)</code></p>
<p> Return true of the given value considered empty</p>
</li>
</ul>

<ul>
<li><p><code>core.exists(obj, name)</code></p>
<p> Return true if a variable or property in the object exists, just a syntax sugar</p>
</li>
</ul>

<ul>
<li><p><code>core.cloneObj()</code></p>
<p> A copy of an object, this is a shallow copy, only arrays and objects are created but all other types are just referenced in the new object</p>
<ul>
<li>first argument is the object to clone, can be null</li>
<li>all additional arguments are treated as name value pairs and added to the cloned object as additional properties
Example:<pre><code>  core.cloneObj({ 1: 2 }, &quot;3&quot;, 3, &quot;4&quot;, 4)
</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.newError(options)</code></p>
<p> Return a new Error object, options can be a string which will create an error with a message only
or an object with message, code, status, and name properties to build full error</p>
</li>
</ul>

<ul>
<li><p><code>core.newObj()</code></p>
<p> Return new object using arguments as name value pairs for new object properties</p>
</li>
</ul>

<ul>
<li><p><code>core.mergeObj(obj, options)</code></p>
<p> Merge an object with the options, all properties in the options override existing in the object, returns a new object</p>
<p>Example</p>
<pre><code> var o = core.mergeObject({ a:1, b:2, c:3 }, { c:5, d:1 })
 o = { a:1, b:2, c:5, d:1 }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.flattenObj(obj, options)</code></p>
<p> Flatten a javascript object into a single-depth object, all nested values will have property names appended separated by comma</p>
<p>Example</p>
<pre><code>    &gt; core.flattenObj({ a: { c: 1 }, b: { d: 1 } } )
    { &#39;a.c&#39;: 1, &#39;b.d&#39;: 1 }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.extendObj()</code></p>
<p> Add properties to existing object, first arg is the object, the rest are pairs: name, value,....
If the second argument is an object then add all properties from this object only.</p>
<pre><code>   core.extendObj({ a: 1 }, &#39;b&#39;, 2, &#39;c&#39; 3 )
   core.extendObj({ a: 1 }, { b: 2, c: 3 })
</code></pre></li>
</ul>

<ul>
<li><p><code>core.delObj()</code></p>
<p> Delete properties from the object, first arg is an object, the rest are properties to be deleted</p>
</li>
</ul>

<ul>
<li><p><code>core.searchObj(obj, options)</code></p>
<p> Return an object consisting of properties that matched given criteria in the given object.
optins can define the following properties:</p>
<ul>
<li>name - search by property name, return all objects that contain given property</li>
<li>value - search by value, return all objects that have a property with given value</li>
<li>sort if true then sort found columns by the property value.</li>
<li>names - if true just return list of column names</li>
<li>flag - if true, return object with all properties set to flag value</li>
</ul>
<p>Example</p>
<pre><code>    core.searchObj({id:{index:1},name:{index:3},type:{index:2},descr:{}}, { name: &#39;index&#39;, sort: 1 });
    { id: { index: 1 }, type: { index: 2 }, name: { index: 3 } }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.objGet(obj, name, options)</code></p>
<p> Return a property from the object, name specifies the path to the property, if the required property belong to another object inside the top one
the name uses . to separate objects. This is a convenient method to extract properties from nested objects easily.
Options may contains the following properties:</p>
<ul>
<li>list - return the value as a list even if there is only one value found</li>
<li>obj - return the value as an object, if the result is a simple type, wrap into an object like { name: name, value: result }</li>
<li>str - return the value as a string, convert any other type into string</li>
<li>num - return the value as a number, convert any other type by using toNumber</li>
</ul>
<p>Example:</p>
<pre><code>    &gt; core.objGet({ response: { item : { id: 123, name: &quot;Test&quot; } } }, &quot;response.item.name&quot;)
    &quot;Test&quot;
    &gt; core.objGet({ response: { item : { id: 123, name: &quot;Test&quot; } } }, &quot;response.item.name&quot;, { list: 1 })
    [ &quot;Test&quot; ]
</code></pre></li>
</ul>

<ul>
<li><p><code>core.objSet(obj, name, value, options)</code></p>
<p> Set a property of the object, name can be an array or a string with property path inside the object, all non existent intermediate
objects will be create automatically. The options can have the folowing properties:</p>
<ul>
<li>incr - if 1 the numeric value will be added to the existing if any</li>
<li>push - add to the array, if it is not an array a new empty aray is created</li>
</ul>
<p>Example</p>
<pre><code>    var a = core.objSet({}, &quot;response.item.count&quot;, 1)
    core.objSet(a, &quot;response.item.count&quot;, 1, { incr: 1 })
</code></pre></li>
</ul>

<ul>
<li><p><code>core.stringify(obj, filter)</code></p>
<p> JSON stringify without exceptions, on error just returns an empty string and logs the error</p>
</li>
</ul>

<ul>
<li><p><code>core.jsonParse(obj, options)</code></p>
<p> Silent JSON parse, returns null on error, no exceptions raised.
options can specify the output in case of an error:</p>
<ul>
<li>list - return empty list</li>
<li>obj - return empty obj</li>
<li>str - return empty string</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.cookieGet(domain, callback)</code></p>
<p> Return cookies that match given domain</p>
</li>
</ul>

<ul>
<li><p><code>core.cookieSave(cookiejar, setcookies, hostname, callback)</code></p>
<p> Save new cookies arrived in the request,
merge with existing cookies from the jar which is a list of cookies before the request</p>
</li>
</ul>

<ul>
<li><p><code>core.profiler(type, cmd)</code></p>
<p> Start/stop CPU V8 profiler, on stop, core.cpuProfile will contain the profiler nodes</p>
</li>
</ul>

<ul>
<li><p><code>core.createRepl(options)</code></p>
<p> Create REPL interface with all modules available</p>
</li>
</ul>

<ul>
<li><p><code>core.watchTmp(dir, options, callback)</code></p>
<p> Watch temp files and remove files that are older than given number of seconds since now, remove only files that match pattern if given
Options properties:</p>
<ul>
<li>match - a regexp that specifies only files to be watched</li>
<li>ignore - a regexp of files to be ignored</li>
<li>seconds - number of seconds a file to be older to be deleted</li>
<li>nodirs - if 1 skip deleting directories</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.watchFiles(dir, pattern, callback)</code></p>
<p> Watch files in a dir for changes and call the callback</p>
</li>
</ul>

<ul>
<li><p><code>core.watchLogs(options, callback)</code></p>
<p> Watch log files for errors and report via email or POST url, see config parameters starting with <code>logwatcher-</code> about how this works</p>
</li>
</ul>

<ul>
<li><p><code>core.checkTest()</code></p>
<p> To be used in the tests, this function takes the following arguments
checkTest(next, err, failure, ....)</p>
<ul>
<li>next is a callback to be called after printing error condition if any, it takes err as its argument</li>
<li>err - is the error object passed by the most recent operation</li>
<li>failure - must be true for failed test, the condition is evaluated by the caller and this is the result of it</li>
<li>all other arguments are printed in case of error or result being false</li>
</ul>
<p>NOTE: In forever mode (-test-forever) any error is ignored and not reported</p>
<p>Example</p>
<pre><code>    function(next) {
        db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, function(err, row) {
            core.checkTest(next, err, row &amp;&amp; row.id == &quot;123&quot;, &quot;Record not found&quot;, row)
        });
    }
</code></pre></li>
</ul>

<ul>
<li><p><code>core.runTest(obj, options, callback)</code></p>
<p> Run the test function which is defined in the object, all arguments will be taken from the command line.
The common command line arguments that supported:</p>
<ul>
<li>-test-cmd - name of the function to run</li>
<li>-test-workers - number of workers to run the test at the same time</li>
<li>-test-delay - number of milliseconds before starting worker processes, default is 500ms</li>
<li>-test-timeout - number of milliseconds between test steps, i.e. between invokations of the checkTest</li>
<li>-test-iterations - how many times to run this test function, default is 1</li>
<li>-test-forever - run forever without reporting any errors, for performance testing</li>
</ul>
<p>All common command line arguments can be used, like -db-pool to specify which db to use.</p>
<p>After finish or in case of error the process exits, so this is not supposded tobe run inside the
production backend, only as standalone utility for running unit tests</p>
<p>Example:</p>
<pre><code>    var bk = require(&quot;backendjs&quot;), core = bk.core, db = bk.db;
    var tests = {
        test1: function(next) {
            db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, function(err, row) {
                core.checkTest(next, err, row &amp;&amp; row.id == &quot;123&quot;, &quot;Record not found&quot;, row)
            });
        },
        ...
    }
    bk.run(function() { core.runTest(tests); });

    # node tests.js -test-cmd test1
</code></pre></li>
</ul>

<h2 id="module-db">Module: DB</h2>
<ul>
<li><p><code>db</code></p>
<p> The Database API, a thin abstraction layer on top of SQLite, PostgreSQL, DynamoDB and Cassandra.
The idea is not to introduce new abstraction layer on top of all databases but to make
the API usable for common use cases. On the source code level access to all databases will be possible using
this API but any specific usage like SQL queries syntax or data types available only for some databases will not be
unified or automatically converted but passed to the database directly. Only conversion between JavaScript types and
database types is unified to some degree meaning JavaScript data type will be converted into the corresponding
data type supported by any particular database and vice versa.</p>
<p>Basic operations are supported for all database and modelled after NoSQL usage, this means no SQL joins are supported
by the API, only single table access. SQL joins can be passed as SQL statements directly to the database using low level db.query
API call, all high level operations like add/put/del perform SQL generation for single table on the fly.</p>
<p>The common convention is to pass options object with flags that are common for all drivers along with specific,
this options object can be modified with new properties but all driver should try not to
modify or delete existing properties, so the same options object can be reused in subsequent operations.</p>
<p>All queries and update operations ignore properties that starts with underscore.</p>
<p>Before the DB functions can be used the <code>core.init</code> MUST be called first, the typical usage:</p>
<pre><code>    var backend = require(&quot;backendjs&quot;), core = backend.core, db = backend.db;
    core.init(function(err) {
        db.add(...
        ...
    });
</code></pre><p>All database methods can use default db pool or any other available db pool by using <code>pool: name</code> in the options. If not specified,
then default db pool is used, sqlite is default if no -db-pool config parameter specified in the command line or the config file.</p>
<p>Example, use PostgreSQL db pool to get a record and update the current pool</p>
<pre><code>    db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, { pool: &quot;pgsql&quot; }, function(err, row) {
        if (row) db.update(&quot;bk_account&quot;, row);
    });
</code></pre><p>Most database pools can be configured with options <code>min</code> and <code>max</code> for number of connections to be maintained, so no overload will happen and keep warm connection for
faster responses. Even for DynamoDB which uses HTTPS this can be configured without hitting provisioned limits which will return an error but
put extra requests into the waiting queue and execute once some requests finished.</p>
<p>Example:</p>
<pre><code>    db-pgsql-pool-max = 100
    db-dynamodb-pool-max = 100
</code></pre><p>Also, to spread functionality between different databases it is possible to assign some tables to the specific pools using <code>db-X-pool-tables</code> parameters
thus redirecting the requests to one or another databases depending on the table, this for example can be useful when using fast but expensive
database like DynamoDB for real-time requests and slower SQL database running on some slow instance for rare requests, reports or statistics processing.</p>
<p>Example, run the backend with default PostgreSQL database but keep all config parametrs in the DynamoDB table for availability:</p>
<pre><code>    db-pool = pgsql
    db-dynamodb-pool = default
    db-dynamodb-pool-tables = bk_config
</code></pre></li>
</ul>
<p> The following databases are supported with the basic db API methods: Sqlite, PostgreSQL, MySQL, DynamoDB, MongoDB, Cassandra, Redis, LMDB, LevelDB</p>
<p> All these drivers fully support all methods and operations, some natively, some with emulation in the user space except Redis driver cannot perform sorting
 due to using Hash items for records, sorting can be done in memory but with pagination it is not possible so this part must be mentioned specifically. But the rest of the
 opertions on top of Redis are fully supported which makes it a good candidate to use for in-memory tables like sessions with the same database API, later moving to
 other database will not require any application code changes.</p>
<p> Multiple connections of the same tipy can be opened, just add -n suffix to all database config parameters where n is 1 to <code>count</code> property in the config descriptor.</p>
<p> Example:</p>
<pre><code>      db-pgsql-pool = postgresql://locahost/backend
      db-pgsql-pool-1 = postgresql://localhost/billing
      db-pgsql-pool-max-1 = 100
</code></pre>
<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Configuration store, same parameters as in the commandline or config file, can be placed in separate config groups
      // to be used by different backends or workers, &#39;core&#39; is default global group
      bk_config: { name: { primary: 1 },                      // name of the parameter
                   type: { primary: 1 },                      // config type
                   value: {},                                 // the value
                   mtime: { type: &quot;bigint&quot;, now: 1 }
      },

      // General purpose properties, can be used to store arbitrary values
      bk_property: { name: { primary: 1 },
                     value: {},
                     mtime: { type: &quot;bigint&quot;, now: 1 }
      },

      // Pending jobs or other requests to be processed
      bk_queue: { id: { type: &quot;uuid&quot;, primary: 1 },
                  tag: {},
                  type: {},
                  job: { type: &quot;json&quot; },
                  args: { type: &quot;json&quot; },
                  stime: { type: &quot;bigint&quot; },                        // time when valid for processing
                  etime: { type: &quot;bigint&quot; },                        // expiration time
                  ctime: { type: &quot;bigint&quot;, readonly: 1, now: 1 },   // creation time
                  mtime: { type: &quot;bigint&quot;, now: 1 }
      },
</code></pre></li>
</ul>

<ul>
<li><p><code>db.init(options, callback)</code></p>
<p> Initialize database pools.
Options can have the following properties:</p>
<ul>
<li>noPools - disables all other pools except sqlite, similar to <code>-db-no-pools</code> config parameter, id db-local is configured and
  different than sqlite it is initialized always as well</li>
<li>noInitTables - if defined it is used instesd of the global parameter</li>
<li>noCacheColumns - if defined it is used instead fo the global parameter</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.initConfig(options, callback)</code></p>
<p> Load configuration from the config database, must be configured with <code>db-config-type</code> pointing to the database pool where bk_config table contains
configuration parameters.</p>
<p>The priority of the paramaters is fixed and goes form the most broad to the most specific, most specific always wins, this allows
for very flexible configuration policies defined by the app or place where instances running and separated by the run mode.</p>
<p>The following list of properties will be queried from the config database and the sorting order is very important, the last values
will override values received for the eqrlier properties, for example, if two properties defined in the <code>bk_config</code> table with the
types <code>myapp</code> and <code>prod-myapp</code>, then the last value will be used only.</p>
<p>All attributes will be added multiple times in the following order, <code>name</code> being the attribute listed below:
  <code>name</code>, runMode-<code>name</code>, appName-<code>name</code>, runMode-appName-<code>name</code></p>
<p>The priority of the attributes is the following:</p>
<ul>
<li>the run mode specified in the command line <code>-run-mode</code>: <code>prod</code></li>
<li>the application name: <code>myapp</code></li>
<li>the application version specified in the package.json: <code>1.0.0</code></li>
<li>the network where the instance is running, first 2 octets from the current IP address: <code>192.168</code></li>
<li>the region where the instance is running, AWS region or other name: <code>us-east-1</code></li>
<li>the network where the instance is running, first 2 octets from the current IP address: <code>192.168.1</code></li>
<li>the zone where the instance is running, AWS availability zone or other name: <code>us-east-1a</code></li>
<li>current instance tag or a custom tag for ad-hoc queries: <code>nat</code></li>
<li>current instance IP address: <code>192.168.1.1</code></li>
</ul>
<p>On return, the callback second argument will receive all parameters received form the database as a list: -name value ...</p>
</li>
</ul>

<ul>
<li><p><code>db.initTables(tables, options, callback)</code></p>
<p> Create tables in all pools</p>
</li>
</ul>

<ul>
<li><p><code>db.initPoolTables(name, tables, options, callback)</code></p>
<p> Init the pool, create tables and columns:</p>
<ul>
<li>name - db pool to create the tables in</li>
<li>tables - an object with list of tables to create or upgrade</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.shutdownWeb(optios, callback)</code></p>
<p> Gracefully close all database pools when the shutdown is initiated by a Web process</p>
</li>
</ul>

<ul>
<li><p><code>db.dropPoolTables(name, tables, options, callback)</code></p>
<p> Delete all specified tables from the pool, if <code>name</code> is empty then default pool will be used, <code>tables</code> is an object with table names as
properties, same table definition format as for create table method</p>
</li>
</ul>

<ul>
<li><p><code>db.getPoolTables(name)</code></p>
<p> Return all tables know to the given pool, returned tables are in the object with
column information merged from cached columns from the database with description columns
given by the application. Property fake: 1 in any column signifies not a real column but
a column described by the application and not yet created by the database driver or could not be added
due to some error.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPools()</code></p>
<p> Return a list of all active database pools, returns list of objects with name: and type: properties</p>
</li>
</ul>

<ul>
<li><p><code>db.createPool(options)</code></p>
<p> Create a new database pool with default methods and properties</p>
<ul>
<li>options - an object with default pool properties<ul>
<li>type - pool type, this is the db driver name</li>
<li>pool or name - pool name</li>
<li>pooling - create generic pool for connection caching</li>
<li>watchfile - file path to be watched for changes, all clients will be destroyed gracefully</li>
<li>min - min number of open database connections</li>
<li>max - max number of open database connections, all attempts to run more will result in clients waiting for the next available db connection, if set to Infinity no<pre><code>  pooling will be enabled and result in unlimited connections, this is default for DynamoDB
</code></pre></li>
<li>max_queue - how many db requests can be in the waiting queue, above that all requests will be denied instead of putting in the waiting queue
The following pool callback can be assigned to the pool object:</li>
</ul>
</li>
<li>connect - a callback to be called when actual database client needs to be created, the callback signature is
function(pool, callback) and will be called with first arg an error object and second arg is the database instance, required for pooling</li>
<li>close - a callback to be called when a db connection needs to be closed, optional callback with error can be provided to this method</li>
<li>bindValue - a callback function(val, info) that returns the value to be used in binding, mostly for SQL drivers, on input value and col info are passed, this callback
may convert the val into something different depending on the DB driver requirements, like timestamp as string into milliseconds</li>
<li>convertError - a callback function(table, op, err, options) that converts native DB driver error into other human readable format</li>
<li>resolveTable - a callback function(op, table, obj, options) that returns poosible different table at the time of the query, it is called by the <code>db.prepare</code> method
and if exist it must return the same or new table name for the given query parameters.</li>
</ul>
<p>The db methods cover most use cases but in case native driver needs to be used this is how to get the client and use it with its native API,
it is required to call <code>pool.free</code> at the end to return the connection back to the connection pool.</p>
<pre><code>    var pool = db.getPool(&quot;&quot;, { pool: &quot;mongodb&quot; });
    pool.get(function(err, client) {
        var collection = client.collection(&#39;bk_account&#39;);
        collection.findOne({ id: &#39;123&#39; }, function() {
            pool.free(client);
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.showResult(err, rows, info)</code></p>
<p> Convenient helper to show results from the database requests, can be used as the callback in all db method.</p>
<p>Example:</p>
<pre><code>    db.select(&quot;bk_account&quot;, {}, db.showResult);
</code></pre></li>
</ul>

<ul>
<li><p><code>db.query(req, options, callback)</code></p>
<p> Execute query using native database driver, the query is passed directly to the driver.</p>
<ul>
<li>req - can be a string or an object with the following properties:<ul>
<li>text - SQL statement or other query in the format of the native driver, can be a list of statements</li>
<li>values - parameter values for SQL bindings or other driver specific data</li>
<li>op - operations to be performed, used by non-SQL drivers</li>
<li>obj - actual object with data for non-SQL drivers</li>
<li>table - table name for the operation</li>
</ul>
</li>
<li>options may have the following properties:<ul>
<li>pool - name of the database pool where to execute this query.
The difference with the high level functions that take a table name as their firt argument, this function must use pool
explicitely if it is different from the default. Other functions can resolve
the pool by table name if some tables are assigned to any specific pool by configuration parameters <code>db-pool-tables</code>.</li>
<li>unique - perform sorting the result and eliminate any duplicate rows by the column name specified in the <code>unique</code> property</li>
<li>filter - function to filter rows not to be included in the result, return false to skip row, args are: function(row, options)</li>
<li>async_filter - perform filtering of the result but with possible I/O so it can delay returning results: function(rows, options, callback),
   the calback on result will return err and rows as any other regular database callbacks. This filter can be used to perform
   filtering based on the ata in the other table for example.</li>
<li>silence_error - do not report about the error in the log, still the error is retirned to the caller</li>
<li>noprocessrows - if true then skip post processing result rows, return the data as is, this will result in returning combined
columns as is</li>
<li>noconvertrows - if true skip converting the data from the database format into Javascript data types</li>
<li>cached - if true perform cache invalidation for the operations that resulted in modification of the table record(s)</li>
<li>total - if true then it is supposed to return only one record with property <code>count</code>, skip all post processing and convertion</li>
</ul>
</li>
<li>callback(err, rows, info) where<ul>
<li>info is an object with information about the last query: inserted_oid,affected_rows,next_token</li>
<li>rows is always returned as a list, even in case of error it is an empty list</li>
</ul>
</li>
</ul>
<p>Example with SQL driver</p>
<pre><code>    db.query({ text: &quot;SELECT a.id,c.type FROM bk_account a,bk_connection c WHERE a.id=c.id and a.id=?&quot;, values: [&#39;123&#39;] }, { pool: &#39;pgsql&#39; }, function(err, rows, info) {
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.add(table, obj, options, callback)</code></p>
<p> Insert new object into the database</p>
<ul>
<li>obj - an JavaScript object with properties for the record, primary key properties must be supplied</li>
<li>options may contain the following properties:<ul>
<li>all_columns - do not check for actual columns defined in the pool tables and add all properties from the obj, only will work for NoSQL dbs,
by default all properties in the obj not described in the table definition for the given table will be ignored.</li>
<li>skip_columns - ignore properties by name listed in the this array</li>
<li>mtime - if set, mtime column will be added automatically with the current timestamp, if mtime is a
string then it is used as a name of the column instead of default mtime name</li>
<li>skip_null - if set, all null values will be skipped, otherwise will be written into the DB as NULLs</li>
</ul>
</li>
</ul>
<p>On return the <code>obj</code> will contain all new columns generated before adding the record</p>
<p>Example</p>
<pre><code> db.add(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.put(table, obj, options, callback)</code></p>
<p> Add/update an object in the database, if object already exists it will be replaced with all new properties from the obj</p>
<ul>
<li>obj - an object with record properties, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.put(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.update(table, obj, options, callback)</code></p>
<p> Update existing object in the database.</p>
<ul>
<li>obj - is an actual record to be updated, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method with the following additional properties:<ul>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
</ul>
</li>
</ul>
<p>Example</p>
<pre><code>    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });

    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39;, prefix: &#39;Mr&#39; }, { pool: pgsql&#39; }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.updateAll(table, query, obj, options, callback)</code></p>
<p> Update all records that match given condition in the <code>query</code>, one by one, the input is the same as for <code>db.select</code> and every record
returned will be updated using <code>db.update</code> call by the primary key, so make sure options.select include the primary key for every row found by the select.</p>
<p>All properties from the <code>obj</code> will be set in every matched record.</p>
<p>The callback will receive on completion the err and all rows found and updated. This is mostly for non-SQL databases and for very large range it may take a long time
to finish due to sequential update every record one by one.
Special properties that can be in the options for this call:</p>
<ul>
<li>concurrency - how many update queries to execute at the same time, default is 1, this is done by using core.forEachLimit.</li>
<li>process - a function callback that will be called for each row before updating it, this is for some transformations of the record properties
 in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
 as <code>options.process(row, options)</code></li>
</ul>
<p>Example, update birthday format if not null</p>
<pre><code>    db.updateAll(&quot;bk_account&quot;,
                { birthday: 1 },
                { mtime: Date.now() },
                { ops: { birthday: &quot;not null&quot; },
                  concurrency: 2,
                  process: function(r, o) {
                        r.birthday = core.strftime(new Date(r.birthday, &quot;%Y-%m-D&quot;));
                  } },
                  function(err, rows) {
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.incr(table, obj, options, callback)</code></p>
<p> Counter operation, increase or decrease column values, similar to update but all specified columns except primary
key will be incremented, use negative value to decrease the value.</p>
<p><em>Note: The record must exist already for SQL databases, for DynamoDB and Cassandra a new record will be created
if does not exist yet.</em></p>
<p>Example</p>
<pre><code> db.incr(&quot;bk_counter&quot;, { id: &#39;123&#39;, like0: 1, invite0: 1 }, function(err, rows, info) {
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.del(table, obj, options, callback)</code></p>
<p> Delete an object in the database, no error if the object does not exist</p>
<ul>
<li>obj - an object with primary key properties only, other properties will be ignored</li>
<li>options - same properties as for <code>db.update</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.del(&quot;bk_account&quot;, { id: &#39;123&#39; }, function(err, rows, info) {
     console.log(&#39;updated:&#39;, info.affected_rows);
 });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.delAll(table, query, options, callback)</code></p>
<p> Delete all records that match given condition, one by one, the input is the same as for <code>db.select</code> and every record
returned will be deleted using <code>db.del</code> call. The callback will receive on completion the err and all rows found and deleted.
Special properties that can be in the options for this call:</p>
<ul>
<li>concurrency - how many delete requests to execute at the same time by using core.forEachLimit.</li>
<li>process - a function callback that will be called for each row before deleting it, this is for some transformations of the record properties
in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
as <code>options.process(row, options)</code></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.replace(table, obj, options, callback)</code></p>
<p> Add/update the object, check existence by the primary key. This is not equivalent of REPLACE INTO, it does <code>db.get</code>
to check if the object exists in the database and performs <code>db.add</code> or <code>db.update</code> depending on the existence.</p>
<ul>
<li>obj is a JavaScript object with properties that correspond to the table columns</li>
<li>options define additional flags that may<ul>
<li>check_mtime - defines a column name to be used for checking modification time and skip if not modified, must be a date value</li>
<li>check_data - verify every value in the given object with actual value in the database and skip update if the record is the same,
if it is an array then check only specified columns</li>
</ul>
</li>
</ul>
<p>Example: updates record 123 only if gender is not &#39;m&#39; or adds new record</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, { check_data: true });
</code></pre><p>Example: updates record 123 only if mtime of the record is less or equal yesterday</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, mtime: Date.now() - 86400000 }, { check_mtime: &#39;mtime&#39; });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.list(table, query, options, callback)</code></p>
<p> Convenient helper to retrieve all records by primary key, the obj must be a list with key property or a string with list of primary key column
Example</p>
<pre><code>db.list(&quot;bk_account&quot;, [&quot;id1&quot;, &quot;id2&quot;], function(err, rows) { console.log(err, rows) });
db.list(&quot;bk_account&quot;, &quot;id1,id2&quot;, function(err, rows) { console.log(err, rows) });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.batch(table, op, objs, options, callback)</code></p>
<p> Perform a batch of operations at the same time.</p>
<ul>
<li>op - is one of add, put, update, del</li>
<li>objs a list of objects to put/delete from the database</li>
<li>options can have the follwoing:<ul>
<li>concurrency - number of how many operations to run at the same time, 1 means sequential</li>
<li>ignore_error - will run all operations without stopping on error, the callback will have third argument which is an array of arrays with failed operations</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    db.batch(&quot;bc_counter&quot;, &quot;add&quot;, [{id:1&quot;,like0:1}, {id:&quot;2&quot;,like0:2}], db.showResult)
</code></pre></li>
</ul>

<ul>
<li><p><code>db.scan(table, query, options, rowCallback, endCallback)</code></p>
<p> Convenient helper for scanning a table for some processing, rows are retrieved in batches and passed to the callback until there are no more
records matching given criteria. The obj is the same as passed to the <code>db.select</code> method which defined a condition which records to get.
The rowCallback must be present and is called for every row or batch retrieved and second parameter which is the function to be called
once the processing is complete. At the end, the callback will be called just with 1 argument, err, this indicates end of scan operation.
Basically, db.scan is the same as db.select but can be used to retrieve large number of records in batches and allows async processing of such records.</p>
<p>Parameters:</p>
<ul>
<li>table - table to scan</li>
<li>query - an object with query conditions, same as in <code>db.select</code></li>
<li>options - same as in <code>db.select</code>, with the following additions:<ul>
<li>count - size of every batch, default is 100</li>
<li>batch - if true rowCallback will be called with all rows from the batch, not every row individually, batch size is defined by the count property</li>
<li>noprocessrows - default is 1 to pass raw records for processing, to work with normal records pass 0 to disable default behaviour</li>
</ul>
</li>
<li>rowCallback - process records when called like this `callback(rows, next)</li>
<li>endCallback - end of scan when called like this: `callback(err)</li>
</ul>
<p>Example:</p>
<pre><code>    db.scan(&quot;bk_account&quot;, {}, { count: 10, pool: &quot;dynamodb&quot; }, function(row, next) {
        // Copy all accounts from one db into another
        db.add(&quot;bk_account&quot;, row, { pool: &quot;pgsql&quot; }, next);
    }, function(err) { });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.migrate(table, options, callback)</code></p>
<p> Migrate a table via temporary table, copies all records into a temp table, then re-create the table with up-to-date definitions and copies all records back into the new table.
The following options can be used:</p>
<ul>
<li>preprocess - a callback function(row, options, next) that is called for every row on the original table, next must be called to move to the next row, if err is returned as first arg then the processing will stop</li>
<li>postprocess - a callback function(row, options, next) that is called for every row on the destination table, same rules as for preprocess</li>
<li>tmppool - the db pool to be used for temporary table</li>
<li>tpmdrop - if 1 then the temporary table willbe dropped at the end in case of success, by default it is kept</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.search(table, query, options, callback)</code></p>
<p> Perform full text search on the given table, the database implementation may ignore table name completely
in case of global text index.
Query is in general a text string with the format that is supported by the underlying driver, bkjs does not parse the query at all.
Options make take the same properties as in the select method. Without full text support
this works the same way as the <code>select</code> method.</p>
</li>
</ul>

<ul>
<li><p><code>db.join(table, rows, options, callback)</code></p>
<p> Join the given list of records with the records from other table by primary key.
The properties from the joined table will be merged with the original rows preserving the existing properties</p>
<ul>
<li>options.keys defines custom primary key to use instead of table&#39;s primary key</li>
<li>options.existing is 1 then return only joined records.</li>
<li>options.override - joined table properties will replace existing ones</li>
</ul>
<p>Example:</p>
<pre><code>    db.join(&quot;bk_account&quot;, [{id:&quot;123&quot;,key1:1},{id:&quot;234&quot;,key1:2}], db.showResult)
</code></pre></li>
</ul>

<ul>
<li><p><code>db.getLocations(table, query, options, callback)</code></p>
<p> Geo locations search, paginate all results until the end.
table must be defined with the following required columns:</p>
<ul>
<li>geohash - location as primary key hash column</li>
<li>id or other column name to be used as a RANGE key for DynamoDB/Cassandra or part of the compsoite primary key for SQL, the result will be sorted by this column for all databases</li>
<li>latitude and longitude as floating numbers to store the actual location</li>
</ul>
<p>When defining the table for location searches the begining of the table must be defined as the following:</p>
<pre><code>    api.describeTables({
            geo: { geohash: { primary: 1 },
                   id: { primary: 1 },
                   latitude: { type: &quot;real&quot; },
                   longitude: { type: &quot;real&quot; },
            }
    });
</code></pre><p>the rest of the columns can be defined as needed, no special requirements.</p>
<p><code>obj</code> must contain the following:</p>
<ul>
<li>latitude</li>
<li>longitude</li>
</ul>
<p>other properties:</p>
<ul>
<li>distance - in km, the radius around the point, in not given the <code>min-distance</code> will be used</li>
</ul>
<p>all other properties will be used as additional conditions</p>
<p><code>options</code> optional properties:</p>
<ul>
<li>top - number of first &#39;top&#39;th records from each neighboring area, to be used with sorting by the range key to take
 only highest/lowest matches, useful for trending/statistics, count still defines the total number of locations</li>
<li>geokey - name of the geohash primary key column, by default it is <code>geohash</code>, it is possible to keep several different
 geohash indexes within the same table with different geohash length which will allow to perform
 searches more precisely dependgin on the distance given</li>
<li>round - a number that defines the &quot;precision&quot; of  the distance, it rounds the distance to the nearest
round number and uses decimal point of the round number to limit decimals in the distance</li>
<li>sort - sorting order, by default the RANGE key is used for DynamoDB, it is possible to specify any Index as well,
in case of SQL this is the second part of the primary key</li>
</ul>
<p>On first call, query must contain latitude and longitude of the center and optionally distance for the radius. On subsequent calls options must be the
the next_token returned by the previous call and query will be ignored</p>
<p>On return, the callback&#39;s third argument contains the object with next_token that must be provided for subsequent searches until rows array is empty.</p>
<p>Example</p>
<pre><code>    var query = { latitude: -118, longitude: 30, distance: 10 };
    db.getLocations(&quot;bk_location&quot;, query, { round: 5 }, function(err, rows, info) {
        ...
        // Get next page using previous info object
        db.getLocations(&quot;bk_location&quot;, query, info.next_token, function(err, rows, info) {
            ...
        });
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.select(table, query, options, callback)</code></p>
<p> Select objects from the database that match supplied conditions.</p>
<ul>
<li>query - can be an object with properties for the condition, all matching records will be returned</li>
<li>query - can be a list where each item is an object with primary key condition. Only records specified in the list must be returned.</li>
<li>options can use the following special properties:<ul>
<li>ops - operators to use for comparison for properties, an object with column name and operator. The follwoing operators are available:
 <code>&gt;, gt, &lt;, lt, =, !=, &lt;&gt;, &gt;=, ge, &lt;=, le, in, between, regexp, iregexp, begins_with, like%, ilike%</code></li>
<li>opsMap - operator mapping between supplied operators and actual operators supported by the db</li>
<li>typesMap - type mapping between supplied and actual column types, an object</li>
<li>select - a list of columns or expressions to return or all columns if not specified</li>
<li>start - start records with this primary key, this is the next_token passed by the previous query</li>
<li>count - how many records to return</li>
<li>sort - sort by this column. <em>NOTE: for DynamoDB this may affect the results if columns requsted are not projected in the index, with sort
   <code>select</code> property might be used to get all required properties.</em></li>
<li>desc - if sorting, do in descending order</li>
<li>page - starting page number for pagination, uses count to find actual record to start</li>
<li>unique - specified the column name to be used in determinint unique records, if for some reasons there are multiple record in the location
  table for the same id only one instance will be returned</li>
</ul>
</li>
</ul>
<p>On return, the callback can check third argument which is an object with some predefined properties along with driver specific properties returned by the query:</p>
<ul>
<li>affected_rows - how many records this operation affected, for add/put/update</li>
<li>inserted_oid - last created auto generated id</li>
<li>next_token - next primary key or offset for pagination by passing it as .start property in the options, if null it means there are no more pages availabe for this query</li>
</ul>
<p>Example: get by primary key, refer above for default table definitions</p>
<pre><code>  db.select(&quot;bk_message&quot;, { id: &#39;123&#39; }, { count: 2 }, function(err, rows) {

  });
</code></pre><p>Example: get all icons with type greater or equal to 2</p>
<pre><code>  db.select(&quot;bk_icon&quot;, { id: &#39;123&#39;, type: &#39;2&#39; }, { select: &#39;id,type&#39;, ops: { type: &#39;ge&#39; } }, function(err, rows) {

  });
</code></pre><p>Example: get unread msgs sorted by time, recent first</p>
<pre><code>  db.select(&quot;bk_message&quot;, { id: &#39;123&#39;, status: &#39;N:&#39; }, { sort: &quot;status&quot;, desc: 1, ops: { status: &quot;begins_with&quot; } }, function(err, rows) {

  });
</code></pre><p>Example: allow all accounts icons to be visible</p>
<pre><code>  db.select(&quot;bk_account&quot;, {}, function(err, rows) {
      rows.forEach(function(row) {
          row.acl_allow = &#39;auth&#39;;
          db.update(&quot;bk_icon&quot;, row);
      });
  });
</code></pre><p>Example: scan accounts with custom filter, not by primary key: all females</p>
<pre><code>  db.select(&quot;bk_account&quot;, { gender: &#39;f&#39; }, function(err, rows) {

  });
</code></pre><p>Example: select connections using primary key and other filter columns: all likes for the last day</p>
<pre><code>  db.select(&quot;bk_connection&quot;, { id: &#39;123&#39;, type: &#39;like&#39;, mtime: Date.now()-86400000 }, { ops: { type: &quot;begins_with&quot;, mtime: &quot;gt&quot; } }, function(err, rows) {

  });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.get(table, query, options, callback)</code></p>
<p> Retrieve one record from the database by primary key, returns found record or null if not found
Options can use the following special properties:</p>
<ul>
<li>select - a list of columns or expressions to return, default is to return all columns</li>
<li>op - operators to use for comparison for properties, see <code>db.select</code></li>
<li>cached - if specified it runs getCached version</li>
</ul>
<p>Example</p>
<pre><code>    db.get(&quot;bk_account&quot;, { id: &#39;12345&#39; }, function(err, row) {
       if (row) console.log(row.name);
    });
</code></pre></li>
</ul>

<ul>
<li><p><code>db.getCached(op, table, query, options, callback)</code></p>
<p> Retrieve cached result or put a record into the cache prefixed with table:key[:key...]
Options accept the same parameters as for the usual get action but it is very important that all the options
be the same for every call, especially <code>select</code> parameters which tells which columns to retrieve and cache.
Additional options:</p>
<ul>
<li>prefix - prefix to be used for the key instead of table name</li>
</ul>
<p>Example:</p>
<pre><code>db.getCached(&quot;get&quot;, &quot;bk_account&quot;, { id: req.query.id }, { select: &quot;latitude,longitude&quot; }, function(err, row) {
    var distance = utils.geoDistance(req.query.latitude, req.query.longitude, row.latitude, row.longitudde);
});
</code></pre></li>
</ul>

<ul>
<li><p><code>db.getCache(table, query, options, callback)</code></p>
<p> Retrieve an object from the cache by key</p>
</li>
</ul>

<ul>
<li><p><code>db.putCache(table, obj, options)</code></p>
<p> Store a record in the cache</p>
</li>
</ul>

<ul>
<li><p><code>db.delCache(table, query, options)</code></p>
<p> Notify or clear cached record, this is called after del/update operation to clear cached version by primary keys</p>
</li>
</ul>

<ul>
<li><p><code>db.getCacheKey(table, query, options)</code></p>
<p> Returns concatenated values for the primary keys, this is used for caching records by primary key</p>
</li>
</ul>

<ul>
<li><p><code>db.create(table, columns, options, callback)</code></p>
<p> Create a table using column definitions represented as a list of objects. Each column definition can
contain the following properties:</p>
<ul>
<li>name - column name</li>
<li>type - column type, one of: int, real, string, counter or other supported type</li>
<li>primary - column is part of the primary key</li>
<li>unique - column is part of an unique key</li>
<li>index - column is part of an index</li>
<li>value - default value for the column</li>
<li>len - column length</li>
<li>pub - columns is public, <em>this is very important property because it allows anybody to see it when used in the default API functions, i.e. anybody with valid
credentials can retrieve all public columns from all other tables, and if one of the other tables is account table this may expose some personal infoamtion,
so by default only a few columns are marked as public in the bk_account table</em></li>
<li>hidden - completely ignored by all update operations but could be used by the public columns cleaning procedure, if it is computed and not stored in the db
it can contain pub property to be returned to the client</li>
<li>readonly - only add/put operations will use the value, incr/update will not affect the value</li>
<li>writeonly - only incr/update can chnage this value, add/put will ignore it</li>
<li>now - means on every add/put/update set this column with current time as Date.now()</li>
<li>autoincr - for counter tables, mark the column to be auto-incremented by the connection API if the connection type has the same name as the column name</li>
</ul>
<p><em>Some properties may be defined multiple times with number suffixes like: unique1, unique2, index1, index2 to create more than one index for the table, same
properties define a composite key in the order of definition or sorted by the property value, for example: <code>{ a: {index:2 }, b: { index:1 } }</code> will create index (b,a)
because of the index: property value being not the same.</em></p>
<p>NOTE: Index creation is not required and all index properties can be omitted, it can be done more effectively using native tools for any specific database,
this format is for simple and common use cases without using any other tools but it does not cover all possible variations for every database. But all indexes and
primary keys created outside of the backend application still be be detected properly by <code>db.cacheColumns</code> method for every database.</p>
<p>Each database pool also can support native options that are passed directly to the driver in the options, these properties are
defined in the object with the same name as the db driver, all properties are combined, for example to define provisioned throughput for the DynamoDB index:</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index: 1, dynamodb: { readCapacity: 50, writeCapacity: 50 } },
                              type: { primary: 1, pub: 1, projection: 1 },
                              name: { index: 1, pub: 1 } }
                            });
</code></pre><p>Create DynamoDB table with global secondary index, first index property if not the same as primary key hash defines global index, if it is the same then local,
below we create global secondary index on property &#39;name&#39; only, in the example above it was local secondary index for id and name. also local secondary index is
created on id,title.</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index1: 1 },
                              type: { primary: 1, projection: 1 },
                              name: { index: 1 }
                              title: { index1: 1, projection1: 1 } }
                            });
</code></pre><p>When using real DynamoDB creating a table may take some time, for such cases if options.waitTimeout is not specified it defaults to 1min,
so the callback is called as soon as the table is active or after the timeout whichever comes first.</p>
</li>
</ul>
<p> Pass MongoDB options directly:
        db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, mongodb: { w: 1, capped: true, max: 100, size: 100 } },
                                  type: { primary: 1, pub: 1 },
                                  name: { index: 1, pub: 1, mongodb: { sparse: true, min: 2, max: 5 } }
                                });</p>

<ul>
<li><p><code>db.upgrade(table, columns, options, callback)</code></p>
<p> Upgrade a table with missing columns from the definition list</p>
</li>
</ul>

<ul>
<li><p><code>db.drop(table, options, callback)</code></p>
<p> Drop a table</p>
</li>
</ul>

<ul>
<li><p><code>db.prepare(op, table, obj, options)</code></p>
<p> Prepare for execution for the given operation: add, del, put, update,...
Returns prepared object to be passed to the driver&#39;s .query method. This method is a part of the driver
helpers and is not used directly in the applications.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPool(table, options)</code></p>
<p> Return database pool by table name or default pool, options may contain { pool: name } to return
the pool by given name. This call always return valid pool object, in case no requiested pool found it returns
special empty pool which provides same interface but returns errors instesd of results.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPoolByName(name)</code></p>
<p> Returns given pool if it exists and initialized otherwise returns null</p>
</li>
</ul>

<ul>
<li><p><code>db.getOptions(table, options)</code></p>
<p> Return combined options for the pool including global pool options</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumns(table, options)</code></p>
<p> Return columns for a table or null, columns is an object with column names and objects for definition</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumn(table, name, options)</code></p>
<p> Return the column definition for a table</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumnProperty(table, name, attr, options)</code></p>
<p> Return a value for the given property from the column definition, return empty string if no column or attribute found</p>
</li>
</ul>

<ul>
<li><p><code>db.getCapacity(table)</code></p>
<p> Return an object with capacity property which is the max write capacity for the table, for DynamoDB only. It check writeCapacity property
of all table columns.</p>
</li>
</ul>

<ul>
<li><p><code>db.checkCapacity(obj, callback)</code></p>
<p> Check if number of write requests exceeds the capacity per second, delay if necessary, for DynamoDB only but can be used for pacing
write requests with any database or can be used generically. The <code>obj</code> must be initialized with <code>db.getCapacity</code> call.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSelectedColumns(table, options)</code></p>
<p> Return list of selected or allowed only columns, empty list if no options.select is specified</p>
</li>
</ul>

<ul>
<li><p><code>db.skipColumn(name, val, options, columns)</code></p>
<p> Verify column against common options for inclusion/exclusion into the operation, returns 1 if the column must be skipped</p>
</li>
</ul>

<ul>
<li><p><code>db.filterColumns(obj, rows, options)</code></p>
<p> Given object with data and list of keys perform comparison in memory for all rows, return only rows that match all keys. This method is usee
by custom filters in <code>db.select</code> by the drivers which cannot perform comparisons with non-indexes columns like DynamoDb, Cassandra.
The rows that satisfy primary key conditions are retunred and then called this function to eliminate the records that do not satisfy non-indexed column conditions.</p>
<p>Options support the following propertis:</p>
<ul>
<li>keys - list of columns to check, these may or may not be the primary keys, any columns to be compared</li>
<li>cols - an object with columns definition</li>
<li>ops - operations for columns</li>
<li>typesMap - types for the columns if different from the actual Javascript type</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getKeys(table, options)</code></p>
<p> Return cached primary keys for a table or empty array</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchKeys(table, options)</code></p>
<p> Return keys for the table search, if options.keys provided and not empty it will be used otherwise
table&#39;s primary keys will be returned. This is a wrapper that makes sure that valid keys are used and
deals with input errors like empty keys list to be consistent between different databases.
This function always returns an Array even if it is empty.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchQuery(table, obj, options)</code></p>
<p> Return query object based on the keys specified in the options or primary keys for the table, only search properties
will be returned in the query object</p>
</li>
</ul>

<ul>
<li><p><code>db.getQueryForKeys(keys, obj, options)</code></p>
<p> Returns an object based on the list of keys, basically returns a subset of properties</p>
</li>
</ul>

<ul>
<li><p><code>db.getBindValue(table, options, val, info)</code></p>
<p> Return possibly converted value to be used for inserting/updating values in the database,
is used for SQL parametrized statements</p>
<p>Parameters:</p>
<ul>
<li>options - standard pool parameters with pool: property for specific pool</li>
<li>val - the JavaScript value to convert into bind parameter</li>
<li>info - column definition for the value from the cached columns</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getColumnValue(table, options, val, info)</code></p>
<p> Return transformed value for the column value returned by the database, same parameters as for getBindValue</p>
</li>
</ul>

<ul>
<li><p><code>db.convertError(table, op, err, options)</code></p>
<p> Convert native database error in some generic human readable string</p>
</li>
</ul>

<ul>
<li><p><code>db.cacheColumns(options, callback)</code></p>
<p> Reload all columns into the cache for the pool</p>
</li>
</ul>

<ul>
<li><p><code>db.mergeColumns(pool)</code></p>
<p> Merge JavaScript column definitions with the db cached columns</p>
</li>
</ul>

<ul>
<li><p><code>db.mergeKeys(pool)</code></p>
<p> Update pool keys with the primary keys form the table definitions in addition to the actual cached
column info from the database, if no caching performed than this just set the keys assuming it will work, for databases that do
not provide info about primary keys</p>
</li>
</ul>

<ul>
<li><p><code>db.processRows(pool, table, rows, options)</code></p>
<p> Custom row handler that is called for every row in the result, this assumes that pool.processRow callback has been assigned previously by db.setProcessRow.
This function is called automatically by the db.query but can be called manually for rows that are not received from the database, for example on
adding new records and returning them back to the client. In such case, the <code>pool</code> argument can be passed as null, it will be found by the table name.
<code>rows</code> can be list of records or single record.</p>
</li>
</ul>

<ul>
<li><p><code>db.setProcessRow(table, options, callback)</code></p>
<p> Assign processRow callback for a table, this callback will be called for every row on every result being retrieved from the
specified table thus providing an opportunity to customize the result.</p>
<p>All assigned callback to this table will be called in the order of the assignment.</p>
<p>The callback accepts 3 arguments: function(row, options, columns)
 where - row is a row from the table, options are the obj passed to the db called and columns is an object with table&#39;s columns</p>
<p><strong>If the callback returns true, row will be filtered out and not included in the final result set.</strong></p>
</li>
</ul>
<p>  Example</p>
<pre><code>  db.setProcessRow(&quot;bk_account&quot;, function(row, opts, cols) {
      if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
  });

  db.setProcessRow(&quot;bk_icon&quot;, function(row, opts, cols) {
      if (row.type == &quot;private&quot; &amp;&amp; row.id != opts.account.id) return true;
  });
</code></pre>
<ul>
<li><p><code>db.sqlInitPool(options)</code></p>
<p> Create a database pool for SQL like databases</p>
<ul>
<li>options - an object defining the pool, the following properties define the pool:<ul>
<li>pool - pool name/type, of not specified SQLite is used</li>
<li>max - max number of clients to be allocated in the pool</li>
<li>idle - after how many milliseconds an idle client will be destroyed</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlCacheColumns(options, callback)</code></p>
<p> Cache columns using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlPrepare(op, table, obj, options)</code></p>
<p> Prepare SQL statement for the given operation</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlQuote(val)</code></p>
<p> Quote value to be used in SQL expressions</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValue(value, type, dflt, min, max)</code></p>
<p> Return properly quoted value to be used directly in SQL expressions, format according to the type</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValueIn(list, type)</code></p>
<p> Return list in format to be used with SQL IN ()</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlExpr(name, value, options)</code></p>
<p> Build SQL expressions for the column and value
options may contain the following properties:</p>
<ul>
<li>op - SQL operator, default is =</li>
<li>type - can be data, string, number, float, expr, default is string</li>
<li>value - default value to use if passed value is null or empty</li>
<li>min, max - are used for numeric values for validation of ranges</li>
<li>expr - for op=expr, contains sprintf-like formatted expression to be used as is with all &#39;%s&#39; substituted with actual value</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlTime(d)</code></p>
<p> Return time formatted for SQL usage as ISO, if no date specified returns current time</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlFilter(columns, values, params)</code></p>
<p> Given columns definition object, build SQL query using values from the values object, all conditions are joined using AND,</p>
<ul>
<li>columns is a list of objects with the following properties:<ul>
<li>name - column name, also this is the key to use in the values object to get value by</li>
<li>col - actual column name to use in the SQL</li>
<li>alias - optional table prefix if multiple tables involved</li>
<li>value - default value</li>
<li>type - type of the value, this is used for proper formatting: boolean, number, float, date, time, string, expr</li>
<li>op - any valid SQL operation: =,&gt;,&lt;, between, like, not like, in, not in, ~*,.....</li>
<li>group - for grouping multiple columns with OR condition, all columns with the same group will be in the same ( .. OR ..)</li>
<li>always - only use default value if true</li>
<li>required - value default or supplied must be in the query, otherwise return empty SQL</li>
<li>search - additional name for a value, for cases when generic field is used for search but we search specific column</li>
</ul>
</li>
<li>values - actual values for the condition as an object, usually req.query</li>
<li>params - if given will contain values for binding parameters</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlLimit(options)</code></p>
<p> Build SQL orderby/limit/offset conditions, config can define defaults for sorting and paging</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlWhere(table, query, keys, options)</code></p>
<p> Build SQL where condition from the keys and object values, returns SQL statement to be used in WHERE</p>
<ul>
<li>query - properties for the condition, in case of an array the primary keys for IN condition will be used only</li>
<li>keys - a list of columns to use for the condition, other properties will be ignored</li>
<li>options may contains the following properties:<ul>
<li>pool - pool to be used for driver specific functions</li>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlCreate(table, obj, options)</code></p>
<p> Create SQL table using table definition</p>
<ul>
<li>table - name of the table to create</li>
<li>obj - object with properties as column names and each property value is an object:<ul>
<li>name - column name</li>
<li>type - type of the column, default is TEXT, options: int, real or other supported types</li>
<li>value - default value for the column</li>
<li>primary - part of the primary key</li>
<li>index - indexed column, part of the composite index</li>
<li>unique - must be combined with index property to specify unique composite index</li>
<li>len - max length of the column</li>
<li>notnull - true if should be NOT NULL</li>
<li>auto - true for AUTO_INCREMENT column</li>
</ul>
</li>
<li>options may contains:<ul>
<li>upgrade - perform alter table instead of create</li>
<li>typesMap - type mapping, convert lowercase type into other type supported by any specific database</li>
<li>noDefaults - ignore default value if not supported (Cassandra)</li>
<li>noNulls - NOT NULL restriction is not supported (Cassandra)</li>
<li>noMultiSQL - return as a list, the driver does not support multiple SQL commands</li>
<li>noLengths - ignore column length for columns (Cassandra)</li>
<li>noIfExists - do not support IF EXISTS on table or indexes</li>
<li>noCompositeIndex - does not support composite indexes (Cassandra)</li>
<li>noAuto - no support for auto increment columns</li>
<li>skipNull - object with operations which dont support null(empty) values (DynamoDB cannot add/put empty/null values)</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpgrade(table, obj, options)</code></p>
<p> Create ALTER TABLE ADD COLUMN statements for missing columns</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDrop(table, obj, options)</code></p>
<p> Create SQL DROP TABLE statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlSelect(table, query, options)</code></p>
<p> Select object from the database,
options may define the following properties:</p>
<ul>
<li>keys is a list of columns for the condition</li>
<li>select is list of columns or expressions to return</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlInsert(table, obj, options)</code></p>
<p> Build SQL insert statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpdate(table, obj, options)</code></p>
<p> Build SQL statement for update</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDelete(table, obj, options)</code></p>
<p> Build SQL statement for delete</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlInitPool(options)</code></p>
<p> Setup PostgreSQL pool driver</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlCacheIndexes(options, callback)</code></p>
<p> Cache indexes using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>db.pgsqlBindValue(val, info)</code></p>
<p> Convert JS array into db PostgreSQL array format: {..}</p>
</li>
</ul>

<ul>
<li><p><code>db.sqliteInitPool(options)</code></p>
<p> Initialize local SQLite cache database by name or full path</p>
</li>
</ul>

<ul>
<li><p><code>db.sqliteConnect(options, callback)</code></p>
<p> Common code to open or create local SQLite databases, execute all required initialization statements, calls callback
with error as first argument and database object as second</p>
</li>
</ul>

<ul>
<li><p><code>db.mysqlInitPool(options)</code></p>
<p> Setup MySQL database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.dynamodbInitPool(options)</code></p>
<p> Setup DynamoDB database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.mongodbInitPool(options)</code></p>
<p> MongoDB pool</p>
</li>
</ul>

<ul>
<li><p><code>db.cassandraInitPool(options)</code></p>
<p> Cassandra pool</p>
</li>
</ul>

<ul>
<li><p><code>db.leveldbInitPool(options)</code></p>
<p> Setup LMDB/LevelDB database driver, this is simplified driver which supports only basic key-value operations,
table parameter is ignored, the object only supports the properties name and value in the record objects.</p>
<p>Because this driver supports 2 databases it requires type to be specified, possible values are: <code>lmdb, leveldb</code></p>
<p>Options are passed to the LMDB low level driver as MDB_ flags according to <a href="http://symas.com/mdb/doc/">http://symas.com/mdb/doc/</a> and
as properties for LevelDB as described in <a href="http://leveldb.googlecode.com/svn/trunk/doc/index.html">http://leveldb.googlecode.com/svn/trunk/doc/index.html</a></p>
<p>The LevelDB database can only be shared by one process so if no unique options.db is given, it will create a unique database using core.processId()</p>
</li>
</ul>

<ul>
<li><p><code>db.nndbInitPool(options)</code></p>
<p> Create a database pool that works with nanomsg server, all requests will be forwarded to the nanomsg socket,
the server can be on the same machine or on the remote, 2 nanomsg socket types are supported: NN_PUSH or NN_REQ.
In push mode no replies are expected, only sending db updates, in Req mode the server will reply on &#39;get&#39; command only,
all other commands work as in push mode. Only &#39;get,put,del,incr&#39; comamnd are supported, add,update will be sent as put, LevelDB or LMDB
on the other side only support simple key-value operations.
Options can define the following:</p>
<ul>
<li>socket - nanomsg socket type, default is utils.NN_PUSH, can be utils.NN_REQ</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.redisInitPool(options)</code></p>
<p> Redis database pool, uses Hash to store the records</p>
</li>
</ul>

<ul>
<li><p><code>db.elasticsearchInitPool(options)</code></p>
<p> Create a database pool that works with ElasticSearch server</p>
</li>
</ul>

<ul>
<li><p><code>db.couchdbInitPool(options)</code></p>
<p> Create a database pool that works with CouchDB server.</p>
<p>In addition to the standard commands it can execute any CouchDB HTTP API directly</p>
<pre><code>db.query({ op: &quot;GET&quot;, text: &quot;/db/url&quot; }, { pool: &quot;couchdb&quot; }, db.showResult)
db.query({ op: &quot;PUT&quot;, text: &quot;/db/url&quot;, obj: { a: 1 b: 2 } }, { pool: &quot;couchdb&quot; }, db.showResult)
</code></pre></li>
</ul>

<ul>
<li><p><code>db.riakInitPool(options)</code></p>
<p> Create a database pool that works with the Riak database.</p>
<p>By default the driver uses simple key-value mode of operations, to enable bucket-type mode
pass bucketType in the <code>-db-riak-options</code>:</p>
<p>To use maps for the object records set <code>useMaps</code> in the <code>-db-riak-options</code></p>
<pre><code>-db-riak-options &#39;{ &quot;bucketType&quot;: &quot;bk&quot;, &quot;useMaps&quot;: 1 }&#39;
</code></pre><p>In addition to the standard commands it can execute any Riak HTTP API directly</p>
<pre><code>db.query({ op: &quot;GET&quot;, text: &quot;/buckets?buckets=true&quot; }, { pool: &quot;riak&quot; }, db.showResult)
db.query({ op: &quot;POST&quot;, text: &quot;/buckets/bucket/counter/name&quot;, obj: 1 }, { pool: &quot;riak&quot; }, db.showResult)
</code></pre></li>
</ul>

<h2 id="module-ipc">Module: IPC</h2>
<ul>
<li><p><code>ipc</code></p>
<p> IPC communications between processes and support for caching and messaging</p>
</li>
</ul>

<ul>
<li><p><code>ipc.onMessage(msg)</code></p>
<p> A handler for unhandled messages, it is called by the server and client. In case of the server, <code>this</code> is a worker object, so to send a message back to the worker
use <code>this.send()</code>.</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initClient()</code></p>
<p> This function is called by Web worker process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initServer()</code></p>
<p> This function is called by the Web master server process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initServerCaching()</code></p>
<p> Initialize caching system for the configured cache type, can be called many time to re-initialize if the environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initClientCaching()</code></p>
<p> Initialize web worker caching system, can be called anytime the environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initServerQueueing()</code></p>
<p> Initialize queue system for the server process, can be called multiple times in case environment has changed</p>
</li>
</ul>

<ul>
<li><p><code>ipc.initClientQueueing()</code></p>
<p> Initialize web worker queue system, client part, sends all publish messages to this socket which will be broadcasted into the
publish socket by the receiving end. Can be called anytime to reconfigure if the environment has changed.</p>
</li>
</ul>

<ul>
<li><p><code>ipc.shutdown()</code></p>
<p> Close all caching and messaging clients, can be called by a server or a worker</p>
</li>
</ul>

<ul>
<li><p><code>ipc.command(msg, callback, timeout)</code></p>
<p> Send a command to the master process via IPC messages, callback is used for commands that return value back</p>
</li>
</ul>

<ul>
<li><p><code>ipc.send(op, name, value, options, callback)</code></p>
<p> Always send text to the master, convert objects into JSON, value and callback are optional</p>
</li>
</ul>

<ul>
<li><p><code>ipc.bind(name, type, host, port, options, callback)</code></p>
<p> Bind a socket to the address and port i.e. initialize the server socket</p>
</li>
</ul>

<ul>
<li><p><code>ipc.connect(name, type, host, port, options, callback)</code></p>
<p> Connect to the host(s)</p>
</li>
</ul>

<ul>
<li><p><code>ipc.setup(name, type, options, callback)</code></p>
<p> Setup a socket or client, return depends on the client type</p>
</li>
</ul>

<ul>
<li><p><code>ipc.close(name, type)</code></p>
<p> Close a socket or a client</p>
</li>
</ul>

<ul>
<li><p><code>ipc.subscribe(key, callback, data)</code></p>
<p> Subscribe to the publishing server for messages starting with the given key, the callback will be called only on new data received, the data
is passed to the callback as first argument, if not specified then &quot;undefined&quot; will still be passed, the actual key will be passed as the second
argument, the mesages received as the third argument</p>
<p>Example:</p>
<pre><code>    ipc.subscribe(&quot;alert:&quot;, function(req, key, data) {
        req.res.json(data);
    }, req);
</code></pre></li>
</ul>

<ul>
<li><p><code>ipc.unsubscribe(key)</code></p>
<p> Close subscription</p>
</li>
</ul>

<ul>
<li><p><code>ipc.publish(key, data)</code></p>
<p> Publish an event to be sent to the subscribed clients</p>
</li>
</ul>

<h2 id="module-logger">Module: LOGGER</h2>
<ul>
<li><p><code>logger</code></p>
<p> Simple logger utility for debugging</p>
</li>
</ul>

<ul>
<li><p><code>logger.setSyslog (on)</code></p>
<p> Set or close syslog mode</p>
</li>
</ul>

<ul>
<li><p><code>logger.setFile(file)</code></p>
<p> Redirect logging into file</p>
</li>
</ul>

<ul>
<li><p><code>logger.setDebugFilter(str)</code></p>
<p> Enable debugging level for this label, if used with the same debugging level it will be printed regardless of the global level</p>
</li>
</ul>

<ul>
<li><p><code>logger.setChannel(name)</code></p>
<p> Assign output channel to system logger, default is stdout</p>
</li>
</ul>

<ul>
<li><p><code>logger.printSyslog(level, msg)</code></p>
<p> syslog allows facility to be specified after log level like info:local0 for LOG_LOCAL0</p>
</li>
</ul>

<ul>
<li><p><code>logger.debug()</code></p>
<p> Make it one line to preserve space, syslog cannot output very long lines</p>
</li>
</ul>

<ul>
<li><p><code>logger.trace()</code></p>
<p> Print stack backtrace as error</p>
</li>
</ul>

<ul>
<li><p><code>logger.logger()</code></p>
<p> A generic logger method, safe, first arg is supposed to be a logging level, if not valid the call is ignored</p>
</li>
</ul>

<ul>
<li><p><code>logger.print()</code></p>
<p> Default write handler</p>
</li>
</ul>

<ul>
<li><p><code>logger.write(str)</code></p>
<p> Stream emulation</p>
</li>
</ul>

<h2 id="module-metrics">Module: METRICS</h2>
<h2 id="module-msg">Module: MSG</h2>
<ul>
<li><p><code>msg</code></p>
<p> Messaging and push notifications for mobile and other clients, supports Apple, Google and AWS/SNS push notifications.</p>
</li>
</ul>

<ul>
<li><p><code>msg.init(callback)</code></p>
<p> Initialize supported notification services, this must be called before sending any push notifications</p>
</li>
</ul>

<ul>
<li><p><code>msg.shutdown(options, callback)</code></p>
<p> Shutdown notification services, wait till all pending messages are sent before calling the callback</p>
</li>
</ul>

<ul>
<li><p><code>msg.shutdownWorker(options, callback)</code></p>
<p> Gracefully drain all message queues on worker exit</p>
</li>
</ul>

<ul>
<li><p><code>msg.shutdownWeb(options, callback)</code></p>
<p> Gracefully drain all message queues on web process exit</p>
</li>
</ul>

<ul>
<li><p><code>msg.send(options, callback)</code></p>
<p> Deliver a notification using the specified service, apple is default.
Options may contain the following properties:</p>
<ul>
<li>device_id - device where to send the message to</li>
<li>service - which service to use for delivery: aws, apns, gcm, ads, mpns, wns</li>
<li>msg - text message to send</li>
<li>badge - badge number to show if supported by the service</li>
<li>type - set type of the message, service specific</li>
<li>id - send id with the notification, this is application specific data, sent as is</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>msg.initAPN()</code></p>
<p> Initiaize Apple Push Notification service in the current process, Apple supports multiple connections to the APN gateway but
not too many so this should be called on the dedicated backend hosts, on multi-core servers every spawn web process will initialize a
connection to APN gateway.</p>
</li>
</ul>

<ul>
<li><p><code>msg.closeAPN(callback)</code></p>
<p> Close APN agent, try to send all pending messages before closing the gateway connection</p>
</li>
</ul>

<ul>
<li><p><code>msg.sendAPN(device_id, options, callback)</code></p>
<p> Send push notification to an Apple device, returns true if the message has been queued.</p>
<p>The options may contain the following properties:</p>
<ul>
<li>msg - message text</li>
<li>badge - badge number</li>
<li>type - set type of the packet</li>
<li>id - send id in the user properties</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>msg.initGCM()</code></p>
<p> Initialize Google Cloud Messaginh servie to send push notifications to mobile devices</p>
</li>
</ul>

<ul>
<li><p><code>msg.closeGCM(callback)</code></p>
<p> Close GCM connection, flush the queue</p>
</li>
</ul>

<ul>
<li><p><code>msg.sendGCM(device_id, options, callback)</code></p>
<p> Send push notification to an Android device, return true if queued.</p>
</li>
</ul>

<ul>
<li><p><code>msg.sendAWS(device_id, options, callback)</code></p>
<p> Send push notification to a device using AWS SNS service, device_d must be a valid SNS endpoint ARN.</p>
<p>The options may contain the following properties:</p>
<ul>
<li>msg - message text</li>
<li>badge - badge number</li>
<li>type - set type of the packet</li>
<li>id - send id in the user properties</li>
</ul>
</li>
</ul>

<h2 id="module-server">Module: SERVER</h2>
<ul>
<li><p><code>server</code></p>
<p> The main server class that starts various processes</p>
</li>
</ul>

<ul>
<li><p><code>server.start()</code></p>
<p> Start the server process, call the callback to perform some initialization before launchng any server, just after core.init</p>
</li>
</ul>

<ul>
<li><p><code>server.startMonitor(options)</code></p>
<p> Start process monitor, running as root</p>
</li>
</ul>

<ul>
<li><p><code>server.startMaster(options)</code></p>
<p> Setup worker environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWorker(options)</code></p>
<p> Job worker process</p>
</li>
</ul>

<ul>
<li><p><code>server.startWeb(options)</code></p>
<p> Create Express server, setup worker environment, call supplied callback to set initial environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebProcess()</code></p>
<p> Spawn web server from the master as a separate master with web workers, it is used when web and master processes are running on the same server</p>
</li>
</ul>

<ul>
<li><p><code>server.handleChildProcess(child, type, method)</code></p>
<p> Setup exit listener on the child process and restart it</p>
</li>
</ul>

<ul>
<li><p><code>server.startProcess()</code></p>
<p> Restart the main process with the same arguments and setup as a monitor for the spawn child</p>
</li>
</ul>

<ul>
<li><p><code>server.startWatcher()</code></p>
<p> Watch source files for modifications and restart</p>
</li>
</ul>

<ul>
<li><p><code>server.startRepl(port, bind)</code></p>
<p> Start command prompt on TCP socket, context can be an object with properties assigned with additional object to be accessible in the shell</p>
</li>
</ul>

<ul>
<li><p><code>server.startDaemon()</code></p>
<p> Create daemon from the current process, restart node with -daemon removed in the background</p>
</li>
</ul>

<ul>
<li><p><code>server.startShell(options)</code></p>
<p> Start REPL shell or execute any subcommand if specified</p>
</li>
</ul>

<ul>
<li><p><code>server.onexit()</code></p>
<p> Kill all child processes on exit</p>
</li>
</ul>

<ul>
<li><p><code>server.onkill()</code></p>
<p> Terminates the server process</p>
</li>
</ul>

<ul>
<li><p><code>server.sleep(options, callback)</code></p>
<p> Sleep and keep a worker busy</p>
</li>
</ul>

<ul>
<li><p><code>server.shutdown(options, callback)</code></p>
<p> Shutdown the system immediately, mostly to be used in the remote jobs as the last task</p>
</li>
</ul>

<ul>
<li><p><code>server.respawn(callback)</code></p>
<p> If respawning too fast, delay otherwise schedule new process after short timeout</p>
</li>
</ul>

<ul>
<li><p><code>server.spawnProcess(args, skip, opts)</code></p>
<p> Start new process reusing global process arguments, args will be added and args in the skip list will be removed</p>
</li>
</ul>

<ul>
<li><p><code>server.runJob(job)</code></p>
<p> Run all jobs from the job spec at the same time, when the last job finishes and it is running in the worker process, the process terminates.</p>
</li>
</ul>

<ul>
<li><p><code>server.execJob(job)</code></p>
<p> Execute job in the background by one of the workers, object must be known exported module
and method must be existing method of the given object. The method function must take options
object as its first argument and callback as its second argument.
More than one job can be specified, property of the object defines name for the job to run:</p>
<p>Example:</p>
<pre><code>    { &#39;scraper.run&#39;: {}, &#39;server.shutdown&#39;: {} }
</code></pre><p>If the same object.method must be executed several times, prepend subsequent jobs with $</p>
<p>Example:</p>
<pre><code>    { &#39;scraper.run&#39;: { &quot;arg&quot;: 1 }, &#39;$scraper.run&#39;: { &quot;arg&quot;: 2 }, &#39;$$scraper.run&#39;: { &quot;arg&quot;: 3 } }
</code></pre><p>Supported options by the server:</p>
<ul>
<li>skipqueue - in case of a duplicate or other condition when this job cannot be executed it is put back to
  the waiting queue, this options if set to 1 makes the job to be ignored on error</li>
<li>runalways - no checks for existing job wth the same name should be done</li>
<li>runone - only run the job if there is no same running job, this options serializes similar jobs</li>
<li>runlast - run when no more pending or running jobs</li>
<li>runafter - specifies another job in canoncal form obj.method which must finish and not be pending in
order for this job to start, this implements chaining of jobs to be executed one after another
but submitted at the same time</li>
</ul>
<p>Exampe: submit 3 jobs to run sequentially:</p>
<pre><code>    &#39;scraper.import&#39;
    { &#39;scraper.sync&#39;: { runafter: &#39;scraper.import&#39; } }
    { &#39;server.shutdown&#39;: { runafter: &#39;scraper.sync&#39; } }
</code></pre></li>
</ul>

<ul>
<li><p><code>server.launchJob(job, options, callback)</code></p>
<p> Remote mode, launch remote instance to perform scraping or other tasks
By default, shutdown the instance after job finishes unless noshutdown:1 is specified in the options</p>
</li>
</ul>

<ul>
<li><p><code>server.queueJob(job)</code></p>
<p> Run a job, the string is in the format:
object/method/name/value/name/value....
All spaces must be are replaced with %20 to be used in command line parameterrs</p>
</li>
</ul>

<ul>
<li><p><code>server.execJobQueue()</code></p>
<p> Process pending jobs, submit to idle workers</p>
</li>
</ul>

<ul>
<li><p><code>server.scheduleCronjob(spec, obj)</code></p>
<p> Create a new cron job, for remote jobs additional property args can be used in the object to define
arguments for the instance backend process, properties must start with -</p>
<p>If a job object contains the <code>tag</code> property, it will be submitted into the bk_queue for execution by that worker</p>
<p>Example:</p>
<pre><code>    { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 */10 * * * *&quot;, &quot;job&quot;: &quot;server.processQueue&quot; },
    { &quot;type&quot;: &quot;local&quot;, &quot;cron&quot;: &quot;0 10 7 * * *&quot;, &quot;id&quot;: &quot;processQueue&quot;, &quot;job&quot;: &quot;api.processQueue&quot; }
    { &quot;type&quot;: &quot;remote&quot;, &quot;cron&quot;: &quot;0 5 * * * *&quot;, &quot;args&quot;: { &quot;-workers&quot;: 2 }, &quot;job&quot;: { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host1&quot; }, &quot;$scraper.run&quot;: { &quot;url&quot;: &quot;host2&quot; } } }
    { &quot;type&quot;: &quot;local&quot;, &quot;cron&quot;: &quot;0 */10 * * * *&quot;, &quot;tag&quot;: &quot;host-12&quot;, &quot;job&quot;: &quot;server.processQueue&quot; },
</code></pre></li>
</ul>

<ul>
<li><p><code>server.runCronjob(id)</code></p>
<p> Execute a cronjob by id now, it must have been scheduled already and id property must be specified in the crontab
When REPL is activated on the master server with -repl-port then connecting to the running master server via telnet it is possible to execute
cron jobs manually</p>
<p>Example:</p>
<pre><code>// Start the backend with repl-port like `bkjs run-backend -repl-port 2080`

# telnet localhost 2080
&gt; server.runCronjob(&quot;processQueue&quot;)
</code></pre></li>
</ul>

<ul>
<li><p><code>server.scheduleJob(options, callback)</code></p>
<p> Perform execution according to type</p>
</li>
</ul>

<ul>
<li><p><code>server.loadSchedules()</code></p>
<p> Load crontab from JSON file as list of job specs:</p>
<ul>
<li>type - local, remote, server<ul>
<li>local means spawn a worker to run the job function</li>
<li>remote means launch an AWS instance</li>
<li>server means run inside the master process, do not spawn a worker</li>
</ul>
</li>
<li>cron - cron time interval spec: &#39;second&#39; &#39;minute&#39; &#39;hour&#39; &#39;dayOfMonth&#39; &#39;month&#39; &#39;dayOfWeek&#39;</li>
<li>job - a string as obj.method or an object with job name as property name and the value is an object with<pre><code> additional options for the job passed as first argument, a job callback always takes options and callback as 2 arguments
</code></pre></li>
<li>args - additional arguments to be passed to the backend in the command line for the remote jobs</li>
</ul>
<p>Example:</p>
<pre><code>    [ { &quot;type&quot;: &quot;local&quot;, cron: &quot;0 0 * * * *&quot;, job: &quot;scraper.run&quot; }, ..]
</code></pre></li>
</ul>

<ul>
<li><p><code>server.submitJob(options, callback)</code></p>
<p> Submit job for execution, it will be saved in the server queue and the master or matched job server will pick it up later.</p>
<p>The options can specify:</p>
<ul>
<li>tag - job name for execution, this can be used to run on specified servers by IP address or other tag asigned</li>
<li>type - job type: local, remote, server</li>
<li>stime - start time ofr the job, it will wait until this time is current to be processed</li>
<li>etime - expiration time, after this this job is ignored</li>
<li>job - an object with a job spec, for name-only job the object can look like { job: null }</li>
<li>args - additional arguments for remote job to pass in the command line via user-data</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>server.processSQS(options, callback)</code></p>
<p> Process AWS SQS queue for any messages and execute jobs, a job object must be in the same format as for the cron jobs.</p>
<p>The options can specify:</p>
<ul>
<li>queue - SQS queue ARN, if not specified the <code>-server-job-queue</code> will be used</li>
<li>timeout - how long to wait for messages, seconds, default is 5</li>
<li>count - how many jobs to receive, if not specified use <code>-api-max-jobs</code> config parameter</li>
<li>visibilityTimeout - The duration in seconds that the received messages are hidden from subsequent retrieve requests
 after being retrieved by a ReceiveMessage request.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>server.processQueue(options, callback)</code></p>
<p> Load submitted jobs for execution, it is run by the master process every <code>-server-jobs-interval</code> seconds.
Requires connection to the PG database, how jobs appear in the table and the order of execution is not concern of this function,
the higher level management tool must take care when and what to run and in what order.</p>
</li>
</ul>


